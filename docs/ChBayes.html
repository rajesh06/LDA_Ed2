<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Bayesian Inference and Modeling | Loss Data Analytics</title>
  <meta name="description" content="Chapter 9 Bayesian Inference and Modeling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Bayesian Inference and Modeling | Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 9 Bayesian Inference and Modeling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/openacttexts/Loss-Data-Analytics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Bayesian Inference and Modeling | Loss Data Analytics" />
  
  <meta name="twitter:description" content="Chapter 9 Bayesian Inference and Modeling | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ChapSimulation.html"/>
<link rel="next" href="ChapPremiumFoundations.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the markdown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
        MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};

// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}


// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};

Survey.StylesManager.applyTheme("modern");

</script>  
<!-- This completes the code for the quizzes -->

<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>

<script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  if (t = document.getElementById("webex-total_correct")) {
    var correct = document.getElementsByClassName("webex-correct").length;
    var solvemes = document.getElementsByClassName("webex-solveme").length;
    var radiogroups = document.getElementsByClassName("webex-radiogroup").length;
    var selects = document.getElementsByClassName("webex-select").length;
    
    t.innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");
  
  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");
  
  var cl = this.classList
  
  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }
  
  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;
  
  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }
  
  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }
  
  update_total_correct();
}

window.onload = function() {
  console.log("onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;
  }
  
  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }
  
  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
  }

  update_total_correct();
}

</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="Format/style.css" type="text/css" />
<link rel="stylesheet" href="includeWebex/webex.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#other-collaborators"><i class="fa fa-check"></i>Other Collaborators</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version"><i class="fa fa-check"></i>Version</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ChapIntro.html"><a href="ChapIntro.html"><i class="fa fa-check"></i><b>1</b> Loss Data and Insurance Activities</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Data Driven Insurance Activities</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:DataDriven"><i class="fa fa-check"></i><b>1.1.2</b> Why Data Driven?</a></li>
<li class="chapter" data-level="1.1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ChapIntro.html"><a href="ChapIntro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="ChapIntro.html"><a href="ChapIntro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="ChapIntro.html"><a href="ChapIntro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="ChapIntro.html"><a href="ChapIntro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ChapIntro.html"><a href="ChapIntro.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
<li class="chapter" data-level="1.5" data-path="ChapIntro.html"><a href="ChapIntro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Elements"><i class="fa fa-check"></i><b>2.1</b> Elements of Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#key-data-analytic-concepts"><i class="fa fa-check"></i><b>2.1.1</b> Key Data Analytic Concepts</a></li>
<li class="chapter" data-level="2.1.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:DataAlgorithm"><i class="fa fa-check"></i><b>2.1.2</b> Data versus Algorithmic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Process"><i class="fa fa-check"></i><b>2.2</b> Data Analysis Process</a></li>
<li class="chapter" data-level="2.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:SingleVarAnalytics"><i class="fa fa-check"></i><b>2.3</b> Single Variable Analytics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:VarTypes"><i class="fa fa-check"></i><b>2.3.1</b> Variable Types</a></li>
<li class="chapter" data-level="2.3.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:EDACDA"><i class="fa fa-check"></i><b>2.3.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="2.3.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#model-construction"><i class="fa fa-check"></i><b>2.3.3</b> Model Construction</a></li>
<li class="chapter" data-level="2.3.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#model-selection"><i class="fa fa-check"></i><b>2.3.4</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:ManyVarAnalytics"><i class="fa fa-check"></i><b>2.4</b> Analytics with Many Variables</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#supervised-and-unsupervised-learning"><i class="fa fa-check"></i><b>2.4.1</b> Supervised and Unsupervised Learning</a></li>
<li class="chapter" data-level="2.4.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#algorithmic-modeling"><i class="fa fa-check"></i><b>2.4.2</b> Algorithmic Modeling</a></li>
<li class="chapter" data-level="2.4.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Data Modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:DataLearn"><i class="fa fa-check"></i><b>2.5</b> Data</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-types"><i class="fa fa-check"></i><b>2.5.1</b> Data Types</a></li>
<li class="chapter" data-level="2.5.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-structures-and-storage"><i class="fa fa-check"></i><b>2.5.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="2.5.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-cleaning"><i class="fa fa-check"></i><b>2.5.3</b> Data Cleaning</a></li>
<li class="chapter" data-level="2.5.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#Sec:BigDataAnalysis"><i class="fa fa-check"></i><b>2.5.4</b> Big Data Analysis</a></li>
<li class="chapter" data-level="2.5.5" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#ethical-issues"><i class="fa fa-check"></i><b>2.5.5</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>2.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:MultiEDA"><i class="fa fa-check"></i><b>2.6.1</b> Technical Supplement: Multivariate Exploratory Analysis</a></li>
<li class="chapter" data-level="2.6.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#tree-based-models"><i class="fa fa-check"></i><b>2.6.2</b> Tree-based Models</a></li>
<li class="chapter" data-level="2.6.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#technical-supplement-some-r-functions"><i class="fa fa-check"></i><b>2.6.3</b> Technical Supplement: Some R Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html"><i class="fa fa-check"></i><b>3</b> Frequency Modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>3.1</b> Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>3.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>3.2</b> Basic Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>3.2.1</b> Foundations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>3.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="3.2.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>3.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>3.3</b> The (<em>a</em>, <em>b</em>, 0) Class</a></li>
<li class="chapter" data-level="3.4" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>3.4</b> Estimating Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>3.4.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="3.4.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>3.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>3.5</b> Other Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>3.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>3.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>3.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="3.8" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>3.9</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>3.9.1</b> TS 3.A. R Code for Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ChapSeverity.html"><a href="ChapSeverity.html"><i class="fa fa-check"></i><b>4</b> Modeling Loss Severity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>4.1</b> Basic Distributional Quantities</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>4.1.1</b> Moments</a></li>
<li class="chapter" data-level="4.1.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LS:Quantiles"><i class="fa fa-check"></i><b>4.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="4.1.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#moment-generating-function"><i class="fa fa-check"></i><b>4.1.3</b> Moment Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>4.2</b> Continuous Distributions for Modeling Loss Severity</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Loss:Gamma"><i class="fa fa-check"></i><b>4.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="4.2.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#pareto-distribution"><i class="fa fa-check"></i><b>4.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="4.2.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LS:Weibull"><i class="fa fa-check"></i><b>4.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="4.2.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>4.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#MethodsCreation"><i class="fa fa-check"></i><b>4.3</b> Methods of Creating New Distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>4.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="4.3.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>4.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="4.3.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LossSev:Raising"><i class="fa fa-check"></i><b>4.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="4.3.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#exponentiation"><i class="fa fa-check"></i><b>4.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="4.3.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#finite-mixtures"><i class="fa fa-check"></i><b>4.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="4.3.6" data-path="ChapSeverity.html"><a href="ChapSeverity.html#continuous-mixtures"><i class="fa fa-check"></i><b>4.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#estimating-loss-distributions"><i class="fa fa-check"></i><b>4.4</b> Estimating Loss Distributions</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:MS:NonParEst"><i class="fa fa-check"></i><b>4.4.1</b> Nonparametric Estimation</a></li>
<li class="chapter" data-level="4.4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#parametric-estimation"><i class="fa fa-check"></i><b>4.4.2</b> Parametric Estimation</a></li>
<li class="chapter" data-level="4.4.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>4.4.3</b> Maximum Likelihood Estimators for Complete Data</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>4.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html"><i class="fa fa-check"></i><b>5</b> Modeling Claim Severity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>5.1</b> Coverage Modifications</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>5.1.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="5.1.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>5.1.2</b> Policy Limits</a></li>
<li class="chapter" data-level="5.1.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#coinsurance-and-inflation"><i class="fa fa-check"></i><b>5.1.3</b> Coinsurance and Inflation</a></li>
<li class="chapter" data-level="5.1.4" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>5.1.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:MS:ModifiedData1"><i class="fa fa-check"></i><b>5.2</b> Parametric Estimation using Modified Data</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:MS:GroupedData"><i class="fa fa-check"></i><b>5.2.1</b> Parametric Estimation using Grouped Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#censored-data"><i class="fa fa-check"></i><b>5.2.2</b> Censored Data</a></li>
<li class="chapter" data-level="5.2.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#truncated-data"><i class="fa fa-check"></i><b>5.2.3</b> Truncated Data</a></li>
<li class="chapter" data-level="5.2.4" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#parametric-estimation-using-censored-and-truncated-data"><i class="fa fa-check"></i><b>5.2.4</b> Parametric Estimation using Censored and Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>5.3</b> Nonparametric Estimation using Modified Data</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#grouped-data"><i class="fa fa-check"></i><b>5.3.1</b> Grouped Data</a></li>
<li class="chapter" data-level="5.3.2" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:MS:PlugIn"><i class="fa fa-check"></i><b>5.3.2</b> Plug-in Principle</a></li>
<li class="chapter" data-level="5.3.3" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#S:MS:RightCensored"><i class="fa fa-check"></i><b>5.3.3</b> Right-Censored Empirical Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ChapClaimSeverity.html"><a href="ChapClaimSeverity.html#CS:further-reading-and-resources"><i class="fa fa-check"></i><b>5.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html"><i class="fa fa-check"></i><b>6</b> Model Selection and Estimation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>6.1</b> Model Selection</a></li>
<li class="chapter" data-level="6.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>6.2</b> Tools for Model Selection and Diagnostics</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:GraphComparison"><i class="fa fa-check"></i><b>6.2.1</b> Graphical Comparison of Distributions</a></li>
<li class="chapter" data-level="6.2.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Tools:Stats"><i class="fa fa-check"></i><b>6.2.2</b> Statistical Comparison of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Iterative:Selection"><i class="fa fa-check"></i><b>6.3</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="6.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Tools:Stats:Likelihood"><i class="fa fa-check"></i><b>6.4</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="6.5" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>6.5</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="6.6" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Cross-Validation"><i class="fa fa-check"></i><b>6.6</b> Model Selection Based on Cross-Validation</a></li>
<li class="chapter" data-level="6.7" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Modified-Data"><i class="fa fa-check"></i><b>6.7</b> Model Selection for Modified Data</a></li>
<li class="chapter" data-level="6.8" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>6.8</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html"><i class="fa fa-check"></i><b>7</b> Aggregate Loss Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#introduction"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>7.2</b> Individual Risk Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>7.2.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="7.2.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#aggregate-loss-distribution"><i class="fa fa-check"></i><b>7.2.2</b> Aggregate Loss Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:AggLoss:CRM"><i class="fa fa-check"></i><b>7.3</b> Collective Risk Model</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#moments-and-distribution-1"><i class="fa fa-check"></i><b>7.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="7.3.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>7.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="7.3.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#closed-form-distributions"><i class="fa fa-check"></i><b>7.3.3</b> Closed-form Distributions</a></li>
<li class="chapter" data-level="7.3.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:AggLoss:Tweedie"><i class="fa fa-check"></i><b>7.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>7.4</b> Computing the Aggregate Claims Distribution</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>7.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="7.4.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#simulation"><i class="fa fa-check"></i><b>7.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>7.5</b> Effects of Coverage Modifications</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>7.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="7.5.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>7.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="7.5.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>7.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>7.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-6.a.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS 6.A.1. Individual Risk Model Properties</a></li>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-6.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it"><i class="fa fa-check"></i>TS 6.A.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-6.a.3.-moment-generating-function-of-aggregate-loss-s_n-in-example-6.3.8"><i class="fa fa-check"></i>TS 6.A.3. Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span> in Example 6.3.8</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ChapSimulation.html"><a href="ChapSimulation.html"><i class="fa fa-check"></i><b>8</b> Simulation and Resampling</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:SimulationFundamentals"><i class="fa fa-check"></i><b>8.1</b> Simulation Fundamentals</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>8.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="8.1.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:InverseTransform"><i class="fa fa-check"></i><b>8.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="8.1.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#simulation-precision"><i class="fa fa-check"></i><b>8.1.3</b> Simulation Precision</a></li>
<li class="chapter" data-level="8.1.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:SimulationStatInference"><i class="fa fa-check"></i><b>8.1.4</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Bootstrap"><i class="fa fa-check"></i><b>8.2</b> Bootstrapping and Resampling</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#bootstrap-foundations"><i class="fa fa-check"></i><b>8.2.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="8.2.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sim:Precision"><i class="fa fa-check"></i><b>8.2.2</b> Bootstrap Precision: Bias, Standard Deviation, and Mean Square Error</a></li>
<li class="chapter" data-level="8.2.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#confidence-intervals"><i class="fa fa-check"></i><b>8.2.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="8.2.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:ParametricBootStrap"><i class="fa fa-check"></i><b>8.2.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:CrossValidation"><i class="fa fa-check"></i><b>8.3</b> Cross-Validation</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>8.3.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="8.3.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>8.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="8.3.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#cross-validation-and-bootstrap"><i class="fa fa-check"></i><b>8.3.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:ImportanceSampling"><i class="fa fa-check"></i><b>8.4</b> Importance Sampling</a></li>
<li class="chapter" data-level="8.5" data-path="ChapSimulation.html"><a href="ChapSimulation.html#Simulation:further-reading-and-resources"><i class="fa fa-check"></i><b>8.5</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#ts-7.a.-bootstrap-applications-in-predictive-modeling"><i class="fa fa-check"></i><b>8.5.1</b> TS 7.A. Bootstrap Applications in Predictive Modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ChBayes.html"><a href="ChBayes.html"><i class="fa fa-check"></i><b>9</b> Bayesian Inference and Modeling</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SecIntro"><i class="fa fa-check"></i><b>9.1</b> A Gentle Introduction to Bayesian Statistics</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SubSecBayesVsFreq"><i class="fa fa-check"></i><b>9.1.1</b> Bayesian versus Frequentist Statistics</a></li>
<li class="chapter" data-level="9.1.2" data-path="ChBayes.html"><a href="ChBayes.html#a-brief-history-lesson"><i class="fa fa-check"></i><b>9.1.2</b> A Brief History Lesson</a></li>
<li class="chapter" data-level="9.1.3" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SubsecBayesRule"><i class="fa fa-check"></i><b>9.1.3</b> Bayes Rule</a></li>
<li class="chapter" data-level="9.1.4" data-path="ChBayes.html"><a href="ChBayes.html#an-introductory-example-of-bayes-rule"><i class="fa fa-check"></i><b>9.1.4</b> An Introductory Example of Bayes Rule</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SecBuildingBlocks"><i class="fa fa-check"></i><b>9.2</b> Building Blocks of Bayesian Inference</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SubsecPosterior"><i class="fa fa-check"></i><b>9.2.1</b> Posterior Distribution</a></li>
<li class="chapter" data-level="9.2.2" data-path="ChBayes.html"><a href="ChBayes.html#likelihood-function"><i class="fa fa-check"></i><b>9.2.2</b> Likelihood Function</a></li>
<li class="chapter" data-level="9.2.3" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SubsecPrior"><i class="fa fa-check"></i><b>9.2.3</b> Prior Distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SecConjugate"><i class="fa fa-check"></i><b>9.3</b> Conjugate Families</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SubsecBetaBin"><i class="fa fa-check"></i><b>9.3.1</b> The BetaBinomial Conjugate Family</a></li>
<li class="chapter" data-level="9.3.2" data-path="ChBayes.html"><a href="ChBayes.html#the-gammapoisson-conjugate-family"><i class="fa fa-check"></i><b>9.3.2</b> The GammaPoisson Conjugate Family</a></li>
<li class="chapter" data-level="9.3.3" data-path="ChBayes.html"><a href="ChBayes.html#the-normalnormal-conjugate-family"><i class="fa fa-check"></i><b>9.3.3</b> The NormalNormal Conjugate Family</a></li>
<li class="chapter" data-level="9.3.4" data-path="ChBayes.html"><a href="ChBayes.html#criticism-of-conjugate-family-models"><i class="fa fa-check"></i><b>9.3.4</b> Criticism of Conjugate Family Models</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SecPosterior"><i class="fa fa-check"></i><b>9.4</b> Posterior Simulation</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="ChBayes.html"><a href="ChBayes.html#introduction-to-markov-chain-monte-carlo-methods"><i class="fa fa-check"></i><b>9.4.1</b> Introduction to Markov Chain Monte Carlo Methods</a></li>
<li class="chapter" data-level="9.4.2" data-path="ChBayes.html"><a href="ChBayes.html#the-gibbs-sampler"><i class="fa fa-check"></i><b>9.4.2</b> The Gibbs Sampler</a></li>
<li class="chapter" data-level="9.4.3" data-path="ChBayes.html"><a href="ChBayes.html#the-metropolishastings-algorithm"><i class="fa fa-check"></i><b>9.4.3</b> The MetropolisHastings Algorithm</a></li>
<li class="chapter" data-level="9.4.4" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SubsecDiag"><i class="fa fa-check"></i><b>9.4.4</b> Markov Chain Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ChBayes.html"><a href="ChBayes.html#ChBayes:SecFurther"><i class="fa fa-check"></i><b>9.5</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChBayes.html"><a href="ChBayes.html#contributors-8"><i class="fa fa-check"></i>Contributors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html"><i class="fa fa-check"></i><b>10</b> Premium Foundations</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:IntroductionRatemaking"><i class="fa fa-check"></i><b>10.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="10.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:AggRateMaking"><i class="fa fa-check"></i><b>10.2</b> Aggregate Ratemaking Methods</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:PurePremium"><i class="fa fa-check"></i><b>10.2.1</b> Pure Premium Method</a></li>
<li class="chapter" data-level="10.2.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:LossRatio"><i class="fa fa-check"></i><b>10.2.2</b> Loss Ratio Method</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:PricingPrinciples"><i class="fa fa-check"></i><b>10.3</b> Pricing Principles</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#premium-principles"><i class="fa fa-check"></i><b>10.3.1</b> Premium Principles</a></li>
<li class="chapter" data-level="10.3.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#properties-of-premium-principles"><i class="fa fa-check"></i><b>10.3.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:HeterogeneousRisks"><i class="fa fa-check"></i><b>10.4</b> Heterogeneous Risks</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:ExposureToRisk"><i class="fa fa-check"></i><b>10.4.1</b> Exposure to Risk</a></li>
<li class="chapter" data-level="10.4.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:RatingFactors"><i class="fa fa-check"></i><b>10.4.2</b> Rating Factors</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:TrendDevelopment"><i class="fa fa-check"></i><b>10.5</b> Development and Trending</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#exposures-and-premiums"><i class="fa fa-check"></i><b>10.5.1</b> Exposures and Premiums</a></li>
<li class="chapter" data-level="10.5.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#losses-claims-and-payments"><i class="fa fa-check"></i><b>10.5.2</b> Losses, Claims, and Payments</a></li>
<li class="chapter" data-level="10.5.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:CompareMethods"><i class="fa fa-check"></i><b>10.5.3</b> Comparing Pure Premium and Loss Ratio Methods</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:GiniStatistic"><i class="fa fa-check"></i><b>10.6</b> Selecting a Premium</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#classic-lorenz-curve"><i class="fa fa-check"></i><b>10.6.1</b> Classic Lorenz Curve</a></li>
<li class="chapter" data-level="10.6.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#performance-curve-and-a-gini-statistic"><i class="fa fa-check"></i><b>10.6.2</b> Performance Curve and a Gini Statistic</a></li>
<li class="chapter" data-level="10.6.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#out-of-sample-validation"><i class="fa fa-check"></i><b>10.6.3</b> Out-of-Sample Validation</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>10.7</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#ts-9.a.-rate-regulation"><i class="fa fa-check"></i>TS 9.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html"><i class="fa fa-check"></i><b>11</b> Risk Classification</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>11.2</b> Poisson Regression Model</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>11.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="11.2.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>11.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="11.2.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>11.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="11.2.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#exercises-5"><i class="fa fa-check"></i><b>11.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>11.3</b> Categorical Variables and Multiplicative Tariff</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>11.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="11.3.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>11.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="11.3.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>11.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="11.3.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>11.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>11.4</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-10.a.-estimating-poisson-regression-models"><i class="fa fa-check"></i>TS 10.A. Estimating Poisson Regression Models</a></li>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-10.b.-selecting-rating-factors"><i class="fa fa-check"></i>TS 10.B. Selecting Rating Factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ChapCredibility.html"><a href="ChapCredibility.html"><i class="fa fa-check"></i><b>12</b> Experience Rating Using Credibility Theory</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>12.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="12.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>12.2</b> Limited Fluctuation Credibility</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:frequency"><i class="fa fa-check"></i><b>12.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="12.2.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>12.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="12.2.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>12.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="12.2.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#partial-credibility"><i class="fa fa-check"></i><b>12.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Cred:Buhlmann"><i class="fa fa-check"></i><b>12.3</b> Bhlmann Credibility</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>12.3.1</b> Credibility <em>Z</em>, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#bhlmann-straub-credibility"><i class="fa fa-check"></i><b>12.4</b> Bhlmann-Straub Credibility</a></li>
<li class="chapter" data-level="12.5" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Cred:BayesInf"><i class="fa fa-check"></i><b>12.5</b> Bayesian Inference and Bhlmann Credibility</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#Sec:Cred:gammaPoisson"><i class="fa fa-check"></i><b>12.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="12.5.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#beta-binomial-model"><i class="fa fa-check"></i><b>12.5.2</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="12.5.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#exact-credibility"><i class="fa fa-check"></i><b>12.5.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="ChapCredibility.html"><a href="ChapCredibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>12.6</b> Estimating Credibility Parameters</a>
<ul>
<li class="chapter" data-level="12.6.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>12.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="12.6.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#nonparametric-estimation-for-bhlmann-and-bhlmann-straub-models"><i class="fa fa-check"></i><b>12.6.2</b> Nonparametric Estimation for Bhlmann and Bhlmann-Straub Models</a></li>
<li class="chapter" data-level="12.6.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#semiparametric-estimation-for-bhlmann-and-bhlmann-straub-models"><i class="fa fa-check"></i><b>12.6.3</b> Semiparametric Estimation for Bhlmann and Bhlmann-Straub Models</a></li>
<li class="chapter" data-level="12.6.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>12.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="ChapCredibility.html"><a href="ChapCredibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>12.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html"><i class="fa fa-check"></i><b>13</b> Insurance Portfolio Management including Reinsurance</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#introduction-to-insurance-portfolios"><i class="fa fa-check"></i><b>13.1</b> Introduction to Insurance Portfolios</a></li>
<li class="chapter" data-level="13.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Tails"><i class="fa fa-check"></i><b>13.2</b> Tails of Distributions</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>13.2.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="13.2.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>13.2.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>13.3</b> Risk Measures</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>13.3.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="13.3.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>13.3.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="13.3.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>13.3.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>13.4</b> Reinsurance</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>13.4.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="13.4.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>13.4.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="13.4.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>13.4.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#further-resources-and-contributors-1"><i class="fa fa-check"></i><b>13.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html"><i class="fa fa-check"></i><b>14</b> Loss Reserving</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:motivation"><i class="fa fa-check"></i><b>14.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:claim-types"><i class="fa fa-check"></i><b>14.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="14.1.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#why-reserving"><i class="fa fa-check"></i><b>14.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Data"><i class="fa fa-check"></i><b>14.2</b> Loss Reserve Data</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#from-micro-to-macro"><i class="fa fa-check"></i><b>14.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="14.2.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#run-off-triangles"><i class="fa fa-check"></i><b>14.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="14.2.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#loss-reserve-notation"><i class="fa fa-check"></i><b>14.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="14.2.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Rcode"><i class="fa fa-check"></i><b>14.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Chain-ladder"><i class="fa fa-check"></i><b>14.3</b> The Chain-Ladder Method</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:DeterministicCL"><i class="fa fa-check"></i><b>14.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="14.3.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#macks-distribution-free-chain-ladder-model"><i class="fa fa-check"></i><b>14.3.2</b> Macks Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="14.3.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#r-code-for-chain-ladder-predictions"><i class="fa fa-check"></i><b>14.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:GLMs"><i class="fa fa-check"></i><b>14.4</b> GLMs and Bootstrap for Loss Reserves</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-specification"><i class="fa fa-check"></i><b>14.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="14.4.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>14.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="14.4.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#bootstrap"><i class="fa fa-check"></i><b>14.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#LossRe:further-reading-and-resources"><i class="fa fa-check"></i><b>14.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html"><i class="fa fa-check"></i><b>15</b> Experience Rating using Bonus-Malus</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:Intro"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:NCD"><i class="fa fa-check"></i><b>15.2</b> <em>NCD</em> System in Several Countries</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#ncd-system-in-malaysia"><i class="fa fa-check"></i><b>15.2.1</b> <em>NCD</em> System in Malaysia</a></li>
<li class="chapter" data-level="15.2.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#ncd-systems-in-other-countries"><i class="fa fa-check"></i><b>15.2.2</b> NCD Systems in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:BMS"><i class="fa fa-check"></i><b>15.3</b> <em>BMS</em> and Markov Chain Model</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>15.3.1</b> Transition Probability</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:StatDist"><i class="fa fa-check"></i><b>15.4</b> <em>BMS</em> and Stationary Distribution</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>15.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="15.4.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-code-for-a-stationary-distribution"><i class="fa fa-check"></i><b>15.4.2</b> <code>R</code> Code for a Stationary Distribution</a></li>
<li class="chapter" data-level="15.4.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>15.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="15.4.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-program-for-premium-evolution"><i class="fa fa-check"></i><b>15.4.4</b> <code>R</code> Program for Premium Evolution</a></li>
<li class="chapter" data-level="15.4.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>15.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="15.4.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-program-for-convergence-rate"><i class="fa fa-check"></i><b>15.4.6</b> <code>R</code> Program for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:PremRtg"><i class="fa fa-check"></i><b>15.5</b> <em>BMS</em> and Premium Rating</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>15.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="15.5.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#a-priori-risk-classification"><i class="fa fa-check"></i><b>15.5.2</b> A Priori Risk Classification</a></li>
<li class="chapter" data-level="15.5.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#modelling-of-residual-heterogeneity"><i class="fa fa-check"></i><b>15.5.3</b> Modelling of Residual Heterogeneity</a></li>
<li class="chapter" data-level="15.5.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution-allowing-for-residual-heterogeneity"><i class="fa fa-check"></i><b>15.5.4</b> Stationary Distribution Allowing for Residual Heterogeneity</a></li>
<li class="chapter" data-level="15.5.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#determination-of-optimal-relativities"><i class="fa fa-check"></i><b>15.5.5</b> Determination of Optimal Relativities</a></li>
<li class="chapter" data-level="15.5.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#numerical-illustrations"><i class="fa fa-check"></i><b>15.5.6</b> Numerical Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Further"><i class="fa fa-check"></i><b>15.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="15.6.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>15.6.1</b> Further Reading and References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html"><i class="fa fa-check"></i><b>16</b> Dependence Modeling</a>
<ul>
<li class="chapter" data-level="16.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>16.1</b> Multivariate Variables</a></li>
<li class="chapter" data-level="16.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>16.2</b> Classic Measures of Scalar Associations</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>16.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="16.2.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>16.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="16.2.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>16.2.3</b> Nominal Variables</a></li>
<li class="chapter" data-level="16.2.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ordinal-variables"><i class="fa fa-check"></i><b>16.2.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="16.2.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#interval-variables"><i class="fa fa-check"></i><b>16.2.5</b> Interval Variables</a></li>
<li class="chapter" data-level="16.2.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#discrete-and-continuous-variables"><i class="fa fa-check"></i><b>16.2.6</b> Discrete and Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>16.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="16.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>16.4</b> Application Using Copulas</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#data-description"><i class="fa fa-check"></i><b>16.4.1</b> Data Description</a></li>
<li class="chapter" data-level="16.4.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>16.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="16.4.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>16.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="16.4.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>16.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>16.5</b> Types of Copulas</a>
<ul>
<li class="chapter" data-level="16.5.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#normal-gaussian-copulas"><i class="fa fa-check"></i><b>16.5.1</b> Normal (Gaussian) Copulas</a></li>
<li class="chapter" data-level="16.5.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#t--and-elliptical-copulas"><i class="fa fa-check"></i><b>16.5.2</b> <em>t</em>- and Elliptical Copulas</a></li>
<li class="chapter" data-level="16.5.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#archimedean-copulas"><i class="fa fa-check"></i><b>16.5.3</b> Archimedean Copulas</a></li>
<li class="chapter" data-level="16.5.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>16.5.4</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>16.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="16.7" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>16.7</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ts-15.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 15.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="CAppA.html"><a href="CAppA.html"><i class="fa fa-check"></i><b>17</b> Appendix A: Review of Statistical Inference</a>
<ul>
<li class="chapter" data-level="17.1" data-path="CAppA.html"><a href="CAppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>17.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="CAppA.html"><a href="CAppA.html#random-sampling"><i class="fa fa-check"></i><b>17.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="17.1.2" data-path="CAppA.html"><a href="CAppA.html#sampling-distribution"><i class="fa fa-check"></i><b>17.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="17.1.3" data-path="CAppA.html"><a href="CAppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>17.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="CAppA.html"><a href="CAppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>17.2</b> Point Estimation and Properties</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="CAppA.html"><a href="CAppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>17.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="17.2.2" data-path="CAppA.html"><a href="CAppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>17.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="CAppA.html"><a href="CAppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>17.3</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="CAppA.html"><a href="CAppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>17.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="17.3.2" data-path="CAppA.html"><a href="CAppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>17.3.2</b> Large-sample Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="17.3.3" data-path="CAppA.html"><a href="CAppA.html#confidence-interval"><i class="fa fa-check"></i><b>17.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>17.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="17.4.1" data-path="CAppA.html"><a href="CAppA.html#basic-concepts"><i class="fa fa-check"></i><b>17.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="17.4.2" data-path="CAppA.html"><a href="CAppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>17.4.2</b> Student-<em>t</em> test based on <em>mle</em></a></li>
<li class="chapter" data-level="17.4.3" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>17.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="17.4.4" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>17.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="CAppB.html"><a href="CAppB.html"><i class="fa fa-check"></i><b>18</b> Appendix B: Iterated Expectations</a>
<ul>
<li class="chapter" data-level="18.1" data-path="CAppB.html"><a href="CAppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>18.1</b> Conditional Distribution and Conditional Expectation</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="CAppB.html"><a href="CAppB.html#conditional-distribution"><i class="fa fa-check"></i><b>18.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="18.1.2" data-path="CAppB.html"><a href="CAppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>18.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="CAppB.html"><a href="CAppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>18.2</b> Iterated Expectations and Total Variance</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="CAppB.html"><a href="CAppB.html#S:AppB:LIE"><i class="fa fa-check"></i><b>18.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="18.2.2" data-path="CAppB.html"><a href="CAppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>18.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="18.2.3" data-path="CAppB.html"><a href="CAppB.html#application"><i class="fa fa-check"></i><b>18.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="CAppB.html"><a href="CAppB.html#S:AppConjugateDistributions"><i class="fa fa-check"></i><b>18.3</b> Conjugate Distributions</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="CAppB.html"><a href="CAppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>18.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="18.3.2" data-path="CAppB.html"><a href="CAppB.html#S:IterExp:Conjugate"><i class="fa fa-check"></i><b>18.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="CAppC.html"><a href="CAppC.html"><i class="fa fa-check"></i><b>19</b> Appendix C: Maximum Likelihood Theory</a>
<ul>
<li class="chapter" data-level="19.1" data-path="CAppC.html"><a href="CAppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>19.1</b> Likelihood Function</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="CAppC.html"><a href="CAppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>19.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="19.1.2" data-path="CAppC.html"><a href="CAppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>19.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="CAppC.html"><a href="CAppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>19.2</b> Maximum Likelihood Estimators</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="CAppC.html"><a href="CAppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>19.2.1</b> Definition and Derivation of <em>MLE</em></a></li>
<li class="chapter" data-level="19.2.2" data-path="CAppC.html"><a href="CAppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>19.2.2</b> Asymptotic Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="19.2.3" data-path="CAppC.html"><a href="CAppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>19.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="CAppC.html"><a href="CAppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>19.3</b> Statistical Inference Based on Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="19.3.1" data-path="CAppC.html"><a href="CAppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>19.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="19.3.2" data-path="CAppC.html"><a href="CAppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>19.3.2</b> <em>MLE</em> and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html"><i class="fa fa-check"></i><b>20</b> Appendix D: Summary of Distributions</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:DiscreteDistributions"><i class="fa fa-check"></i><b>20.1</b> Discrete Distributions</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>20.1.1</b> The <em>(a,b,0)</em> Class</a></li>
<li class="chapter" data-level="20.1.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>20.1.2</b> The <em>(a,b,1)</em> Class</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:ContinuousDistributions"><i class="fa fa-check"></i><b>20.2</b> Continuous Distributions</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>20.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="20.2.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>20.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="20.2.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>20.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="20.2.4" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>20.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="20.2.5" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>20.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="20.2.6" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>20.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>20.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html"><i class="fa fa-check"></i><b>21</b> Appendix E: Conventions for Notation</a>
<ul>
<li class="chapter" data-level="21.1" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:General"><i class="fa fa-check"></i><b>21.1</b> General Conventions</a></li>
<li class="chapter" data-level="21.2" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Abbreviations"><i class="fa fa-check"></i><b>21.2</b> Abbreviations</a></li>
<li class="chapter" data-level="21.3" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:StatSymbols"><i class="fa fa-check"></i><b>21.3</b> Common Statistical Symbols and Operators</a></li>
<li class="chapter" data-level="21.4" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Symbols"><i class="fa fa-check"></i><b>21.4</b> Common Mathematical Symbols and Functions</a></li>
<li class="chapter" data-level="21.5" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#further-readings"><i class="fa fa-check"></i><b>21.5</b> Further Readings</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="DataResources.html"><a href="DataResources.html"><i class="fa fa-check"></i><b>22</b> Appendix. Data Resources</a>
<ul>
<li class="chapter" data-level="22.1" data-path="DataResources.html"><a href="DataResources.html#S:WiscPropFundA"><i class="fa fa-check"></i><b>22.1</b> Wisconsin Property Fund</a></li>
<li class="chapter" data-level="22.2" data-path="DataResources.html"><a href="DataResources.html#Sec:DataTravel"><i class="fa fa-check"></i><b>22.2</b> ANU Corporate Travel Data</a></li>
<li class="chapter" data-level="22.3" data-path="DataResources.html"><a href="DataResources.html#Sec:DataGPA"><i class="fa fa-check"></i><b>22.3</b> ANU Group Personal Accident Data</a></li>
<li class="chapter" data-level="22.4" data-path="DataResources.html"><a href="DataResources.html#Sec:DataAuto"><i class="fa fa-check"></i><b>22.4</b> ANU Motor Vehicle Data</a></li>
<li class="chapter" data-level="22.5" data-path="DataResources.html"><a href="DataResources.html#spanish-personal-insurance-data"><i class="fa fa-check"></i><b>22.5</b> Spanish Personal Insurance Data</a></li>
<li class="chapter" data-level="22.6" data-path="DataResources.html"><a href="DataResources.html#r-package-casdatasets"><i class="fa fa-check"></i><b>22.6</b> R Package CASdatasets</a></li>
<li class="chapter" data-level="22.7" data-path="DataResources.html"><a href="DataResources.html#other-data-sources"><i class="fa fa-check"></i><b>22.7</b> Other Data Sources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ChBayes" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Bayesian Inference and Modeling<a href="ChBayes.html#ChBayes" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview.</em> Up to this point in the book, we have focused almost exclusively on the frequentist approach to estimate our various loss distribution parameters. In this chapter, we switch gears and discuss a different paradigm: Bayesianism. These approaches are different as Bayesian and frequentist statisticians disagree on the source of the uncertainty: Bayesian statistics assumes that the observed data sample is fixed and that model parameters are random, whereas frequentism considers the opposite (i.e., a random sample but fixedyet unknownmodel parameters).</p>
<p>In this chapter, we introduce Bayesian inference and modeling with a particular focus on loss data analytics. We begin in Section <a href="ChBayes.html#ChBayes:SecIntro">9.1</a> by explaining the basics of Bayesian statistics: we compare it to frequentism and provide some historical context for the paradigm. We also introduce the seminal Bayes rule that serves as a key component in Bayesian statistics. Then, building on this, we present the main ingredients of Bayesian inference in Section <a href="ChBayes.html#ChBayes:SecBuildingBlocks">9.2</a>: the posterior distribution, the likelihood function, and the prior distribution. Section <a href="ChBayes.html#ChBayes:SecConjugate">9.3</a> provides some examples of simple cases where the prior distribution is chosen for algebraic convenience, giving rise to a closed-form expression for the posterior; these are called conjugate families in the literature. Finally, the last section of this chapter, Section <a href="ChBayes.html#ChBayes:SecPosterior">9.4</a>, is dedicated to cases where we cannot get closed-form expressions and for which numerical integration is needed. Specifically, we discuss two influential Markov chain Monte Carlo samplers: the Gibbs sampler and the MetropolisHastings algorithm. We also discuss how to interpret the chains obtained by these methods (i.e., Markov chain diagnostics).</p>
<div id="ChBayes:SecIntro" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> A Gentle Introduction to Bayesian Statistics<a href="ChBayes.html#ChBayes:SecIntro" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#ChBayes:SecIntro">9.1</a>, you learn how to:</p>
<ul>
<li>Describe qualitatively Bayesianism as an alternative to the frequentist approach.</li>
<li>Give the historical context for Bayesian statistics.</li>
<li>Use Bayes rule to find conditional probabilities.</li>
<li>Understand the basics of Bayesian statistics.</li>
</ul>
<hr />
<div id="ChBayes:SubSecBayesVsFreq" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> Bayesian versus Frequentist Statistics<a href="ChBayes.html#ChBayes:SubSecBayesVsFreq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classic frequentist statistics rely on frequentist probabilityan interpretation of probability in which an events probability is defined as the limit of its relative frequency (or propensity) in many, repeatable trials. It draws conclusion from a sample that is one of many hypothetical datasets that could have been collected; the uncertainty is therefore due to the sampling error associated with the sample, while model parameters and various quantities of interest are fixed (but unknown to the experimenter).</p>
<hr />
<p><strong>Example 8.1.1. Coin Toss.</strong> Considering the simple case of coin tossing, if we flip a fair coin many times, we expect to see heads about 50% of the time. If we flip the coin only a few times, however, we could see a different distribution just by chance. Indeed, there is a non-zero probability of observing all heads if the sample is small enough. Figure <a href="ChBayes.html#fig:ChBayesHistogram">9.1</a> illustrates this very fact by showing the number of heads observed in 100 samples of five iid tosses; in this specific example, we observe six samples for which all tosses are heads.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesHistogram"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesHistogram-1.png" alt="Frequency histogram of the number of heads in a sample of five data points" width="80%" />
<p class="caption">
Figure 9.1: <strong>Frequency histogram of the number of heads in a sample of five data points</strong>
</p>
</div>
<p>Yet, as the sample size increases, the relative frequency of heads should get closer to 50% if the coin is fair. Figure <a href="ChBayes.html#fig:ChBayesConvergence">9.2</a> reports that, if the number of tosses increases, then relative frequency of heads gets closer to 0.5the probability of seeing heads on a given coin toss. In other words, increasing the sample size makes the sample less uncertain, and the experimenter should be reaching a probability of 0.5 in the limit, assuming they can reproduce the experiment an infinite number of times.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesConvergence"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesConvergence-1.png" alt="Cumulative relative frequencies of heads for an increasing sample size" width="80%" />
<p class="caption">
Figure 9.2: <strong>Cumulative relative frequencies of heads for an increasing sample size</strong>
</p>
</div>
<hr />
<p>Bayesians see things differently: they interpret probabilities as degrees of certainty about some quantity of interest. To find such probabilities, they draw on prior knowledge about those quantities, expressing ones beliefs before some data are taken into account. Then, as data are collected, knowledge about the world is updated, allowing us to incorporate such new information is a consistent manner; the resulting quantity is referred to as the posterior, which summarizes the information in both the prior and the data.</p>
<p>In the context of Bayesian inference and modeling, this interpretation of probability implies that model parameters are assumed to be random variablesunlike the frequentist approach that considers them fixed. Starting from the prior distribution, the datasummarized via the likelihood functionare used to update the prior distribution and create a posterior distribution of the parameters (see Section <a href="ChBayes.html#ChBayes:SecBuildingBlocks">9.2</a> for more details on the posterior distribution, the likelihood function, and the prior distribution). The influence of the prior distribution on the posterior distribution becomes weaker as the size of the observed data sample increases: the prior information is less and less relevant as new information comes in.</p>
<hr />
<p><strong>Example 8.1.1. Coin Toss, continued.</strong> We now reconsider the coin tossing experiment above through a Bayesian lens. Let us first assume that we have a (potentially unfair) coin, and we wish to understand the probability of obtaining heads, denoted by <span class="math inline">\(q\)</span> in this example. Consistent with the Bayesian paradigm, this parameter is random; let us assume that the random variable associated with the probability of observing heads denoted by <span class="math inline">\(Q\)</span>. For simplicity, we assume that we do not have prior information on the specific coin under investigation.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Assuming again that our sample contains only five iid tosses, we know that the probability of observing <span class="math inline">\(x\)</span> heads is given by the binomial distribution with <span class="math inline">\(m=5\)</span> such that</p>
<p><span class="math display">\[
p_{X|Q=q}(x) = \mathrm{Pr}(X = x \, | \, Q = q) = \binom{5}{x} q^x (1-q)^{5-x}, \quad x \in \{0,1,...,5\},
\]</span>
where <span class="math inline">\(0 \leq q \leq 1\)</span>, which emphasizes the fact that this probability depends on parameter <span class="math inline">\(q\)</span> by explicitly conditioning on it (unlike the notation used so far in this book, note that we append subscripts to the various pdf and pmf in this chapter to denote the random variables under study; this additional notation allows us to consider pdf and pmf of different random variables in the same problem).</p>
<p>Let us generate a sample of these five tosses:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="ChBayes.html#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb133-2"><a href="ChBayes.html#cb133-2" aria-hidden="true" tabindex="-1"></a>nbheads <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>)</span>
<span id="cb133-3"><a href="ChBayes.html#cb133-3" aria-hidden="true" tabindex="-1"></a>num_flips <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb133-4"><a href="ChBayes.html#cb133-4" aria-hidden="true" tabindex="-1"></a>coin <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;heads&quot;</span>, <span class="st">&quot;tails&quot;</span>)</span>
<span id="cb133-5"><a href="ChBayes.html#cb133-5" aria-hidden="true" tabindex="-1"></a>flips <span class="ot">&lt;-</span> <span class="fu">sample</span>(coin, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb133-6"><a href="ChBayes.html#cb133-6" aria-hidden="true" tabindex="-1"></a>nbheads <span class="ot">&lt;-</span> <span class="fu">sum</span>(flips <span class="sc">==</span> <span class="st">&quot;heads&quot;</span>)</span>
<span id="cb133-7"><a href="ChBayes.html#cb133-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Number of heads:&quot;</span>, nbheads)</span></code></pre></div>
<pre><code>Number of heads: 3</code></pre>
<p>Based on this simulation, we obtain a data sample that contains three heads and two tails. Therefore, using Bayesian statistics, we can show that</p>
<p><span class="math display">\[
f_{Q|X=3}(q) \, \propto \, q^3 (1-q)^{2},
\]</span>
where <span class="math inline">\(\propto\)</span> means proportional to (note that obtaining this equation requires some tools that will be introduced in Section <a href="ChBayes.html#ChBayes:SecBuildingBlocks">9.2</a>).<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> Figure <a href="ChBayes.html#fig:ChBayesBayesianToss">9.3</a> illustrates this pdf and reports the uncertainty about parameter <span class="math inline">\(q\)</span> based on this sample of five data points. In this example, one can see that the uncertainty is quite large; this is a by-product of using only five data points. Indeed, based on these five observations, one could argue that the probability should be close to <span class="math inline">\(\frac{3}{5}=0.6\)</span>. This Bayesian analysis shows that 0.6 is likely, but that it is also very uncertaina conclusion that is not direct in the frequentist approach.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesBayesianToss"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesBayesianToss-1.png" alt="Posterior probability density function of the parameter \(q\) for a sample of five data points" width="80%" />
<p class="caption">
Figure 9.3: <strong>Posterior probability density function of the parameter <span class="math inline">\(q\)</span> for a sample of five data points</strong>
</p>
</div>
<p>Figure <a href="ChBayes.html#fig:ChBayesBayesianTossAnimation">9.4</a> reports the analog of Figure <a href="ChBayes.html#fig:ChBayesConvergence">9.2</a> through a Bayesian lens: we see the evolution of the posterior density of parameter <span class="math inline">\(q\)</span> as a function of the sample size for the same sample used in Figure <a href="ChBayes.html#fig:ChBayesConvergence">9.2</a>. As we obtain more evidence, the posterior density becomes more concentrated around 0.5a consequence of using a fair coin in the simulations above. Yet, even if the sample size if 1,000, we still see some parameter uncertainty.</p>

<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="ChBayes.html#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="at">file =</span> <span class="st">&quot;IntermediateCalcs/BayesChap/Example811Animate.Rdata&quot;</span>)</span>
<span id="cb135-2"><a href="ChBayes.html#cb135-2" aria-hidden="true" tabindex="-1"></a><span class="co"># load(file= &#39;../IntermediateCalcs/BayesChap/Example811Animate.Rdata&#39;)</span></span>
<span id="cb135-3"><a href="ChBayes.html#cb135-3" aria-hidden="true" tabindex="-1"></a><span class="fu">animate</span>(animp, <span class="at">fps =</span> <span class="dv">5</span>, <span class="at">nframes =</span> <span class="dv">200</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesBayesianTossAnimation"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesBayesianTossAnimation-1.gif" alt="Posterior probability density function of parameter \(q\) as a function of the sample size" width="80%" />
<p class="caption">
Figure 9.4: <strong>Posterior probability density function of parameter <span class="math inline">\(q\)</span> as a function of the sample size</strong>
</p>
</div>
<hr />
<p><strong>But why be Bayesian?</strong> There are indeed several advantages to the Bayesian approach. First, this approach allows us to describe the entire distribution of parameters conditional on the data. This allows us, for example, to provide probability statements regarding the parameters that could be interpreted as such. Second, it provides a unified approach for estimating parameters. Some non-Bayesian methods, such as least squares, require a separate approach to estimate variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. Third, it allows experimenters to blend prior information from other sources with the data in a coherent manner.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></p>
</div>
<div id="a-brief-history-lesson" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> A Brief History Lesson<a href="ChBayes.html#a-brief-history-lesson" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Interestingly, some have argued that the birth of Bayesian statistics is intimately related to insurance; see, for instance, <span class="citation">Cowles (<a href="#ref-cowles2013applied" role="doc-biblioref">2013</a>)</span>. Specifically, the Great Fire of London in 1666destroying more than 10,000 homes and about 100 churchesled to the rise of insurance as we know it today. Shortly after, the first full-fledged fire insurance company came into existence in England during the 1680s. By the turn of the century, the idea of insurance was well ingrained and its use was booming in England. Yet, the lack of statistical models and methodsmuch needed to understand riskdrove some insurers to bankruptcy.</p>
<p>Thomas Bayes, an English statistician, philosopher and Presbyterian minister, applied his mind to some of these important statistical questions raised by insurers. This culminated into Bayes theory of probability in his seminar essay entitled <em>Essay towards solving a problem in the doctrine of chances</em>, published posthumously in 1763. This essay laid out the foundation of what we now know as Bayesian statistics.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ThomasBayes"></span>
<img src="Figures/ThomasBayes.png" alt="Portrait of an unknown Presbyterian clergyman identified as Thomas Bayes in ODonnell (1936)" width="33%" />
<p class="caption">
Figure 9.5: <strong>Portrait of an unknown Presbyterian clergyman identified as Thomas Bayes in <span class="citation">ODonnell (<a href="#ref-o1936history" role="doc-biblioref">1936</a>)</span></strong>
</p>
</div>
<p>Thomas Bayes work also helped Pierre-Simon Laplace, a famous French scholar and polymath, to develop and popularize the Bayesian interpretation of probability in the late 1700s and early 1800s. He also moved beyond Bayes essay and generalized his framework. Laplaces efforts were followed by many, and Bayesian thinking continued to progress throughout the years with the help of statisticians like Bruno de Finetti, Harold Jeffreys, Dennis Lindley, and Leonard Jimmie Savage.</p>
<p>Nowadays, Bayesian statistics and modeling is widely used in science, thanks to the increase in computational power over the past 30 years. Actuarial science and loss modeling, more specifically, have also been breeding grounds for Bayesian methodology. So, Bayesian statistics circles back to insurance, in a sense, where it all started.</p>
</div>
<div id="ChBayes:SubsecBayesRule" class="section level3 hasAnchor" number="9.1.3">
<h3><span class="header-section-number">9.1.3</span> Bayes Rule<a href="ChBayes.html#ChBayes:SubsecBayesRule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This subsection introduces how the Bayes rule is applied to calculating conditional probability for events.</p>
<p><strong>Conditional Probability.</strong> The concept of conditional probability considers the relationship between probabilities of two (or more) events happening. In its most simple form, being interested in conditional probability boils down to answering this question: <em>given that event <span class="math inline">\(B\)</span> happened, how does this affect the probability that <span class="math inline">\(A\)</span> happens?</em> To answer this question, we can define formally the concept of conditional probability:</p>
<p><span class="math display">\[
\Pr\left(A \, \middle| \, B\right) = \frac{\Pr(A \cap B)}{\Pr(B)}.
\]</span>
To be properly defined, we must assume that <span class="math inline">\(\Pr(B)\)</span> is larger than zero; that is, event <span class="math inline">\(B\)</span> is not impossible. Simply put, a conditional probability turns <span class="math inline">\(B\)</span> into the new probability space, and then cares only about the part of <span class="math inline">\(A\)</span> that is inside <span class="math inline">\(B\)</span> (i.e., <span class="math inline">\(A \cap B\)</span>).</p>
<hr />
<p><strong>Example 8.1.2. Actuarial Exam Question.</strong>
An insurance company estimates that 40% of policyholders who have an extended health policy will renew next year, and 70% of policyholders who have a long-term disability policy will renew next year. The company also estimates that 50% of their clients who have both policies will renew at least one next year. The company records report that 65% of clients have an extended health policy, 40% have a long-term disability policy, and 10% have both. Using the data above, calculate the percentage of policyholders that will review at least one policy next year.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.1.2" href="javascript:toggleEX('toggleExample.8.1.2','displayExample.8.1.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.1.2" style="display: none">
<p><strong>Solution.</strong>
Let <span class="math inline">\(E\)</span> be the event that a policyholder has an extended health policy, <span class="math inline">\(D\)</span> be the event that a policyholder has a long-term disability policy, and <span class="math inline">\(R\)</span> be the event that a policyholder renews a policy. We are given:</p>
<ul>
<li><span class="math inline">\(\Pr(E)=0.65\)</span>,</li>
<li><span class="math inline">\(\Pr(D)=0.40\)</span>,</li>
<li><span class="math inline">\(\Pr(E \cap D)=0.10\)</span>,</li>
<li><span class="math inline">\(\Pr(R \,| \,E\cap D^{\text{c}})=0.40\)</span>,</li>
<li><span class="math inline">\(\Pr(R \,| \,E^{\text{c}}\cap D)=0.70\)</span>,</li>
<li><span class="math inline">\(\Pr(R\,| \,E\cap D)=0.50\)</span>.</li>
</ul>
<p>We are looking for <span class="math inline">\(\Pr(R)\)</span>.</p>
<p>Note that
<span class="math display">\[
\Pr(E\cap D^{\text{c}}) = \Pr(E) - \Pr(E \cap D) = 0.65 - 0.10 = 0.55,
\]</span>
and
<span class="math display">\[
\Pr(E^{\text{c}}\cap D) = \Pr(D) - \Pr(E \cap D) = 0.40 - 0.10 = 0.30.
\]</span>
Moreover, note that <span class="math inline">\(E\cap D^{\text{c}}\)</span>, <span class="math inline">\(E^{\text{c}}\cap D\)</span>, and <span class="math inline">\(E\cap D\)</span> are mutually disjoint, and that</p>
<p><span class="math display">\[\begin{align}
\Pr(R) = &amp; \, \Pr( R \cap (E \cap D^{\text{c}}) ) + \Pr( R \cap (E^{\text{c}} \cap D) ) + \Pr( R \cap (E \cap D) ) \\
= &amp; \, \Pr( R \, |\, (E \cap D^{\text{c}}) ) \Pr(E \cap D^{\text{c}}) + \Pr( R  \, |\, (E^{\text{c}} \cap D) ) \Pr(E^{\text{c}} \cap D) \\
&amp;\, + \Pr( R  \, |\, (E \cap D) ) \Pr(E \cap D) \\
= &amp;\, 0.40 \times 0.55 + 0.70 \times 0.30 + 0.50 \times 0.10 \\
= &amp;\, 0.48.
\end{align}\]</span></p>
</div>
<hr />
<p><strong>Independence.</strong> If two events are unrelated to one another, we say that they are independent. Specifically, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if</p>
<p><span class="math display">\[
\Pr(A \cap B) = \Pr(A) \, \Pr(B).
\]</span>
For positive probability events, independence between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is also equivalent to</p>
<p><span class="math display">\[
\Pr(A\,|\, B) = \Pr(A) \quad \text{and} \quad \Pr(B\,|\, A) = \Pr(B),
\]</span>
which means that the occurrence of event <span class="math inline">\(B\)</span> does not have an impact on the occurence of <span class="math inline">\(A\)</span>, and vice versa.</p>
<p><strong>Bayes Rule.</strong> Intuitively speaking, Bayes rule provides a mechanism to put our Bayesian thinking into practice. It allows us to update our information by combining the datafrom the likelihoodand the prior together to obtain a posterior probability.</p>
<hr />
<p><strong>Proposition 8.1.1. Bayes Rule for Events.</strong>
For events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, the posterior probability of event <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> follows</p>
<p><span class="math display">\[
\Pr(A\,|\,B) = \frac{\Pr(B\, |\, A) \Pr(A)}{\Pr(B)},
\]</span>
where the law of total probability allows us to find</p>
<p><span class="math display">\[
\Pr(B) = \Pr(A) \Pr(B \, | \, A) + \Pr(A^{\text{c}}) \Pr(B \, |\, A^{\text{c}}).
\]</span>
Note, again, that this works as long as event <span class="math inline">\(B\)</span> is possible (i.e., <span class="math inline">\(\Pr(B) &gt; 0\)</span>).<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.1" href="javascript:toggleTheory('toggleTheory.Theory.1','displayCode.Theory.1');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.1" style="display: none">
<p><strong>Proof.</strong>
Bayes rule may be derived from the definition of conditional probability shown above:</p>
<p><span class="math display">\[
\Pr(A\, |\, B) = \frac{\Pr(A \cap B)}{\Pr(B)}
\]</span></p>
<p>if <span class="math inline">\(\Pr(B) &gt; 0\)</span>. Similarly,</p>
<p><span class="math display">\[
\Pr(B\, |\, A) = \frac{\Pr(A \cap B)}{\Pr(A)}
\]</span></p>
<p>if <span class="math inline">\(\Pr(A) &gt; 0\)</span>. Solving for <span class="math inline">\(\Pr(A \cap B)\)</span> in the last equation and substituting into the first one yields Bayes rule:</p>
<p><span class="math display">\[
\Pr(A\,|\,B) = \frac{\Pr(B\, |\, A) \Pr(A)}{\Pr(B)}.
\]</span></p>
</div>
<hr />
<p>Simply put, the posterior probability of event <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is obtained by combining the likelihood of <span class="math inline">\(B\)</span> given a fixed <span class="math inline">\(A\)</span>proxied by <span class="math inline">\(\Pr(B\, |\, A)\)</span>with the prior probability of observing <span class="math inline">\(A\)</span>, and then dividing it by the marginal probability of event <span class="math inline">\(B\)</span> to make sure that the probabilities sum up to one.</p>
<hr />
<p><strong>Example 8.1.3. Actuarial Exam Question.</strong>
An automobile insurance company insures drivers of all ages. An actuary compiled the following statistics on the companys insured drivers:</p>
<p><span class="math display">\[
\begin{matrix}
    \begin{array}{c|c|c} \hline
    \text{Age of Driver} &amp; \text{Probability of Accident} &amp; \text{Portion of Company&#39;s } \\
    &amp; &amp; \text{Insured Driver} \\\hline
    \text{16-20} &amp; 0.06 &amp; 0.08 \\\hline
    \text{21-30} &amp; 0.03 &amp; 0.15 \\\hline
    \text{31-65} &amp; 0.02 &amp; 0.49 \\\hline
    \text{66-99} &amp; 0.04 &amp; 0.28 \\\hline
    \end{array}
\end{matrix}
\]</span></p>
<p>A randomly selected driver that the company insures has an accident. Calculate the probability that the driver was age 16-20.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.1.3" href="javascript:toggleEX('toggleExample.8.1.3','displayExample.8.1.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.1.3" style="display: none">
<p><strong>Solution.</strong>
Let <span class="math inline">\(B\)</span> be the event of an insured driver having an accident, and let</p>
<ul>
<li><span class="math inline">\(A_1\)</span> be the event related to the drivers age being in the range 16-20,</li>
<li><span class="math inline">\(A_2\)</span> be the event related to the drivers age being in the range 21-30,</li>
<li><span class="math inline">\(A_3\)</span> be the event related to the drivers age being in the range 31-65,</li>
<li><span class="math inline">\(A_4\)</span> be the event related to the drivers age being in the range 66-99.</li>
</ul>
<p>Then,</p>
<p><span class="math display">\[\begin{align}
\Pr(A_1\, |\, B) = &amp;\, \frac{\Pr(B\, |\, A_1) \Pr(A_1)}{\Pr(B\, |\, A_1) \Pr(A_1)+\Pr(B\, |\, A_2) \Pr(A_2)+\Pr(B\, |\, A_3) \Pr(A_3)+\Pr(B\, |\, A_4) \Pr(A_4)} \\
=&amp;\, \frac{0.06\times 0.08}{0.06\times 0.08 + 0.03 \times 0.15 + 0.02 \times 0.49 + 0.04\times 0.28} \\
=&amp;\, 0.1584.
\end{align}\]</span></p>
</div>
</div>
<div id="an-introductory-example-of-bayes-rule" class="section level3 hasAnchor" number="9.1.4">
<h3><span class="header-section-number">9.1.4</span> An Introductory Example of Bayes Rule<a href="ChBayes.html#an-introductory-example-of-bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The example above illustrates how to use Bayes rule in an academic context; the focus of this book is, nonetheless, data analytics. We therefore also wish to illustrate Bayes rule by using <em>real</em> data. In this introductory example, we use the Singapore auto data <code>sgautonb</code> of the R package <code>CASdatasets</code> that was already used in Chapter <a href="ChapFrequency-Modeling.html#ChapFrequency-Modeling">3</a> (see also Section <a href="#S:data-CASdatasets"><strong>??</strong></a> for more details).</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="ChBayes.html#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(CASdatasets)</span>
<span id="cb136-2"><a href="ChBayes.html#cb136-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(sgautonb)</span></code></pre></div>
<p>This dataset contains information about the number of car accidents and some risk factors (i.e., the type of the vehicle insured, the age of the vehicle, the sex of the policyholder, and the age of the policyholder grouped into seven categories).<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<hr />
<p><strong>Example 8.1.4. Singapore Insurance Data.</strong>
A new insurance companytargeting an older segment of the populationestimates that 20% of their policyholders will be 65 years old and older. The actuaries working at the insurance company believes that the Singapore insurance dataset is credible to understand the accident occurrence of the new company. Based on this information, find the probability that a randomly selected driver having (at least) an accident is 65 years old and older for the new insurance company.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.1.4" href="javascript:toggleEX('toggleExample.8.1.4','displayExample.8.1.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.1.4" style="display: none">
<p><strong>Solution.</strong>
Let <span class="math inline">\(O\)</span> denote the event related to the policyholder being 65 years old and older (i.e., Age Category 6 in the dataset), and <span class="math inline">\(A\)</span> the event of a policyholder having at least an accident. Using Bayes rule, we have that</p>
<p><span class="math display">\[
\Pr(O\, |\, A) = \frac{\Pr(A\, |\, O)\Pr(O)}{\Pr(A)},
\]</span></p>
<p>where the prior probability <span class="math inline">\(\Pr(O)\)</span> is given by the problem statement: <span class="math inline">\(\Pr(O) = 0.20\)</span>. This implies that <span class="math inline">\(\Pr(O^{\text{c}}) = 1-0.20 = 0.80\)</span>. From the Singapore insurance data, we know that <span class="math inline">\(\Pr(A\, |\, O) = 0.1082803\)</span> and <span class="math inline">\(\Pr(A\, |\, O^{\text{c}}) = 0.06415506\)</span>, which allow us to use the law of total probability to obtain:</p>
<p><span class="math display">\[
\Pr(A) = \Pr(A\, |\, O)\Pr(O) + \Pr(A\, |\, O^{\text{c}})\Pr(O^{\text{c}}).
\]</span></p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="ChBayes.html#cb137-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(sgautonb<span class="sc">$</span>AgeCat)</span>
<span id="cb137-2"><a href="ChBayes.html#cb137-2" aria-hidden="true" tabindex="-1"></a>nO <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">==</span> <span class="dv">6</span>)</span>
<span id="cb137-3"><a href="ChBayes.html#cb137-3" aria-hidden="true" tabindex="-1"></a>nOc <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">!=</span> <span class="dv">6</span>)</span>
<span id="cb137-4"><a href="ChBayes.html#cb137-4" aria-hidden="true" tabindex="-1"></a>nAandO <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">==</span> <span class="dv">6</span> <span class="sc">&amp;</span> sgautonb<span class="sc">$</span>Clm_Count <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb137-5"><a href="ChBayes.html#cb137-5" aria-hidden="true" tabindex="-1"></a>nAandOc <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>AgeCat <span class="sc">!=</span> <span class="dv">6</span> <span class="sc">&amp;</span> sgautonb<span class="sc">$</span>Clm_Count <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb137-6"><a href="ChBayes.html#cb137-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-7"><a href="ChBayes.html#cb137-7" aria-hidden="true" tabindex="-1"></a>PAO <span class="ot">&lt;-</span> nAandO<span class="sc">/</span>nO</span>
<span id="cb137-8"><a href="ChBayes.html#cb137-8" aria-hidden="true" tabindex="-1"></a>PAOc <span class="ot">&lt;-</span> nAandOc<span class="sc">/</span>nOc</span>
<span id="cb137-9"><a href="ChBayes.html#cb137-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-10"><a href="ChBayes.html#cb137-10" aria-hidden="true" tabindex="-1"></a>POA <span class="ot">&lt;-</span> PAO <span class="sc">*</span> <span class="fl">0.2</span><span class="sc">/</span>(PAO <span class="sc">*</span> <span class="fl">0.2</span> <span class="sc">+</span> PAOc <span class="sc">*</span> <span class="fl">0.8</span>)</span>
<span id="cb137-11"><a href="ChBayes.html#cb137-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The probability that policyholder having accident is 65 years old and older is&quot;</span>,</span>
<span id="cb137-12"><a href="ChBayes.html#cb137-12" aria-hidden="true" tabindex="-1"></a>    POA)</span></code></pre></div>
<pre><code>The probability that policyholder having accident is 65 years old and older is 0.2967391</code></pre>
<p>The probability that a randomly selected driver has (at least) an accident is 65 years old and older is therefore about 29.7% for the new insurance company. Simply put, we started with an <em>a priori</em> probability of 20%, meaning that unconditionally, we should have about 20% of policyholders aged 65 years old and older, and that 20% of the policyholders should have at least one accident. Then, based on the observed data, this probability is updated to 29.7%: the data seem to imply that, of all people having accidents, there are more older policyholders than what we would have guessed just based on our prior assumption.</p>
</div>
<hr />
<p>In the next section, we will expand on the idea of Bayes rule and apply it to slightly more general cases involving random variables instead of events.</p>
</div>
</div>
<div id="ChBayes:SecBuildingBlocks" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Building Blocks of Bayesian Inference<a href="ChBayes.html#ChBayes:SecBuildingBlocks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#ChBayes:SecBuildingBlocks">9.2</a>, you learn how to:</p>
<ul>
<li>Describe the main components of Bayesian inference; that is, the posterior distribution, the likelihood function, and the prior distribution.</li>
<li>Summarize the different classes of prior used in practice.</li>
</ul>
<hr />
<p>Proposition 8.1.1 above deals with the elementary case of Bayes rule for events. Although this version of Bayes rule is useful to understand the foundation of Bayesian statistics, we will need slightly more general versions of it to achieve Bayesian inference. Specifically, Proposition 8.1.1 needs to be generalized to the case of random variables.</p>
<p>Let us first consider the case of discrete random variables. Assume <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are both discrete random variables that allow for the following joint pmf of</p>
<p><span class="math display">\[
p_{X,Y}(x,y) = \Pr(X=x \,\text{ and }\, Y=y)
\]</span></p>
<p>as well as the following marginal distributions for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[
p_X(x)= \Pr(X=x) = \sum_k p_{X,Y}(x,k) \quad \text{and} \quad p_Y(y)= \Pr(Y=y) = \sum_k p_{X,Y}(k,y),
\]</span></p>
<p>respectively. Using the result of Proposition 8.1.1 and setting event <span class="math inline">\(A\)</span> as <span class="math inline">\(\{Y=y\}\)</span> and <span class="math inline">\(B\)</span> as <span class="math inline">\(\{X=x\}\)</span> yields</p>
<p><span class="math display">\[
p_{Y|X=x}(y)= \frac{p_{X|Y=y}(x)\, p_Y(y)}{p_X(x)},
\]</span></p>
<p>where <span class="math inline">\(p_{Y|X=x}(y) = \Pr\left( Y = y \, \middle|\, X = x\right)\)</span> is the conditional pmf of <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span> being equal to <span class="math inline">\(x\)</span>. Using the law of total probability,</p>
<p><span class="math display">\[
p_X(x)= \sum_k p_{X,Y}(x,k) = \sum_k p_{X|Y=k}(x) \, p_Y(k),
\]</span></p>
<p>we can rewrite the denominator above to get the following version of Bayes rule:</p>
<p><span class="math display">\[
p_{Y|X=x}(y) = \frac{p_{X|Y=y}(x)\, p_Y(y)}{\sum_k p_{X|Y=k}(x) \, p_Y(k)}.
\]</span></p>
<p>We can also obtain a similar Bayes rule for continuous random variables by replacing probability mass functions by probability density functions, and sums by integrals.</p>
<hr />
<p><strong>Proposition 8.2.1. Bayes Rule for Continuous Random Variables.</strong>
For two continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the conditional probability density function of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> follows</p>
<p><span class="math display">\[
f_{Y|X=x}(y) = \frac{f_{X|Y=y}(x)\, f_Y(y)}{f_X(x)},
\]</span></p>
<p>where the marginal distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are given as follows:</p>
<p><span class="math display">\[
f_X(x)=\int_{-\infty}^{\infty} \! f_{X,Y}(x,u) \, du \quad \text{and} \quad f_Y(y)=\int_{-\infty}^{\infty} \! f_{X,Y}(u,y) \, du,
\]</span></p>
<p>respectively. Similar to the discrete random variable case, we can swap the denominator of the equation above for</p>
<p><span class="math display">\[
f_{X}(x) = \int_{-\infty}^{\infty} \! f_{X,Y}(x,u) \, du = \int_{-\infty}^{\infty} \! f_{X|Y=u}(x) \, f_Y (u) \, du
\]</span></p>
<p>by using the law of total probability.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.2" href="javascript:toggleTheory('toggleTheory.Theory.2','displayCode.Theory.2');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.2" style="display: none">
<p><strong>Proof.</strong>
Bayes rule for continuous random variables may be derived from the definition of conditional probability density functions:</p>
<p><span class="math display">\[
f_{Y|X=x}(y) = \frac{f_{X,Y}(x,y)}{f_{X}(x)},
\]</span></p>
<p>if <span class="math inline">\(f_{X}(x)&gt;0\)</span>. Similarly,</p>
<p><span class="math display">\[
f_{X|Y=y}(x) = \frac{f_{X,Y}(x,y)}{f_{Y}(y)}.
\]</span></p>
<p>if <span class="math inline">\(f_{Y}(y) &gt; 0\)</span>. Solving for <span class="math inline">\(f_{X,Y}(x,y)\)</span> in the last equation and substituting into the first one yields Bayes rule for continuous random variables:</p>
<p><span class="math display">\[
f_{Y|X=x}(y) = \frac{f_{X|Y=y}(x)\, f_Y(y)}{f_X(x)}.
\]</span></p>
</div>
<hr />
<p>Note that one can mix the discrete and continuous definitions of Bayes rule to accommodate for cases where the parameters have continuous random variables and the observations are expressed via discrete random variables, or vice versa.</p>
<div id="ChBayes:SubsecPosterior" class="section level3 hasAnchor" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Posterior Distribution<a href="ChBayes.html#ChBayes:SubsecPosterior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Model parameters are assumed to be random variables under the Bayesian paradigm, meaning that Bayes rule for (discrete or continuous) random variables can be applied to update the prior knowledge about parameters by using new data. This is indeed similar to the process used in Section <a href="ChBayes.html#ChBayes:SubSecBayesVsFreq">9.1.1</a>.</p>
<p>Let us consider only one (random) model parameter <span class="math inline">\(\theta\)</span> associated with random variable <span class="math inline">\(\Theta\)</span> for now.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> Further, consider <span class="math inline">\(n\)</span> observations</p>
<p><span class="math display">\[
\mathbf{x} = (x_1, x_2, ..., x_n),
\]</span></p>
<p>which are realizations of the collection of random variables</p>
<p><span class="math display">\[
\mathbf{X} = (X_1, X_2, ..., X_n).
\]</span></p>
<p>If <span class="math inline">\(Y\)</span> in Proposition 8.2.1 is replaced by <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(X\)</span> by <span class="math inline">\(\mathbf{X}\)</span>, we obtain</p>
<p><span class="math display">\[
f_{\Theta|\mathbf{X}=\mathbf{x}}(\theta) = \frac{f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})\, f_{\Theta}(\theta)}{f_{\mathbf{X}}(\mathbf{x})},
\]</span></p>
<p>which represents the posterior distribution of the model parameter after updating the distribution based on the new observations <span class="math inline">\(\mathbf{x}\)</span>, and where</p>
<ul>
<li><span class="math inline">\(f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})\)</span> is the likelihood function, also known as the conditional joint pdf of the observations assuming a given value of parameter <span class="math inline">\(\theta\)</span>,</li>
<li><span class="math inline">\(f_{\Theta}(\theta)\)</span> is the unconditional pdf of the parameter that represents the prior information, and</li>
<li><span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is the marginal likelihood, which is a constant term with respect to <span class="math inline">\(\theta\)</span>, making the posterior density integrate to one.</li>
</ul>
<p>In other words, when applied to Bayesian inference, Bayes rule provides a mean to update the prior distribution of the parameter into a posterior distributionby considering the observations <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Note that the marginal likelihood is constant once we have the observations. It does not depend on <span class="math inline">\(\theta\)</span> and does not impact the overall shape of the pdf: it only provides the adequate scaling to ensure that the density integrates to one. For this reason, it is common to write down the posterior distribution using a proportional relationship instead:</p>
<p><span class="math display">\[
f_{\Theta|\mathbf{X}=\mathbf{x}}(\theta) \propto \underbrace{f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})}_{\text{Likelihood}} \, \, \underbrace{\vphantom{f_{\mathbf{X}|\Theta=\theta}(\mathbf{x})}f_{\Theta}(\theta)}_{\text{Prior}}.
\]</span></p>
<hr />
<p><strong>Example 8.2.1. A Problem Inspired from <span class="citation">Meyers (<a href="#ref-meyers1994quantifying" role="doc-biblioref">1994</a>)</span>.</strong>
A car insurance pays the following (independent) claim amounts on an automobile insurance policy:</p>
<p><span class="math display">\[
1050, \quad\quad 1250, \quad\quad 1550, \quad\quad 2600, \quad\quad 5350, \quad\quad 10200.
\]</span></p>
<p>The amount of a single payment is distributed as a single-parameter Pareto distribution with <span class="math inline">\(\theta = 1000\)</span> and <span class="math inline">\(\alpha\)</span> unknown, such that</p>
<p><span class="math display">\[
f_{X_i|A=\alpha}(x_i) = \frac{\alpha \, 1000^{\alpha}}{x_i^{\alpha+1}}, \quad x_i \in \mathbb{R}_+.
\]</span></p>
<p>We assume that the prior distribution of <span class="math inline">\(\alpha\)</span> is given by a gamma distribution with shape parameter 2 and scale parameter 1, and its pdf is given by</p>
<p><span class="math display">\[
f_{A}(\alpha) = \alpha\,e^{-\alpha}, \quad \alpha \in \mathbb{R}_+.
\]</span></p>
<p>Find the posterior distribution of parameter <span class="math inline">\(\alpha\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.2.1" href="javascript:toggleEX('toggleExample.8.2.1','displayExample.8.2.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.2.1" style="display: none">
<p><strong>Solution.</strong>
The likelihood function is constructed by multiplying the pdf of the single payment amounts because they are independent; that is,</p>
<p><span class="math display">\[
f_{\mathbf{X}|A=\alpha}(\mathbf{x}) = \prod_{i=1}^6 f_{X_i|A=\alpha}(x) = \frac{\alpha^6 \, 1000^{6\alpha}}{\prod_{i=1}^6 x_i^{\alpha+1}} = \alpha^{6} \, e^{-5.66518\alpha-41.44653} .
\]</span></p>
<p>The posterior distribution is given by</p>
<p><span class="math display">\[
f_{A|\mathbf{X}=\mathbf{x}}(\alpha) = \frac{\alpha^{7} \, e^{-6.66518\alpha-41.44653}}{\int_0^{\infty} \! \alpha^{7} \, e^{-6.66518\alpha-41.44653} \, d\alpha} = \frac{\alpha^{7} \, e^{-6.66518\alpha}}{\int_0^{\infty} \! \alpha^{7} \, e^{-6.66518\alpha} \, d\alpha} .
\]</span></p>
<p>Interestingly, we do not need to solve the integral at the denominator to find this distribution. As we know that the results should be a proper pdf and that the numerator looks like a gamma distribution, we can deduce that</p>
<p><span class="math display">\[
f_{A|\mathbf{X}=\mathbf{x}}(\alpha) = \frac{6.66518^8}{\Gamma(8)} \, \alpha^7 \, e^{-6.66518 \, \alpha},
\]</span></p>
<p>which is a gamma distribution with shape parameter 8 and scale parameter <span class="math inline">\(\frac{1}{6.66518}\)</span>. Figure <a href="ChBayes.html#fig:ChBayesBayesianMeyer">9.6</a> reports the posterior distribution of <span class="math inline">\(\alpha\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesBayesianMeyer"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesBayesianMeyer-1.png" alt="Posterior densities of parameter \(\alpha\)" width="80%" />
<p class="caption">
Figure 9.6: <strong>Posterior densities of parameter <span class="math inline">\(\alpha\)</span></strong>
</p>
</div>
</div>
<hr />
<p>The discussion above considered continuous random variables, but the same logic can be applied to discrete random variables by replacing probability density functions by probability mass functions.</p>
<hr />
<p><strong>Example 8.2.2. Coin Toss Revisited.</strong>
Assume that you observe three heads out of five (independent) tosses. Each toss has a probability of <span class="math inline">\(q\)</span> of observing heads and <span class="math inline">\(1-q\)</span> of observing tails. Find the posterior distribution of <span class="math inline">\(q\)</span> assuming a uniform prior distribution over the interval <span class="math inline">\([0,1]\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.2.2" href="javascript:toggleEX('toggleExample.8.2.2','displayExample.8.2.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.2.2" style="display: none">
<p><strong>Solution.</strong>
The prior distribution of <span class="math inline">\(q\)</span> is given by</p>
<p><span class="math display">\[
f_Q(q) = 1, \quad q \in [0,1].
\]</span>
Assuming the likelihood function conditional on <span class="math inline">\(Q=q\)</span> is given by a binomial distribution with <span class="math inline">\(m=5\)</span> and <span class="math inline">\(x=3\)</span>,</p>
<p><span class="math display">\[
p_{X|Q=q}(x) = \binom{5}{3} q^3 (1-q)^2,
\]</span>
we have that the posterior distribution of <span class="math inline">\(q\)</span> is given by</p>
<p><span class="math display">\[
f_{Q|X=3}(q) \,  \propto \, p_{X|Q=q}(x) \, f_Q(q) = q^3 \,(1-q)^2,
\]</span>
which is a beta distribution with <span class="math inline">\(a = 4\)</span>, <span class="math inline">\(b = 3\)</span>, and <span class="math inline">\(\theta = 1\)</span>; that is, we can easily deduce that</p>
<p><span class="math display">\[
f_{Q|X=3}(q) = \frac{\Gamma(7)}{\Gamma(4)\Gamma(3)} \, q^3 \, (1-q)^2.
\]</span></p>
</div>
<hr />
<p>In the following subsections, we will discuss at greater length the two main building blocks used to build the posterior distribution: the likelihood function and the prior distribution.</p>
</div>
<div id="likelihood-function" class="section level3 hasAnchor" number="9.2.2">
<h3><span class="header-section-number">9.2.2</span> Likelihood Function<a href="ChBayes.html#likelihood-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The likelihood function is a fundamental concept in statistical inference. It is used to estimate the parameters of a statistical model based on observed data. As mentioned in previous chapters, the likelihood function can be used to find the maximum likelihood estimator. In Bayesian statistics, the likelihood function is used to update the prior based on the evidence (or data).</p>
<p>As explained above and in <span style="color:red">Appendix C</span>, the likelihood function is defined as the conditional joint pdf or pmf of the observed data, given the model parameters. In other words, it is the probability of observing the data given a specific parameter values.</p>
<p>Mathematically, the likelihood function is written as <span class="math inline">\(f_{\mathbf{X}|\Theta=\theta}(x)\)</span> (for continuous random variables) or <span class="math inline">\(p_{\mathbf{X}|\Theta=\theta}(x)\)</span> (for discrete random variables). Note that, throughout the book, the notation <span class="math inline">\(L(\theta|\mathbf{x})\)</span> has also been used for the likelihood function, and we will use both interchangeably in this chapter.</p>
<p><strong>Special Case: Independent and Identically Distributed Observations.</strong> Oftentimes, in many problems and real-world applications, the observations are assumed to be iid. If they are, then we can easily write the likelihood function as:</p>
<p><span class="math display">\[
f_{\mathbf{X}|\Theta=\theta}(\mathbf{x}) = \prod_{i=1}^n f_{X_i|\Theta=\theta}(x_i) \quad \text{ or } \quad p_{\mathbf{X}|\Theta=\theta}(\mathbf{x}) = \prod_{i=1}^n p_{X_i|\Theta=\theta}(x_i).
\]</span></p>
</div>
<div id="ChBayes:SubsecPrior" class="section level3 hasAnchor" number="9.2.3">
<h3><span class="header-section-number">9.2.3</span> Prior Distribution<a href="ChBayes.html#ChBayes:SubsecPrior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the Bayesian paradigm, the prior distribution represents our knowledge or beliefs about the unknown parameters before we observe any data. It is a probability distribution that expresses the uncertainty about the values of the parameters. The prior distribution is typically specified by choosing a family of probability distributions and selecting specific values for its parameters.</p>
<p>The choice of prior distribution is subjective and often based on external information or previous studies. In some cases, noninformative priors can be used, which represent minimal prior knowledge or assumptions about the parameters. In other cases, informative and weakly informative priors can be used, which incorporate prior knowledge or assumptions based on external sources. The selection of the prior distribution should be carefully considered, and sensitivity analysis can be performed to assess the robustness of the results to different prior assumptions.</p>
<p><strong>Why Does It Matter?</strong> The choice of prior distribution can have a significant impact on the results of a Bayesian analysis. Different prior distributions can lead to different posterior distributions, which are the updated probability distributions for the parameters after we observe the data. Therefore, it is important to choose a prior distribution that reflects our prior knowledge or beliefs about the parameters.</p>
<div id="informative-and-weakly-informative-priors" class="section level4 hasAnchor" number="9.2.3.1">
<h4><span class="header-section-number">9.2.3.1</span> Informative and Weakly Informative Priors<a href="ChBayes.html#informative-and-weakly-informative-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Informative and weakly informative priors are terms used to describe the amount of prior knowledge or beliefs that is incorporated into a statistical model. Informative priors contain substantial prior knowledge about the parameters of a model, while weakly informative priors contain moderate prior knowledge.</p>
<p>Informative priors are useful when there is strong, potentially subjective prior information available about the model parameters, which can help to constrain the posterior distribution and improve inference. For example, in an insurance claims analysis study, an informative prior may be used to incorporate previous knowledge, such as the results of a previous claims study. This can help to reduce the uncertainty in the estimation of the claim distribution and improve the power of the analysis.</p>
<p>On the other hand, weakly informative priors are used when there is someyet littleprior knowledge available or when the goal is to allow the data to drive the inference. Weakly informative priors are designed to mildly impact the posterior distribution and are often chosen based on principles such as symmetry or scale invariance.</p>
<p>Overall, the choice of prior depends on the specific problem at hand and the available prior knowledge or beliefs. Informative priors can be useful when prior information is available and can improve the precision of the posterior distribution. In contrast, weakly informative priors can be useful when the goal is to allow the data to drive the inference and avoid imposing strong prior assumptions.</p>
<hr />
<p><strong>Example 8.2.3. Actuarial Exam Question.</strong>
You are given:</p>
<ul>
<li>Annual claim frequencies follow a Poisson distribution with mean <span class="math inline">\(\lambda\)</span>.</li>
<li>The prior distribution of <span class="math inline">\(\lambda\)</span> has the following pdf:
<span class="math display">\[
f_{\Lambda}(\lambda) = (0.3) \frac{1}{6} e^{-\frac{\lambda}{6}} + (0.7) \frac{1}{12} e^{-\frac{\lambda}{12}}, \quad \text{where } \lambda &gt; 0.
\]</span></li>
</ul>
<p>Ten claims are observed for an insured in Year 1. Calculate the expected value of the posterior distribution of <span class="math inline">\(\lambda\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.2.3" href="javascript:toggleEX('toggleExample.8.2.3','displayExample.8.2.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.2.3" style="display: none">
<p><strong>Solution.</strong>
The posterior distribution can be found from:</p>
<p><span class="math display">\[\begin{align}
f_{\Lambda|X=10}(\lambda) \, = &amp; \frac{p_{X|\Lambda=\lambda}(10) f_{\Lambda}(\lambda)}{p_{X}(10)} \\
= &amp; \, \frac{\frac{e^{-\lambda}\lambda^{10}}{10!} \left( (0.5) \frac{1}{6} e^{-\frac{\lambda}{6}} + (0.5) \frac{1}{12} e^{-\frac{\lambda}{12}} \right)}{\int_0^{\infty} \! \frac{e^{-\lambda}\lambda^{10}}{10!} \left( (0.5) \frac{1}{6} e^{-\frac{\lambda}{6}} + (0.5) \frac{1}{12} e^{-\frac{\lambda}{12}} \right) \, d\lambda} \\
= &amp;\, \frac{\lambda^{10} \left( \frac{0.5}{6} e^{-\frac{7\lambda}{6}} + \frac{0.5}{12} e^{-\frac{13\lambda}{12}} \right)}{118170} .
\end{align}\]</span></p>
<p>The posterior mean is therefore given by</p>
<p><span class="math display">\[\begin{align}
\mathrm{E}\left[\Lambda \, \middle|\, X = 10\right] = &amp;\, \frac{1}{118170} \int_0^{\infty} \! \lambda^{11} \left( \frac{0.5}{6} e^{-\frac{7\lambda}{6}} + \frac{0.5}{12} e^{-\frac{13\lambda}{12}} \right) \, d\lambda \\
= &amp;\, \frac{1}{118170} \left( \frac{0.5}{6}(11!)(6/7)^{12} + \frac{0.5}{12}(11!)(12/13)^{12} \right) \\
= &amp;\, 9.81328.
\end{align}\]</span></p>
</div>
<hr />
</div>
<div id="noninformative-priors" class="section level4 hasAnchor" number="9.2.3.2">
<h4><span class="header-section-number">9.2.3.2</span> Noninformative Priors<a href="ChBayes.html#noninformative-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>It is possible to take the idea of weakly informative priors to the extreme by using noninformative priors. A noninformative prior is a prior distribution that is intentionally chosen to allow the data to have a more decisive influence on the posterior distribution rather than being overly influenced by prior beliefs or assumptions.</p>
<p>Noninformative priors can take different forms, such as flat priors, for instance. A flat prior assigns equal probability to all possible parameter values without additional information or assumptions.</p>
<hr />
<p><strong>Example 8.2.4. Informative Versus Noninformative Priors.</strong>
You wish to investigate the impact of having informative and noninformative priors on a claim frequency analysis. Assume that the claim frequency for each policy follows a Bernoulli random variable with a probability of <span class="math inline">\(q\)</span> such that</p>
<p><span class="math display">\[
q_{X_i|Q=q}(x_i) = q^{x_i} (1-q)^{1-x_i}, \quad x_i \in \{0,1\},
\]</span></p>
<p>where <span class="math inline">\(q \in [0,1]\)</span>, and consider two different prior distributions:</p>
<ul>
<li>Informative: Based on past experience, you know that the claim probability is typically less than 5%, thus justifying the use of a uniform distribution over <span class="math inline">\([0,0.05]\)</span>.</li>
<li>Noninformative: You do not wish your posterior distribution to be impacted by your prior assumption and simply select a uniform distribution over the domain of <span class="math inline">\(p\)</span>, which is <span class="math inline">\([0,1]\)</span>.</li>
</ul>
<p>Using the first 250 lines of the Singapore insurance dataset (see Example 8.1.4 for more details on this dataset), find the two posterior distributions as well as the posterior expected value of the probability <span class="math inline">\(q\)</span> under both prior assumptions.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.2.4" href="javascript:toggleEX('toggleExample.8.2.4','displayExample.8.2.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.2.4" style="display: none">
<p><strong>Solution.</strong>
Let us start with the informative prior, where</p>
<p><span class="math display">\[
f_Q(q)= \frac{1}{0.05 - 0} = 20, \quad \text{ if }\, q \in [0, 0.05],
\]</span></p>
<p>and zero otherwise. In this case, assuming <span class="math inline">\(x = \sum_{i=1}^{250} x_i\)</span>, the posterior density is given by</p>
<p><span class="math display">\[\begin{align}
f_{Q|\mathbf{X}=\mathbf{x}} (q) \propto &amp;\, f_{\mathbf{X}|Q=q}(\mathbf{x}) f_Q(q) \\
\propto&amp; \, \prod_{i=1}^{250} q^{x_i} (1-q)^{1-x_i} \\
= &amp;\, q^{x} (1-q)^{250-x)}, \quad \text{ if }\, 0 \leq q \leq 0.05,
\end{align}\]</span></p>
<p>and zero otherwise. We can numerically obtain the shape of this posterior distribution by dividing <span class="math inline">\(q^{x} (1-q)^{250-x}\)</span> by</p>
<p><span class="math display">\[
\int_0^{0.05} \! q^{x} (1-q)^{250-x} \, dq.
\]</span></p>
<p>The second prior is still uniform, but over <span class="math inline">\([0,1]\)</span> this time, which is given mathematically by</p>
<p><span class="math display">\[
f_Q(q)= \frac{1}{1 - 0} = 1, \quad \text{ if }\, q \in [0,1],
\]</span></p>
<p>and zero otherwise, leading to the following posterior distribution:</p>
<p><span class="math display">\[\begin{align}
f_{Q|\mathbf{X}=\mathbf{x}} (q) \propto &amp;\, f_{\mathbf{X}|Q=q}(\mathbf{x}) f_Q(q) \\
\propto &amp; \, \prod_{i=1}^{250} q^{x_i} (1-q)^{1-x_i} \\
= &amp;\, q^{x} (1-q)^{250-x}, \quad \text{ if }\, 0 \leq q \leq 1,
\end{align}\]</span></p>
<p>and zero otherwise.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="ChBayes.html#cb139-1" aria-hidden="true" tabindex="-1"></a>qs <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="fl">0.12</span>, <span class="at">by =</span> <span class="fl">0.0001</span>)</span>
<span id="cb139-2"><a href="ChBayes.html#cb139-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>Clm_Count[<span class="dv">1</span><span class="sc">:</span><span class="dv">250</span>])</span>
<span id="cb139-3"><a href="ChBayes.html#cb139-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-4"><a href="ChBayes.html#cb139-4" aria-hidden="true" tabindex="-1"></a>integrandposterior1 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb139-5"><a href="ChBayes.html#cb139-5" aria-hidden="true" tabindex="-1"></a>    q<span class="sc">^</span>x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> q)<span class="sc">^</span>(<span class="dv">250</span> <span class="sc">-</span> x) <span class="sc">*</span> <span class="fu">ifelse</span>(q <span class="sc">&gt;=</span> <span class="dv">0</span> <span class="sc">&amp;</span> q <span class="sc">&lt;=</span> <span class="fl">0.05</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb139-6"><a href="ChBayes.html#cb139-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb139-7"><a href="ChBayes.html#cb139-7" aria-hidden="true" tabindex="-1"></a>marglikelihood1 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandposterior1, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb139-8"><a href="ChBayes.html#cb139-8" aria-hidden="true" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> <span class="fu">integrandposterior1</span>(qs)<span class="sc">/</span>marglikelihood1</span>
<span id="cb139-9"><a href="ChBayes.html#cb139-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb139-10"><a href="ChBayes.html#cb139-10" aria-hidden="true" tabindex="-1"></a>integrandposterior2 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb139-11"><a href="ChBayes.html#cb139-11" aria-hidden="true" tabindex="-1"></a>    q<span class="sc">^</span>x <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> q)<span class="sc">^</span>(<span class="dv">250</span> <span class="sc">-</span> x) <span class="sc">*</span> <span class="fu">ifelse</span>(q <span class="sc">&gt;=</span> <span class="dv">0</span> <span class="sc">&amp;</span> q <span class="sc">&lt;=</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb139-12"><a href="ChBayes.html#cb139-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb139-13"><a href="ChBayes.html#cb139-13" aria-hidden="true" tabindex="-1"></a>marglikelihood2 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandposterior2, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb139-14"><a href="ChBayes.html#cb139-14" aria-hidden="true" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> <span class="fu">integrandposterior2</span>(qs)<span class="sc">/</span>marglikelihood2</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesInfovsNoninfoContd1"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesInfovsNoninfoContd1-1.png" alt="Posterior densities based on informative (gray) and noninformative priors (black)" width="80%" />
<p class="caption">
Figure 9.7: <strong>Posterior densities based on informative (gray) and noninformative priors (black)</strong>
</p>
</div>
<p>We also wish to obtain the expected value of <span class="math inline">\(q\)</span> for both posterior distribution. This can be obtained by numerically integrating the following equation:</p>
<p><span class="math display">\[
\mathrm{E}[Q|\mathbf{X}=\mathbf{x}] = \int_0^1 \! q \, f_{Q|\mathbf{X}=\mathbf{x}} (q) \, dq.
\]</span></p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="ChBayes.html#cb140-1" aria-hidden="true" tabindex="-1"></a>integrandexpvalue1 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb140-2"><a href="ChBayes.html#cb140-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">integrandposterior1</span>(q)<span class="sc">/</span>marglikelihood1 <span class="sc">*</span> q</span>
<span id="cb140-3"><a href="ChBayes.html#cb140-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb140-4"><a href="ChBayes.html#cb140-4" aria-hidden="true" tabindex="-1"></a>expectedvalue1 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandexpvalue1, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb140-5"><a href="ChBayes.html#cb140-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The posterior expected value of the parameter when using the informative prior is&quot;</span>,</span>
<span id="cb140-6"><a href="ChBayes.html#cb140-6" aria-hidden="true" tabindex="-1"></a>    expectedvalue1)</span></code></pre></div>
<pre><code>The posterior expected value of the parameter when using the informative prior is 0.03895276</code></pre>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="ChBayes.html#cb142-1" aria-hidden="true" tabindex="-1"></a>integrandexpvalue2 <span class="ot">&lt;-</span> <span class="cf">function</span>(q) {</span>
<span id="cb142-2"><a href="ChBayes.html#cb142-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">integrandposterior2</span>(q)<span class="sc">/</span>marglikelihood2 <span class="sc">*</span> q</span>
<span id="cb142-3"><a href="ChBayes.html#cb142-3" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb142-4"><a href="ChBayes.html#cb142-4" aria-hidden="true" tabindex="-1"></a>expectedvalue2 <span class="ot">&lt;-</span> <span class="fu">integrate</span>(integrandexpvalue2, <span class="dv">0</span>, <span class="dv">1</span>, <span class="at">abs.tol =</span> .Machine<span class="sc">$</span>double.eps<span class="sc">^</span><span class="dv">2</span>)<span class="sc">$</span>value</span>
<span id="cb142-5"><a href="ChBayes.html#cb142-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The posterior expected value of the parameter when using the noninformative prior is&quot;</span>,</span>
<span id="cb142-6"><a href="ChBayes.html#cb142-6" aria-hidden="true" tabindex="-1"></a>    expectedvalue2)</span></code></pre></div>
<pre><code>The posterior expected value of the parameter when using the noninformative prior is 0.04761905</code></pre>
<p>As one can see, these values are different, meaning that the prior distribution can have a material impact on the posterior distribution. One should therefore be careful when selecting a prior distribution.</p>
</div>
<hr />
</div>
<div id="improper-priors" class="section level4 hasAnchor" number="9.2.3.3">
<h4><span class="header-section-number">9.2.3.3</span> Improper Priors<a href="ChBayes.html#improper-priors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An improper prior is a prior distribution that is not a proper probability distribution, meaning that it does not integrate (or sum) to one over the entire parameter space. Improper priors can be used in Bayesian analyses, but they require careful handling because they can lead to improper posterior distributions.</p>
<p>Improper priors are typically used when there is little or no prior information about the parameter of interestsome noninformative priors are indeed improperand they can be thought of as representing a very diffuse or noncommittal prior belief. For instance, the uniform distribution on an infinite interval is a common choice of improper prior.</p>
<hr />
<p><strong>Example 8.2.5. Improper Prior, Proper Posterior.</strong>
Let us assume a random sample <span class="math inline">\(\mathbf{x}\)</span> of size <span class="math inline">\(n\)</span>, which is a realization of the collection of random variables <span class="math inline">\(\mathbf{X} = (X_1,X_2, ..., X_n)\)</span>. Further, assume that each random variable <span class="math inline">\(X_i\)</span> is independent and normally distributed with mean of <span class="math inline">\(\mu\)</span> and variance of 1:</p>
<p><span class="math display">\[
f_{X_i|M=\mu}(x_i) = \frac{1}{\sqrt{2\pi}} \exp\left( - \frac{1}{2} \left( x_i -\mu\right)^2 \right), \quad x_i \in \mathbb{R},
\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is a (random) parameter. Obtain the posterior distribution of <span class="math inline">\(\mu\)</span> assuming that its prior distribution is improper and given by <span class="math inline">\(f_M(\mu) \propto 1\)</span>, where <span class="math inline">\(\mu \in \mathbb{R}\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.2.5" href="javascript:toggleEX('toggleExample.8.2.5','displayExample.8.2.5');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.2.5" style="display: none">
<p><strong>Solution.</strong>
According to Bayes rule, we have that</p>
<p><span class="math display">\[
f_{M|\mathbf{X}=\mathbf{x}}(\mu) = \frac{f_{\mathbf{X}|M=\mu}(\mathbf{x})\, f_{M}(\mu)}{f_{\mathbf{X}}(\mathbf{x})} \propto \prod_{i=1}^n f_{X_i|M=\mu}(x_i)
\]</span></p>
<p>because <span class="math inline">\(f_M(\mu) \propto 1\)</span> and <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> does not depend on <span class="math inline">\(\mu\)</span>. Using the equation above, we can obtain the posterior distribution by simplifying the following equation:</p>
<p><span class="math display">\[\begin{align}
f_{M|\mathbf{X}=\mathbf{x}}(\mu) \propto &amp;\, \left(\frac{1}{\sqrt{2\pi}}\right)^n \exp\left( - \frac{1}{2} \sum_{i=1}^n  \left( x_i -\mu\right)^2 \right) \\
\propto &amp;\, \exp\left( - \frac{1}{2} \left( \sum_{i=1}^n x_i^2 -2\mu \sum_{i=1}^n x_i + n \mu^2 \right) \right) \\
\propto &amp;\, \exp\left( - \frac{n}{2} \left( \frac{\sum_{i=1}^n x_i^2}{n} -\frac{2\mu \sum_{i=1}^n x_i}{n} + \mu^2 \right) \right) \\
\propto &amp;\, \exp\left( - \frac{n}{2} \left( -\frac{2\mu \sum_{i=1}^n x_i}{n} + \mu^2 \right) \right) \\
\propto &amp;\, \exp\left( - \frac{n}{2} \left(  \mu - \frac{\sum_{i=1}^n x_i}{n}\right)^2  \right) \\
\propto &amp;\, \frac{1}{\sqrt{2\pi \frac{1}{n}}} \exp\left( - \frac{1}{2} \frac{\left(  \mu - \frac{\sum_{i=1}^n x_i}{n}\right)^2}{\frac{1}{n}} \right),
\end{align}\]</span></p>
<p>which is a normal distribution with mean <span class="math inline">\(\frac{\sum_{i=1}^n x_i}{n}\)</span> and variance <span class="math inline">\(\frac{1}{n}\)</span>. Interestingly, this posterior distribution is proper even though the prior distribution was improper.</p>
</div>
</div>
</div>
</div>
<div id="ChBayes:SecConjugate" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Conjugate Families<a href="ChBayes.html#ChBayes:SecConjugate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#ChBayes:SecConjugate">9.3</a>, you learn how to:</p>
<ul>
<li>Describe three specific classes of conjugate families.</li>
<li>Use conjugate distributions to determine posterior distributions of parameters.</li>
<li>Understand the pros and cons of conjugate family models.</li>
</ul>
<hr />
<p>In Bayesian statistics, if a posterior distribution is the same distribution as the prior distribution, the prior and posterior are called conjugate distributions. Note that both posterior and prior have similar shapes but will have different parameters, generally speaking.</p>
<p><strong>But Why?</strong> Two main reasons explain why conjugate families have been so popular historically:</p>
<ol style="list-style-type: decimal">
<li>They are easy to use from a computational standpoint: posterior distributions in most conjugate families can be obtained in closed form, making this class of models easy to use even if we do not have access to computing power.</li>
<li>They tend to be easy to interpret: posterior distributions are compromises between data and prior distributions. Having both prior and posterior distributions in the same familybut with different parametersallows us to understand and quantify how the data changed our initial assumptions.</li>
</ol>
<hr />
<div id="ChBayes:SubsecBetaBin" class="section level3 hasAnchor" number="9.3.1">
<h3><span class="header-section-number">9.3.1</span> The BetaBinomial Conjugate Family<a href="ChBayes.html#ChBayes:SubsecBetaBin" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first conjugate family that we investigate in this book is the betabinomial family. Let <span class="math inline">\(\mathbf{X} = (X_1, X_2, ... X_m)\)</span> represent a sample of iid Bernoulli random variables such that</p>
<p><span class="math display">\[
X_i = \begin{cases}
1 &amp; \text{if success}  \\
0 &amp; \text{if failure}
\end{cases}
,
\]</span></p>
<p>with probabilities <span class="math inline">\(q\)</span> and <span class="math inline">\(1-q\)</span>, respectively. Let us further define <span class="math inline">\(x = \sum_{i=1}^m x_i\)</span> the sum of the realized successes.</p>
<p>We know from elementary probability that <span class="math inline">\(X = \sum_{i=1}^m X_i\)</span> follows a binomial distribution (i.e., the number of successes <span class="math inline">\(x\)</span> in <span class="math inline">\(m\)</span> Bernoulli trials) with unknown probability of success <span class="math inline">\(q\)</span> in <span class="math inline">\([0,1]\)</span>, similar to the coin tossing case of Example 8.1.1, such that the likelihood function is given by</p>
<p><span class="math display">\[
p_{\mathbf{X}|Q=q}(\mathbf{x}) = \binom{m}{x} q^x (1-q)^{m-x}, \quad x \in \{0,1,...,m\},
\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^m x_i\)</span>. The latter represents our evidence. Then, we combine it with its usual conjugate priorthe beta distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The pdf of the beta distribution is given as follows:</p>
<p><span class="math display">\[
f_{Q}(q) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} q^{a-1} (1-q)^{b-1}, \quad q \in [0,1],
\]</span></p>
<p>where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are shape parameters of the beta distribution.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<p>We can now combine the prior distributionbetawith the likelihood functionbinomialto obtain the posterior distribution.</p>
<hr />
<p><strong>Proposition 8.3.1. BetaBinomial Conjugate Family.</strong>
Consider a sample of <span class="math inline">\(m\)</span> iid Bernoulli experiments <span class="math inline">\((X_1,X_2,...,X_m)\)</span> each with success probability <span class="math inline">\(q\)</span>. Further assume that the random variable associated with the success probability, <span class="math inline">\(Q\)</span>, has a prior that is beta with shape parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The posterior distribution of <span class="math inline">\(Q\)</span> is therefore given by</p>
<p><span class="math display">\[
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{\Gamma(a+b+m)}{\Gamma(a+x)\Gamma(b+m-x)} q^{a+x-1} (1-q)^{b+m-x-1},
\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^m x_i\)</span>, which is a beta distribution with shape parameters <span class="math inline">\(a+x\)</span> and <span class="math inline">\(b+m-x\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.3" href="javascript:toggleTheory('toggleTheory.Theory.3','displayCode.Theory.3');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.3" style="display: none">
<p><strong>Proof.</strong>
From Section <a href="ChBayes.html#ChBayes:SubsecPosterior">9.2.1</a>, we know that</p>
<p><span class="math display">\[\begin{align}
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{p_{\mathbf{X}|Q=q}(\mathbf{x})\, f_{Q}(q)}{p_{\mathbf{X}}(\mathbf{x})} \propto&amp;\, \binom{m}{x} q^x (1-q)^{m-x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} q^{a-1}(1-q)^{b-1} \notag \\[1ex]
\propto&amp;\, q^{a+x-1} (1-q)^{b+m-x-1}. \notag
\end{align}\]</span></p>
<p>We therefore only need to find the normalizing constant that ensures that the right-hand of the equation above is a density. Interestingly, the right-hand side looks like a beta distribution; specifically,</p>
<p><span class="math display">\[\begin{align}
&amp;\,\int_0^1 \! q^{a+x-1} (1-q)^{b+m-x-1} \, dq \\[1ex]
=&amp;\, \frac{\Gamma(a+x)\Gamma(b+m-x)}{\Gamma(a+b+m)} \int_0^1 \! \frac{\Gamma(a+b+m)}{\Gamma(a+x)\Gamma(b+m-x)} q^{a+x-1} (1-q)^{b+m-x-1} \, dq \\[1ex]
=&amp;\, \frac{\Gamma(a+x)\Gamma(b+m-x)}{\Gamma(a+b+m)},
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{\Gamma(a+b+m)}{\Gamma(a+x)\Gamma(b+m-x)} q^{a+x-1} (1-q)^{b+m-x-1} .
\]</span></p>
</div>
<hr />
<p><strong>Parameters Versus Hyperparameters.</strong> In this context, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are called hyperparametersparameters of the prior. These are different from parameters of the underlying model (i.e., <span class="math inline">\(q\)</span> in the betabinomial family). Hyperparameters are typically known and determined by the experimenter, whereas the underlying model parameters are random in the Bayesian context.</p>
<hr />
<p><strong>Example 8.3.1. Actuarial Exam Question.</strong>
You are given:</p>
<ul>
<li><p>The annual number of claims in Year <span class="math inline">\(i\)</span> for a policyholder has a binomial distribution with pmf
<span class="math display">\[
p_{X_i|Q=q}(x_i) = \binom{2}{x} q^{x_i} (1-q)^{2-x_i}, \quad x_i \in \{ 0, 1, 2\}.
\]</span></p></li>
<li><p>The prior distribution is</p></li>
</ul>
<p><span class="math display">\[
f_Q(q) = 4 q^3, \quad q \in [0,1].
\]</span></p>
<p>The policyholder had one claim in each of Years 1 and 2. Calculate the Bayesian estimate of the expected number of claims in Year 3.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.3.1" href="javascript:toggleEX('toggleExample.8.3.1','displayExample.8.3.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.3.1" style="display: none">
<p><strong>Solution.</strong>
The likelihood function based on this policyholders number of claims in Years 1 and 2 is given by:</p>
<p><span class="math display">\[
p_{\mathbf{X}|Q=q}(\mathbf{x}) = p_{X_1|Q=q}(1) \, p_{X_2|Q=q}(1) = \binom{2}{1} q^{1} (1-q)^{1} \, \binom{2}{1} q^{1} (1-q)^{1} \propto q^{2} (1-q)^2,
\]</span></p>
<p>which is proportional to a binomial pmf with <span class="math inline">\(m=4\)</span>, two successes, and a success probability of <span class="math inline">\(q\)</span>. Because the prior distribution is beta distributed with <span class="math inline">\(a=4\)</span> and <span class="math inline">\(b=1\)</span>, we know that the posterior distribution of parameter <span class="math inline">\(q\)</span> is given by</p>
<p><span class="math display">\[\begin{align}
f_{Q|\mathbf{X}=\mathbf{x}}(q) =&amp;\, \frac{\Gamma(4+1+4)}{\Gamma(4+2)\Gamma(1+4-2)} q^{4+2-1} (1-q)^{1+4-2-1}  \notag \\
=&amp;\, \frac{\Gamma(9)}{\Gamma(6)\Gamma(3)} q^{5} (1-q)^{2}  \notag \\
=&amp;\, 168 q^5 (1-q)^2, \notag
\end{align}\]</span></p>
<p>which is also a beta distribution with shape parameters 6 and 3, respectively.</p>
<p>The expected number of claim in Year 3 is</p>
<p><span class="math display">\[
\mathrm{E}\left[ \mathrm{E} \left[ X_3 \, \middle|\, Q=q \, \right] \right] = \mathrm{E}\left[ 2 q \right] = 2  \, \mathrm{E}\left[ q \right],
\]</span></p>
<p>and <span class="math inline">\(\mathrm{E}\left[ q \right]\)</span> is the expected value of the beta distribution, which is given by</p>
<p><span class="math display">\[
\mathrm{E}\left[ q \right] = \frac{6}{6+3} = \frac{2}{3}.
\]</span>
Ultimately, this leads to an expected number of claim in Year 3 of <span class="math inline">\(2 \left(\tfrac{2}{3} \right) = \tfrac{4}{3}\)</span>.</p>
</div>
<hr />
<p><strong>Example 8.3.2. Impact of Beta Prior on Posterior.</strong>
You wish to investigate the impact of having different beta hyperparameters on the posterior distribution. Assume that the claim frequency for each policy follows a Bernoulli random variable with a probability of <span class="math inline">\(q\)</span> such that</p>
<p><span class="math display">\[
p_{X_i|Q=q}(x_i) = q^{x_i} (1-q)^{1-x_i}, \quad x_i \in \{0,1\},
\]</span></p>
<p>where <span class="math inline">\(q \in [0,1]\)</span>, and consider two different sets of hyperparameters:</p>
<ul>
<li>Set 1: <span class="math inline">\(a = 1\)</span> and <span class="math inline">\(b=10\)</span>.</li>
<li>Set 2: <span class="math inline">\(a = 2\)</span> and <span class="math inline">\(b=2\)</span>.</li>
</ul>
<p>Figure <a href="ChBayes.html#fig:ChBayesBetaPrior1">9.8</a> shows the pdf of these two prior distributions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesBetaPrior1"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesBetaPrior1-1.png" alt="Beta prior densities: \(a=1\) and \(b=10\) (gray), and \(a=2\) and \(b=2\) (black)" width="80%" />
<p class="caption">
Figure 9.8: <strong>Beta prior densities: <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=10\)</span> (gray), and <span class="math inline">\(a=2\)</span> and <span class="math inline">\(b=2\)</span> (black)</strong>
</p>
</div>
<p>Using again the first 250 lines of the Singapore insurance dataset (see Example 8.1.4 for more details on this dataset), find the two posterior distributions.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.3.2" href="javascript:toggleEX('toggleExample.8.3.2','displayExample.8.3.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.3.2" style="display: none">
<p><strong>Solution.</strong>
The likelihood function associated with the observations is given by</p>
<p><span class="math display">\[
p_{\mathbf{X}|Q=q}(\mathbf{x}) = \binom{250}{x} \, q^{x} (1-q)^{250-x}, \quad \text{where }\, x = \sum_{i=1}^{250} x_i,
\]</span></p>
<p>as mentioned already in Example 8.2.4. Combining this likelihood with a beta prior gives a beta posterior:</p>
<p><span class="math display">\[
f_{Q|\mathbf{X}=\mathbf{x}}(q) = \frac{\Gamma(a+b+250)}{\Gamma\left(a+x\right)\, \Gamma\left(b+250-x\right)} \, q^{a+x-1} (1-q)^{b+250-x-1},
\]</span></p>
<p>that can be evaluated for various values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Figure <a href="ChBayes.html#fig:ChBayesBetaPrior2">9.9</a> reports the two posterior distributions associated with the priors mentioned above.</p>

<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="ChBayes.html#cb144-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(sgautonb<span class="sc">$</span>Clm_Count[<span class="dv">1</span><span class="sc">:</span><span class="dv">250</span>])</span>
<span id="cb144-2"><a href="ChBayes.html#cb144-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-3"><a href="ChBayes.html#cb144-3" aria-hidden="true" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(qs, <span class="at">shape1 =</span> <span class="dv">1</span> <span class="sc">+</span> x, <span class="at">shape2 =</span> <span class="dv">10</span> <span class="sc">+</span> <span class="dv">250</span> <span class="sc">-</span> x)</span>
<span id="cb144-4"><a href="ChBayes.html#cb144-4" aria-hidden="true" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> <span class="fu">dbeta</span>(qs, <span class="at">shape1 =</span> <span class="dv">2</span> <span class="sc">+</span> x, <span class="at">shape2 =</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">250</span> <span class="sc">-</span> x)</span>
<span id="cb144-5"><a href="ChBayes.html#cb144-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-6"><a href="ChBayes.html#cb144-6" aria-hidden="true" tabindex="-1"></a>dataposterior <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> qs, <span class="at">y1 =</span> posterior1, <span class="at">y2 =</span> posterior2)</span>
<span id="cb144-7"><a href="ChBayes.html#cb144-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-8"><a href="ChBayes.html#cb144-8" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataposterior, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y1)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;darkgray&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb144-9"><a href="ChBayes.html#cb144-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y2), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span> <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">35</span>) <span class="sc">+</span></span>
<span id="cb144-10"><a href="ChBayes.html#cb144-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">italic</span>(<span class="st">&quot;q&quot;</span>))) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Posterior density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesBetaPrior2"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesBetaPrior2-1.png" alt="Posterior densities based on two different priors: \(a=1\) and \(b=10\) (gray), and \(a=2\) and \(b=2\) (black)" width="80%" />
<p class="caption">
Figure 9.9: <strong>Posterior densities based on two different priors: <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=10\)</span> (gray), and <span class="math inline">\(a=2\)</span> and <span class="math inline">\(b=2\)</span> (black)</strong>
</p>
</div>
</div>
<hr />
<p>The prior distribution (and its hyperparameters) clearly has an impact on the posterior distribution. As a general rule of thumb for the beta prior, a higher <span class="math inline">\(a\)</span> puts more weight on higher values of <span class="math inline">\(q\)</span> and a higher <span class="math inline">\(b\)</span> puts more weight on lower values of <span class="math inline">\(q\)</span>.</p>
</div>
<div id="the-gammapoisson-conjugate-family" class="section level3 hasAnchor" number="9.3.2">
<h3><span class="header-section-number">9.3.2</span> The GammaPoisson Conjugate Family<a href="ChBayes.html#the-gammapoisson-conjugate-family" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now present a second conjugate family: the gammaPoisson family. Let <span class="math inline">\(\mathbf{X} = (X_1, X_2, ..., X_n)\)</span> be a sample of iid Poisson random variables such that</p>
<p><span class="math display">\[
p_{X_i|\Lambda=\lambda}(x_i) = \frac{\lambda^{x_i}\, e^{-\lambda}}{x_i!}, \quad x_i \in \mathbb{R}_+ .
\]</span></p>
<p>The likelihood function associated with this sample would therefore be given by</p>
<p><span class="math display">\[
f_{\mathbf{X}|\Lambda=\lambda}(\mathbf{x}) = \prod_{i=1}^n p_{X_i|\Lambda=\lambda}(x_i) = \prod_{i=1}^n \frac{\lambda^{x_i}\, e^{-\lambda}}{x_i!} = \frac{\lambda^x \, e^{-n\lambda}}{\prod_{i=1}^n x_i !} \propto \lambda^x \, e^{-n\lambda},
\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>. The shape of this likelihood function, as a function of <span class="math inline">\(\lambda\)</span>, is reminiscent of a gamma distribution, hinting to the fact that this distribution would be a good contender for a conjugate prior. Indeed, if we let the prior distribution be gamma with shape hyperparameter <span class="math inline">\(\alpha\)</span> and scale hyperparameter <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[
f_{\Lambda}(\lambda) = \frac{1}{\Gamma(\alpha)\theta^{\alpha}} \lambda^{\alpha-1} \, e^{-\frac{\lambda}{\theta}}, \quad \lambda \in \mathbb{R}_+,
\]</span></p>
<p>we can show that the posterior distribution of <span class="math inline">\(\lambda\)</span> is also gamma.</p>
<hr />
<p><strong>Proposition 8.3.2. GammaPoisson Conjugate Family.</strong>
Consider a sample of <span class="math inline">\(n\)</span> iid Poisson experiments <span class="math inline">\((X_1,X_2,...,X_n)\)</span>, each with rate parameter <span class="math inline">\(\lambda\)</span>. Further assume that the random variable associated with the rate, <span class="math inline">\(\Lambda\)</span>, has a prior that is gamma distributed with shape hyperparameter <span class="math inline">\(\alpha\)</span> and scale hyperparameter <span class="math inline">\(\theta\)</span>. The posterior distribution of <span class="math inline">\(\Lambda\)</span> is therefore given by</p>
<p><span class="math display">\[
f_{\Lambda|\mathbf{X}=\mathbf{x}}(\lambda) = \frac{1}{\Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+1}} \lambda^{\alpha+x-1} \, e^{-\frac{\lambda\,(n\theta+1)}{\theta}},
\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>, which is a gamma distribution with shape parameter <span class="math inline">\(\alpha+x\)</span> and scale parameter <span class="math inline">\(\tfrac{\theta}{n\theta+1}\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.4" href="javascript:toggleTheory('toggleTheory.Theory.4','displayCode.Theory.4');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.4" style="display: none">
<p><strong>Proof.</strong>
From Section <a href="ChBayes.html#ChBayes:SubsecPosterior">9.2.1</a>, we know that</p>
<p><span class="math display">\[\begin{align}
f_{\Lambda|\mathbf{X}=\mathbf{x}}(\lambda) = \frac{p_{\mathbf{X}|\Lambda=\lambda}(\mathbf{x})\, f_{\Lambda}(\lambda)}{p_{\mathbf{X}}(\mathbf{x})} \propto &amp;\, \lambda^x \, e^{-n\lambda} \, \lambda^{\alpha-1} \, e^{-\frac{\lambda}{\theta}} \notag \\
\propto&amp;\, \lambda^{\alpha+x-1} \, e^{-\frac{\lambda (n\theta+1)}{\theta}}, \notag
\end{align}\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>. We therefore only need to find the normalizing constant that ensures that the right-hand of the equation above is a density. Interestingly, the right-hand side looks like a gamma distribution; specifically,</p>
<p><span class="math display">\[\begin{align}
&amp;\,\int_0^{\infty} \! \lambda^{\alpha+x-1} \, e^{-\frac{\lambda (n\theta+1)}{\theta}} \, d\lambda \\
=&amp;\, \Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+1}  \int_0^{\infty} \!\frac{1}{\Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+1}}  \, \lambda^{\alpha+x-1} \, e^{-\frac{\lambda (n\theta+1)}{\theta}} \, d\lambda \\
=&amp;\, \Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+1},
\end{align}\]</span></p>
<p>and</p>
<p><span class="math display">\[
f_{\Lambda|\mathbf{X}=\mathbf{x}}(\lambda) = \frac{1}{\Gamma(\alpha+x)\left(\frac{\theta}{n\theta+1} \right)^{\alpha+1}} \lambda^{\alpha+x-1} \, e^{-\frac{\lambda\,(n\theta+1)}{\theta}}.
\]</span></p>
</div>
<hr />
<p><strong>Example 8.3.3. Actuarial Exam Question.</strong>
You are given:</p>
<ul>
<li>The number of claims incurred in a month by any insured has a Poisson distribution with mean <span class="math inline">\(\lambda\)</span>.</li>
<li>The claim frequencies of different insured are iid.</li>
<li>The prior distribution is gamma with pdf</li>
</ul>
<p><span class="math display">\[
f_{\Lambda}(\lambda) = \frac{(100\lambda)^6}{120\lambda}e^{-100\lambda}, \quad \lambda \in \mathbb{R}_+.
\]</span></p>
<ul>
<li>The number of claims every month is distributed as follows:</li>
</ul>
<p><span class="math display">\[
\begin{matrix}
    \begin{array}{c|c|c} \hline
    \text{Month} &amp; \text{Number of Insured} &amp; \text{Number of Claims} \\
    \hline
    \text{1} &amp; 100 &amp; 6 \\\hline
    \text{2} &amp; 150 &amp; 8 \\\hline
    \text{3} &amp; 200 &amp; 11 \\\hline
    \text{4} &amp; 300 &amp; ? \\\hline
    \end{array}
\end{matrix}
\]</span></p>
<p>Calculate the expected number of claims in Month 4.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.3.3" href="javascript:toggleEX('toggleExample.8.3.3','displayExample.8.3.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.3.3" style="display: none">
<p><strong>Solution.</strong>
The likelihood function based on this policyholders number of claims in Months 1, 2, and 3 is given by:</p>
<p><span class="math display">\[
p_{\mathbf{X}|\Lambda = \lambda}(\mathbf{x}) = p_{X_1|\Lambda = \lambda}(6) \, p_{X_2|\Lambda = \lambda}(8) \, p_{X_3|\Lambda = \lambda}(11) \propto \lambda^{6+8+11}\, e^{-\lambda(100+150+200)}.
\]</span>
Because the prior distribution is gamma distributed with <span class="math inline">\(\alpha=6\)</span> and <span class="math inline">\(\theta=\tfrac{1}{100}\)</span>, we know that the posterior distribution of parameter <span class="math inline">\(\lambda\)</span> is also gamma distributed with shape parameter</p>
<p><span class="math display">\[
\alpha + x = 6+6+8+11 = 31
\]</span></p>
<p>and scale parameter</p>
<p><span class="math display">\[
\frac{\theta}{n\theta+1} = \frac{\frac{1}{100}}{(100+150+200)\frac{1}{100}+1} = \frac{1}{550}.
\]</span></p>
<p>The expected number of claim in Month 4 is</p>
<p><span class="math display">\[
\mathrm{E}\left[ \mathrm{E} \left[ X_4 \, \middle|\, \Lambda=\lambda \, \right] \right] = \mathrm{E}\left[ 300 \lambda \right] = 300  \, \mathrm{E}\left[ \lambda \right],
\]</span></p>
<p>and <span class="math inline">\(\mathrm{E}\left[ \lambda \right]\)</span> is the expected value of the gamma distribution, which is given by</p>
<p><span class="math display">\[
\mathrm{E}\left[ \lambda \right] = \frac{31}{550}.
\]</span></p>
<p>Ultimately, this leads to an expected number of claim in Month 4 of <span class="math inline">\(300 \left(\tfrac{31}{550} \right) = \tfrac{930}{55} \approx 16.91\)</span>.</p>
</div>
<hr />
</div>
<div id="the-normalnormal-conjugate-family" class="section level3 hasAnchor" number="9.3.3">
<h3><span class="header-section-number">9.3.3</span> The NormalNormal Conjugate Family<a href="ChBayes.html#the-normalnormal-conjugate-family" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The last conjugate family is the normalnormal family. Let <span class="math inline">\(\mathbf{X}=(X_1,X_2,...,X_n)\)</span> be a sample of iid normal random variables such that</p>
<p><span class="math display">\[
f_{X_i|M=\mu}(x_i) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( - \frac{1}{2} \frac{(x_i-\mu)^2}{\sigma^2} \right), \quad x_i \in \mathbb{R}.
\]</span></p>
<p>Further, to keep our focus on <span class="math inline">\(\mu\)</span>, we will assume throughout our analysis that the variance parameter <span class="math inline">\(\sigma^2\)</span> is known.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> The likelihood function associated with this sample would therefore be given by</p>
<p><span class="math display">\[\begin{align}
f_{\mathbf{X}|M=\mu}(\mathbf{x}) =&amp;\, \prod_{i=1}^n f_{X_i|M=\mu}(x_i)  \\
=&amp;\, \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n \exp\left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2} \right) \\
\propto&amp;\, \exp\left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2} \right).
\end{align}\]</span></p>
<p>A very natural prior distribution that matches the likelihood structure is unsurprisingly the normal distribution. Let us assume that the prior distribution for <span class="math inline">\(\mu\)</span> is given by</p>
<p><span class="math display">\[
f_{M}(\mu) = \frac{1}{\sqrt{2\pi \tau^2}} \exp\left( -\frac{1}{2} \frac{(\mu-\theta)^2}{\tau^2} \right),
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the mean parameter and <span class="math inline">\(\tau^2\)</span> is the variance parameter. We can then easily show that the posterior distribution of <span class="math inline">\(\mu\)</span> is also given by a normal distribution.</p>
<hr />
<p><strong>Proposition 8.3.3. NormalNormal Conjugate Family.</strong>
Consider a sample of <span class="math inline">\(n\)</span> iid normals <span class="math inline">\((X_1,X_2,...,X_n)\)</span>, each with mean parameter <span class="math inline">\(\mu\)</span> and variance parameter <span class="math inline">\(\sigma^2\)</span> that is known. Further assume that the random variable associated with the mean, <span class="math inline">\(M\)</span>, has a prior that is normally distributed with mean hyperparameter <span class="math inline">\(\theta\)</span> and variance hyperparameter <span class="math inline">\(\tau^2\)</span>. The posterior distribution of <span class="math inline">\(M\)</span> is therefore given by</p>
<p><span class="math display">\[
f_{M|\mathbf{X}=\mathbf{x}}(\mu) = \frac{1}{\sqrt{2\pi\left( \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} \right)} } \exp\left( - \frac{1}{2} \frac{ \left( \mu - \left( \frac{x}{n} \frac{\tau^2}{n\tau^2 +\sigma^2} + \theta \frac{\sigma^2}{n\tau^2 +\sigma^2}  \right) \right)^2 }{ \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} } \right),
\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>, which is a normal distribution with mean parameter</p>
<p><span class="math display">\[
\frac{x}{n} \frac{n\tau^2}{n\tau^2 +\sigma^2} + \theta \frac{\sigma^2}{n\tau^2 +\sigma^2}
\]</span></p>
<p>and variance parameter</p>
<p><span class="math display">\[
\frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2}.
\]</span></p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.5" href="javascript:toggleTheory('toggleTheory.Theory.5','displayCode.Theory.5');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.5" style="display: none">
<p><strong>Proof.</strong>
From Section <a href="ChBayes.html#ChBayes:SubsecPosterior">9.2.1</a>, we know that</p>
<p><span class="math display">\[\begin{align}
f_{M|\mathbf{X}=\mathbf{x}}(\mu) = &amp;\, \frac{f_{\mathbf{X}|M=\mu}(\mathbf{x})\, f_{M}(\mu)}{f_{\mathbf{X}}(\mathbf{x})} \\
\propto &amp;\,
\exp\left( - \frac{1}{2} \sum_{i=1}^n \frac{(x_i-\mu)^2}{\sigma^2} \right) \exp\left( -\frac{1}{2} \frac{(\mu-\theta)^2}{\tau^2} \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{\sum_{i=1}^n x_i^2 - 2 \mu x + n \mu^2 }{\sigma^2} - \frac{1}{2} \frac{\mu^2-2\mu\theta + \theta^2}{\tau^2} \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{ n \mu^2 - 2 \mu x}{\sigma^2} - \frac{1}{2} \frac{\mu^2-2\mu\theta}{\tau^2} \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{  \mu^2\left(n \tau^2 + \sigma^2\right) - 2 \mu \tau^2 x -2\mu \sigma^2 \theta}{\tau^2\sigma^2 } \right) \notag \\
\propto&amp;\, \exp\left( - \frac{1}{2} \frac{ \mu^2 - 2 \mu \left(  x \frac{\tau^2}{n \tau^2 + \sigma^2} + \theta \frac{\sigma^2}{n \tau^2 + \sigma^2} \right) }{\frac{\tau^2\sigma^2}{n \tau^2 + \sigma^2} } \right) \notag \\
\propto&amp;\, \frac{1}{\sqrt{2\pi\left( \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} \right)} } \exp\left( - \frac{1}{2} \frac{ \left( \mu - \left( \frac{x}{n} \frac{n\tau^2}{n\tau^2 +\sigma^2} + \theta \frac{\sigma^2}{n\tau^2 +\sigma^2} \right) \right)^2 }{ \frac{\tau^2\sigma^2}{n\tau^2 +\sigma^2} } \right) , \notag
\end{align}\]</span></p>
<p>where <span class="math inline">\(x = \sum_{i=1}^n x_i\)</span>.</p>
</div>
<hr />
<p>The prior distribution hyperparameters and posterior distribution parameters can be interpreted in the normalnormal conjugate family:</p>
<ul>
<li>For the prior, <span class="math inline">\(\theta\)</span> represents the <em>a priori</em> value of the mean parameter, and <span class="math inline">\(\tau^2\)</span> is related to the precision of that prior mean (i.e., the larger the value, the less precise the prior mean is, and vice versa).</li>
<li>For the posterior, the new mean parameter is a weighted average between the prior mean parameter <span class="math inline">\(\theta\)</span> and the sample mean <span class="math inline">\(\tfrac{x}{n}\)</span>. The new variance parameter is informed by the prior variability <span class="math inline">\(\tau^2\)</span> and the variability of the data <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<hr />
<p><strong>Example 8.3.4. Impact of Normal Prior on Posterior.</strong>
Assume the following observed automobile claims for a small portfolio of policies:</p>
<p><span class="math display">\[
1050, \quad\quad 1250, \quad\quad 1550, \quad\quad 2600, \quad\quad 5350, \quad\quad 10200.
\]</span></p>
<p>Further assume that the logarithm of the claim amount follows a normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. Find the posterior distribution of the mean parameter <span class="math inline">\(\mu\)</span> for a normal prior distribution where <span class="math inline">\(\theta = 7\)</span>. Consider different values of <span class="math inline">\(\tau^2\)</span>; that is, <span class="math inline">\(\tau^2 = 0.1\)</span>, <span class="math inline">\(\tau^2 = 1\)</span>, and <span class="math inline">\(\tau^2 = 10\)</span>. Figure <a href="ChBayes.html#fig:ChBayesNormalPrior1">9.10</a> shows the pdf of these three prior distributions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesNormalPrior1"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesNormalPrior1-1.png" alt="Normal prior densities: \(\tau^2 = 0.1\) (light gray), \(\tau^2=1\) (gray), and \(\tau^2 = 10\) (black)" width="80%" />
<p class="caption">
Figure 9.10: <strong>Normal prior densities: <span class="math inline">\(\tau^2 = 0.1\)</span> (light gray), <span class="math inline">\(\tau^2=1\)</span> (gray), and <span class="math inline">\(\tau^2 = 10\)</span> (black)</strong>
</p>
</div>
<h5 style="text-align: center;">
<a id="displayExample.8.3.4" href="javascript:toggleEX('toggleExample.8.3.4','displayExample.8.3.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.3.4" style="display: none">
<p><strong>Solution.</strong>
Using the results of Proposition 8.3.3, we can obtain the following posterior distributions:</p>

<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="ChBayes.html#cb145-1" aria-hidden="true" tabindex="-1"></a>xi <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1050</span>, <span class="dv">1250</span>, <span class="dv">1550</span>, <span class="dv">2600</span>, <span class="dv">5350</span>, <span class="dv">10200</span>)</span>
<span id="cb145-2"><a href="ChBayes.html#cb145-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">log</span>(xi))</span>
<span id="cb145-3"><a href="ChBayes.html#cb145-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(xi)</span>
<span id="cb145-4"><a href="ChBayes.html#cb145-4" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb145-5"><a href="ChBayes.html#cb145-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-6"><a href="ChBayes.html#cb145-6" aria-hidden="true" tabindex="-1"></a>mean1 <span class="ot">&lt;-</span> theta <span class="sc">*</span> (sigma2<span class="sc">/</span>(n <span class="sc">*</span> tau21 <span class="sc">+</span> sigma2)) <span class="sc">+</span> x<span class="sc">/</span>n <span class="sc">*</span> ((n <span class="sc">*</span> tau21)<span class="sc">/</span>(n <span class="sc">*</span> tau21 <span class="sc">+</span></span>
<span id="cb145-7"><a href="ChBayes.html#cb145-7" aria-hidden="true" tabindex="-1"></a>    sigma2))</span>
<span id="cb145-8"><a href="ChBayes.html#cb145-8" aria-hidden="true" tabindex="-1"></a>mean2 <span class="ot">&lt;-</span> theta <span class="sc">*</span> (sigma2<span class="sc">/</span>(n <span class="sc">*</span> tau22 <span class="sc">+</span> sigma2)) <span class="sc">+</span> x<span class="sc">/</span>n <span class="sc">*</span> ((n <span class="sc">*</span> tau22)<span class="sc">/</span>(n <span class="sc">*</span> tau22 <span class="sc">+</span></span>
<span id="cb145-9"><a href="ChBayes.html#cb145-9" aria-hidden="true" tabindex="-1"></a>    sigma2))</span>
<span id="cb145-10"><a href="ChBayes.html#cb145-10" aria-hidden="true" tabindex="-1"></a>mean3 <span class="ot">&lt;-</span> theta <span class="sc">*</span> (sigma2<span class="sc">/</span>(n <span class="sc">*</span> tau23 <span class="sc">+</span> sigma2)) <span class="sc">+</span> x<span class="sc">/</span>n <span class="sc">*</span> ((n <span class="sc">*</span> tau23)<span class="sc">/</span>(n <span class="sc">*</span> tau23 <span class="sc">+</span></span>
<span id="cb145-11"><a href="ChBayes.html#cb145-11" aria-hidden="true" tabindex="-1"></a>    sigma2))</span>
<span id="cb145-12"><a href="ChBayes.html#cb145-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-13"><a href="ChBayes.html#cb145-13" aria-hidden="true" tabindex="-1"></a>var1 <span class="ot">&lt;-</span> (tau21 <span class="sc">*</span> sigma2)<span class="sc">/</span>(n <span class="sc">*</span> tau21 <span class="sc">+</span> sigma2)</span>
<span id="cb145-14"><a href="ChBayes.html#cb145-14" aria-hidden="true" tabindex="-1"></a>var2 <span class="ot">&lt;-</span> (tau22 <span class="sc">*</span> sigma2)<span class="sc">/</span>(n <span class="sc">*</span> tau22 <span class="sc">+</span> sigma2)</span>
<span id="cb145-15"><a href="ChBayes.html#cb145-15" aria-hidden="true" tabindex="-1"></a>var3 <span class="ot">&lt;-</span> (tau23 <span class="sc">*</span> sigma2)<span class="sc">/</span>(n <span class="sc">*</span> tau23 <span class="sc">+</span> sigma2)</span>
<span id="cb145-16"><a href="ChBayes.html#cb145-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-17"><a href="ChBayes.html#cb145-17" aria-hidden="true" tabindex="-1"></a>posterior1 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(xs, <span class="at">mean =</span> mean1, <span class="at">sd =</span> <span class="fu">sqrt</span>(var1))</span>
<span id="cb145-18"><a href="ChBayes.html#cb145-18" aria-hidden="true" tabindex="-1"></a>posterior2 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(xs, <span class="at">mean =</span> mean2, <span class="at">sd =</span> <span class="fu">sqrt</span>(var2))</span>
<span id="cb145-19"><a href="ChBayes.html#cb145-19" aria-hidden="true" tabindex="-1"></a>posterior3 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(xs, <span class="at">mean =</span> mean3, <span class="at">sd =</span> <span class="fu">sqrt</span>(var3))</span>
<span id="cb145-20"><a href="ChBayes.html#cb145-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-21"><a href="ChBayes.html#cb145-21" aria-hidden="true" tabindex="-1"></a>dataposterior <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> xs, <span class="at">y1 =</span> posterior1, <span class="at">y2 =</span> posterior2, <span class="at">y3 =</span> posterior3)</span>
<span id="cb145-22"><a href="ChBayes.html#cb145-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-23"><a href="ChBayes.html#cb145-23" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dataposterior, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y1)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;lightgray&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb145-24"><a href="ChBayes.html#cb145-24" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y2), <span class="at">color =</span> <span class="st">&quot;darkgray&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> y3),</span>
<span id="cb145-25"><a href="ChBayes.html#cb145-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span> <span class="fu">xlim</span>(<span class="dv">1</span>, <span class="dv">13</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="fl">1.75</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">expression</span>(<span class="fu">italic</span>(<span class="st">&quot;q&quot;</span>))) <span class="sc">+</span></span>
<span id="cb145-26"><a href="ChBayes.html#cb145-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ylab</span>(<span class="st">&quot;Posterior density&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesNormalPosterior2"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesNormalPosterior2-1.png" alt="Posterior densities based on three different priors: \(\tau^2 = 0.1\) (light gray), \(\tau^2=1\) (gray), and \(\tau^2 = 10\) (black)" width="80%" />
<p class="caption">
Figure 9.11: <strong>Posterior densities based on three different priors: <span class="math inline">\(\tau^2 = 0.1\)</span> (light gray), <span class="math inline">\(\tau^2=1\)</span> (gray), and <span class="math inline">\(\tau^2 = 10\)</span> (black)</strong>
</p>
</div>
</div>
<hr />
<p>Interestingly, as shown in Example 8.3.4, the prior distribution can have some impact on the final posterior distribution. When the prior assumption about the mean is very precise, having a few data points do not create a huge gap between the prior and the posterior (see the light gray curves in Figures <a href="ChBayes.html#fig:ChBayesNormalPrior1">9.10</a> and <a href="ChBayes.html#fig:ChBayesNormalPosterior2">9.11</a>). When the prior is very imprecise, on the other hand, then the data are allow to speak, and the posterior can be quite different from the prior distribution.</p>
</div>
<div id="criticism-of-conjugate-family-models" class="section level3 hasAnchor" number="9.3.4">
<h3><span class="header-section-number">9.3.4</span> Criticism of Conjugate Family Models<a href="ChBayes.html#criticism-of-conjugate-family-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While conjugate family models have some advantages, such as ease of interpretation and computational simplicity, they also have some limitations:</p>
<ol style="list-style-type: decimal">
<li>Conjugate families are oftentimes chosen for their mathematical convenience rather than their ability to accurately model the data under study. This can lead to models that are too simplistic and lack the flexibility needed to model real-world phenomena.</li>
<li>Conjugate family models rely on the choice of prior distribution, and different choices can lead to very different posterior distributions.</li>
<li>Conjugate family models are only applicable to a narrow range of problems, which limit their usefulness in practical applications.</li>
</ol>
<p>It is important to note that while conjugate family models have their limitations, they can still be useful in certain situations, especially when the assumptions of the model are well understood and the data are relatively simple.</p>
</div>
</div>
<div id="ChBayes:SecPosterior" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Posterior Simulation<a href="ChBayes.html#ChBayes:SecPosterior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In Section <a href="ChBayes.html#ChBayes:SecPosterior">9.4</a>, you learn how to:</p>
<ul>
<li>Use the standard computational tools for Bayesian inference.</li>
<li>Diagnose Markov chain convergence.</li>
</ul>
<hr />
<div id="introduction-to-markov-chain-monte-carlo-methods" class="section level3 hasAnchor" number="9.4.1">
<h3><span class="header-section-number">9.4.1</span> Introduction to Markov Chain Monte Carlo Methods<a href="ChBayes.html#introduction-to-markov-chain-monte-carlo-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sometimes, using conjugate family models is ill-suited for the problem at hand, and more complicated priors need to be selected. Under other circumstances, complex models involve many parameters making the posterior distribution intractable. In these cases, the posterior distribution of the parameters will not have a closed-form solution, generally speaking, and will need to be estimated via numerical methods.</p>
<p>A common way to generate draws of the parameter posterior distribution is to create Markov chains for which their stationary distributions correspond to the posterior of interest. These Markov chain-based methods are known as Markov chain Monte Carlo (MCMC) methods in the literature. This section provides a brief overview of these methods and of their uses. We do not intent to give much of the theory behind these methods, which would require a deep understanding of Markov chains and their theory.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> Instead, we focus on their applications in insurance and loss modeling. Specifically, in the next two subsections, we introduce the two most common MCMC methods; that is, the Gibbs sampler of <span class="citation">Gelfand and Smith (<a href="#ref-gelfand1990sampling" role="doc-biblioref">1990</a>)</span> and the MetropolisHastings algorithm of <span class="citation">Hastings (<a href="#ref-hastings1970monte" role="doc-biblioref">1970</a>)</span> and <span class="citation">Metropolis et al. (<a href="#ref-metropolis1953equation" role="doc-biblioref">1953</a>)</span>.</p>
</div>
<div id="the-gibbs-sampler" class="section level3 hasAnchor" number="9.4.2">
<h3><span class="header-section-number">9.4.2</span> The Gibbs Sampler<a href="ChBayes.html#the-gibbs-sampler" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned above, sometimes, we cannot use conjugate families. In other cases where the parameter space is large, it can be very hard to find the marginal likelihood <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span> (also known as the normalizing constant); that is, assuming that the model parameters are given by <span class="math inline">\(\pmb{\theta} = [\, \theta_1 \quad ... \theta_2 \quad ... \quad \theta_k \,]\)</span> and contains <span class="math inline">\(k\)</span> parameters, the marginal likelihood given by</p>
<p><span class="math display">\[
f_{\mathbf{X}}(\mathbf{x}) = \int \int ... \int f_{\mathbf{X}|\pmb{\Theta} = \pmb{\theta}} (\mathbf{x}) f_{\pmb{\Theta}}(\pmb{\theta}) \, d\theta_1 \,d\theta_2 \,... \,d\theta_k
\]</span></p>
<p>is hard to compute even when using typical quadrature-based rules, especially if <span class="math inline">\(k\)</span> is large.</p>
<p>Fortunately, under very mild regulatory conditions, samples of the joint estimates of parameters can be obtained by sequentially sampling each parameter individually and by keeping all the other parameters constant. To do so, the distribution of any given parameter conditional on all the other parameters (and the data) needs to be known. These distributions are known as full conditional distributions; that is,</p>
<p><span class="math display">\[
f_{\Theta_i \,|\, \mathbf{X} = \mathbf{x}, \, \pmb{\Theta}_{\backslash i} = \pmb{\theta}_{\backslash i}} (\theta_i),
\]</span></p>
<p>for parameter <span class="math inline">\(\theta_i\)</span>, where <span class="math inline">\(\pmb{\theta}_{\backslash i}\)</span> represents all parameters except for the <span class="math inline">\(i^{\text{th}}\)</span> one, and <span class="math inline">\(\pmb{\Theta}_{\backslash i}\)</span> is the random variable associated with this set of parameters.</p>
<p>The full conditional distribution is an important building block in Gibbs sampling. Indeed, if one can obtain each parameters distribution conditional on having the value of all the other parameters in closed form, then it is possible to generate samples for each parameter. Specifically, starting from an arbitrary set of starting values <span class="math inline">\(\pmb{\theta}^{(0)}_{\vphantom{2}} = [ \, {\theta}_1^{(0)} \quad {\theta}_2^{(0)} \quad ... \quad {\theta}_k^{(0)} \, ]\)</span>, samples for each parameter can be generated by performing the following steps for <span class="math inline">\(m = 1, 2, ..., M\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Draw <span class="math inline">\(\theta^{(m)}_1\)</span> from <span class="math inline">\(f_{\Theta_1^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_2^{\vphantom{(m)}} = \, \theta_2^{(m-1)}, \,..., \, \Theta_k^{\vphantom{(m)}} = \, \theta_k^{(m-1)}} (\theta_1)\)</span>.</li>
<li>Draw <span class="math inline">\(\theta^{(m)}_2\)</span> from <span class="math inline">\(f_{\Theta_2^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_1^{\vphantom{(m)}} = \, \theta_1^{(m)}, \, \Theta_3^{\vphantom{(m)}} = \,\theta_3^{(m-1)}, \,..., \, \Theta_k^{\vphantom{(m)}} = \,\theta_k^{(m-1)}} (\theta_2)\)</span>.</li>
<li>Draw <span class="math inline">\(\theta^{(m)}_3\)</span> from <span class="math inline">\(f_{\Theta_3^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_1^{\vphantom{(m)}} =\, \theta_1^{(m)}, \, \Theta_2^{\vphantom{(m)}} =\, \theta_2^{(m)}, \, \Theta_4^{\vphantom{(m)}} = \,\theta_4^{(m-1)}, \,..., \, \Theta_k^{\vphantom{(m)}} =\, \theta_k^{(m-1)}} (\theta_3)\)</span>.</li>
</ol>
<p> <span class="math inline">\(\vdots\)</span></p>
<p> <span class="math inline">\(k\)</span>. Draw <span class="math inline">\(\theta^{(m)}_k\)</span> from <span class="math inline">\(f_{\Theta_k^{\vphantom{(m)}} \, |\, \mathbf{X} = \mathbf{x}, \, \Theta_1^{\vphantom{(m)}} =\, \theta_1^{(m)}, \,..., \, \Theta_{k-1}^{\vphantom{(m)}} = \,\theta_{k-1}^{(m)}} (\theta_k)\)</span>.</p>
<p>The sample, especially at first, will depend on the initial values, <span class="math inline">\(\pmb{\theta}^{(0)}_{\vphantom{2}}\)</span>, and it might take some time until the sampler can explore fully the distribution. For this reason, in practice, experimenters discard the first <span class="math inline">\(M^*\)</span> iterations to make sure their analysis is not impacted by the choice of initial parameter; this initial period of discarded sample is known as the burn-in period.</p>
<p>The rest of the samplethe remaining <span class="math inline">\(M-M^*\)</span> iterationsis kept to estimate the posterior distribution and any quantities of interest.</p>
<div id="application-to-bayesian-linear-regression" class="section level4 hasAnchor" number="9.4.2.1">
<h4><span class="header-section-number">9.4.2.1</span> Application to Bayesian Linear Regression<a href="ChBayes.html#application-to-bayesian-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In statistics and in its most simple form, a linear regression is an approach for modeling the relationship between a scalar response and an explanatory variable. The former quantity is denoted by <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i \in \{1, ..., n\}\)</span>, and the latter quantity is denoted by <span class="math inline">\(z_{i}\)</span> for <span class="math inline">\(i \in \{1,...,n\}\)</span> in this chapter. Mathematically, we can write this relationship as:</p>
<p><span class="math display">\[
x_i = \alpha + \beta z_{i} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_i\)</span> is a disturbance term that captures the potential for errors in the linear relationship. This error term is typically assumed to be normally distributed with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>In general, the coefficients <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are unknown and need to be estimated. The experimenter can rely on Bayesian inference to find out the posterior distribution of the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> along with that of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>For the rest of the subsection, we investigate a specific application of Gibbs sampling to the context of linear regression.</p>
<p>We begin by computing the likelihood function conditional on the parameter values:</p>
<p><span class="math display">\[\begin{align*}
f_{\mathbf{X}\,|\,A=\alpha,\, B=\beta,\,\Sigma^2=\sigma^2}(\mathbf{x})=&amp;\, \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{\left( x_i - \alpha -\beta z_{i} \right)^{2}}{2 \sigma^2} \right) \\
=&amp;\, \left(  2 \pi \sigma^2 \right)^{-\tfrac{n}{2}} \exp\left(- \frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2 \sigma^2} \right),
\end{align*}\]</span></p>
<p>which is the first building block to construct our posterior distribution.</p>
<p>Then, we need a prior distribution, which could be informative, weakly informative, or noninformative. In this application, we select a prior that allows us to obtain each parameters full conditional distribution in closed form. Specifically, we use a normal distribution for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and an inverse gamma distribution for <span class="math inline">\(\sigma^2\)</span> with shape parameter <span class="math inline">\(\tfrac{n_{\sigma}}{2}\)</span> and scale parameter <span class="math inline">\(\tfrac{\theta_{\sigma}}{2}\)</span>, where</p>
<p><span class="math display">\[\begin{align*}
f_{A}(\alpha) =&amp;\, \frac{1}{\sqrt{2\pi \tau_{\alpha}^2\vphantom{2\pi \tau_{\beta}^2} }} \exp\left( - \frac{1}{2} \frac{(\alpha-\theta_{\alpha})^2}{\tau_{\alpha}^2 } \right), \\
f_{B}(\beta) =&amp;\,  \frac{1}{\sqrt{2\pi \tau_{\beta}^2 }} \exp\left( - \frac{1}{2} \frac{(\beta-\theta_{\beta})^2}{\tau_{\beta}^2 } \right), \\
f_{\Sigma^2}(\sigma^2) =&amp;\,  \frac{(\theta_{\sigma}/2)^{n_{\sigma}/2}}{\Gamma(n_{\sigma}/2)} \left( \frac{1}{\sigma^2} \right)^{n_{\sigma}/2+1} \exp\left( - \frac{\theta_{\sigma}/2}{\sigma^2} \right).
\end{align*}\]</span></p>
<hr />
<p><strong>Proposition 8.4.1. Full Conditional Distributions of Bayesian Linear Regression Parameters.</strong>
Consider a sample of <span class="math inline">\(n\)</span> observations <span class="math inline">\(\mathbf{x} = (x_1,...,x_n)\)</span> for which</p>
<p><span class="math display">\[
x_i = \alpha + \beta z_{i} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_i\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2\)</span>. The full conditional distributions of parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> are given by the following expressions:</p>
<p><span class="math display">\[\begin{align*}
A\sim&amp;\, \text{Normal}\left( \frac{1}{n} \left(\sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}, \frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2} \right), \\
B\sim&amp;\, \text{Normal}\left( \frac{1}{n}\left(  \sum_{i=1}^n z_i\left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 +\sigma^2}, \frac{\tau_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 +\sigma^2} \right)\!, \\
\Sigma^2\sim &amp;\, \text{Inverse Gamma}\left( \frac{n_{\sigma}+n}{2}, \frac{\theta_{\sigma} + \sum_{i=1}^n \left( y_i - \alpha - \beta z_{i} \right)^{2}}{2} \right),
\end{align*}\]</span></p>
<p>respectively, assuming the prior distributions mentioned above.</p>
<h5 style="text-align: center;">
<a id="displayTheory.Theory.6" href="javascript:toggleTheory('toggleTheory.Theory.6','displayCode.Theory.6');"><i><strong>Show A Snippet of Theory</strong></i></a>
</h5>
<div id="toggleTheory.Theory.6" style="display: none">
<p><strong>Proof.</strong>
From Section <a href="ChBayes.html#ChBayes:SecBuildingBlocks">9.2</a>, we know that</p>
<p><span class="math display">\[
f_{A,B,\Sigma^2|\mathbf{X}=\mathbf{x}}(\alpha,\beta,\sigma^2) \propto f_{\mathbf{X}|A=\alpha,\,  B=\beta,\,\Sigma^2=\sigma^2}(\mathbf{x})\,  f_{A}(\alpha)\, f_{B}(\beta) \, f_{\Sigma^2}(\sigma^2),
\]</span></p>
<p>which is useful to derive the full conditional distributions of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Let us begin with <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[\begin{align*}
&amp;\, f_{A|\mathbf{X}=\mathbf{x},\,B=\beta,\,\Sigma^2=\sigma^2}(\alpha) \\
\propto&amp;\, \exp\left(- \frac{1}{2}\frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \exp\left( - \frac{1}{2} \frac{(\alpha-\theta_{\alpha})^2}{\tau_{\alpha}^2 } \right)  \\
\propto &amp;\,\exp\left(-\frac{1}{2} \left( \frac{n \alpha^2 - 2 \alpha \sum_{i=1}^n (x_i - \beta z_i)}{\sigma^2} + \frac{\alpha^2 -2\alpha \theta_{\alpha} }{\tau_{\alpha}^2} \right) \right) \\
\propto &amp;\,\exp\left( -\frac{1}{2} \left( \frac{\alpha^2 - 2 \alpha \left(  \frac{1}{n}\left( \sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}\right)}{\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}}  \right) \right) \\
\propto &amp;\,\frac{1}{\sqrt{2\pi \left(\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}\right)}} \exp\left( -\frac{1}{2} \left( \frac{\left(\alpha - \left( \frac{1}{n}\left(  \sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}\right)\right)^2}{\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}}  \right) \right)
\end{align*}\]</span></p>
<p>which is a normal distribution with mean parameter</p>
<p><span class="math display">\[
\frac{1}{n} \left(\sum_{i=1}^n x_i - \beta z_i \right) \frac{n \tau_{\alpha}^2}{n\tau_{\alpha}^2 + \sigma^2} + \theta_{\alpha} \frac{\sigma^2}{n\tau_{\alpha}^2+\sigma^2}
\]</span></p>
<p>and variance parameter</p>
<p><span class="math display">\[
\frac{\tau_{\alpha}^2\sigma^2}{n\tau_{\alpha}^2+\sigma^2}.
\]</span></p>
<p>The derivation to obtain the full conditional distribution of <span class="math inline">\(\beta\)</span> is similar to that of <span class="math inline">\(\alpha\)</span>:</p>
<p><span class="math display">\[\begin{align*}
&amp;\, f_{B|\mathbf{X}=\mathbf{x},\,A=\alpha,\,\Sigma^2=\sigma^2}(\beta) \\
\propto&amp;\, \exp\left(- \frac{1}{2}\frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \exp\left( - \frac{1}{2} \frac{(\beta-\theta_{\beta})^2}{\tau_{\beta}^2 } \right)  \\
\propto &amp;\,\exp\left(-\frac{1}{2} \left( \frac{\beta^2 \sum_{i=1}^n z_i^2 - 2 \beta \sum_{i=1}^n z_i (x_i - \alpha)}{\sigma^2} + \frac{\beta^2 -2\beta \theta_{\beta} }{\tau_{\beta}^2} \right) \right) \\
\propto &amp;\,\exp\left( -\frac{1}{2} \left( \frac{\beta^2 - 2 \beta \left(  \frac{1}{n}\left( \sum_{i=1}^n z_i \left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2+\sigma^2}\right)}{\frac{\sigma_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2+\sigma^2}}  \right) \right) \\
\propto &amp;\,\frac{1}{\sqrt{2\pi \left(\frac{\tau_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 +\sigma^2}\right)}} \\
&amp;\, \times \exp\left( -\frac{1}{2} \left( \frac{\left(\beta - \left( \frac{1}{n}\left(  \sum_{i=1}^n z_i\left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 +\sigma^2}\right)\right)^2}{\frac{\tau_{\alpha}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2+\sigma^2}}  \right) \right)
\end{align*}\]</span></p>
<p>which is a normal distribution with mean parameter</p>
<p><span class="math display">\[
\frac{1}{n}\left(  \sum_{i=1}^n z_i\left( x_i - \alpha \right) \right) \frac{n \tau_{\beta}^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 + \sigma^2} + \theta_{\beta} \frac{\sigma^2}{\tau_{\beta}^2\sum_{i=1}^n z_i^2 +\sigma^2}
\]</span></p>
<p>and variance parameter</p>
<p><span class="math display">\[
\frac{\tau_{\beta}^2\sigma^2}{\tau_{\beta}^2 \sum_{i=1}^n z_i^2 +\sigma^2}.
\]</span></p>
<p>Finally, we apply the same logic to the variance parameter, <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[\begin{align*}
&amp;\,f_{\Sigma^2|\mathbf{X}=\mathbf{x},\,A=\alpha,\,B=\beta}(\sigma^2) \\
\propto&amp;\, \left(  2 \pi \sigma^2 \right)^{-n/2} \exp\left(- \frac{1}{2}\frac{\sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \left( \frac{1}{\sigma^2} \right)^{n_{\sigma}/2+1} \exp\left( - \frac{\theta_{\sigma}/2}{\sigma^2} \right) \\
\propto&amp;\, \exp\left(- \frac{1}{2}\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{\sigma^2} \right) \left( \frac{1}{\sigma^2} \right)^{(n_{\sigma}+n)/2+1} \\
\propto &amp;\, \frac{\left(\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2}\right)^{(n_{\sigma}+n)/2}}{\Gamma((n_{\sigma}+n)/2)} \left( \frac{1}{\sigma^2} \right)^{(n_{\sigma}+n)/2+1} \exp\left( - \frac{\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2}}{\sigma^2} \right),
\end{align*}\]</span></p>
<p>which is an inverse gamma distribution with shape parameter <span class="math inline">\(\tfrac{n_{\sigma}+n}{2}\)</span> and scale parameter</p>
<p><span class="math display">\[
\frac{\theta_{\sigma} + \sum_{i=1}^n \left( x_i - \alpha - \beta z_{i} \right)^{2}}{2}.
\]</span></p>
</div>
<hr />
<p>We now apply the Gibbs sampler on <em>real</em> data. The example will use motorcycle insurance data from Wasa, a Swedish insurance company, taken from <code>dataOhlsson</code> of the R package <code>insuranceData</code>; see <span class="citation">Wolny-Dominiak and Trzesiok (<a href="#ref-wolny2014package" role="doc-biblioref">2014</a>)</span> for more details.</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="ChBayes.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;insuranceData&quot;</span>)</span>
<span id="cb146-2"><a href="ChBayes.html#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(dataOhlsson)</span></code></pre></div>
<p>This dataset contains information about the number of motorcycle accidents, their claim cost, and some risk factors (e.g., the age of the driver, the age of the vehicle, the geographic zone).</p>
<hr />
<p><strong>Example 8.4.1. Bayesian Linear Regression.</strong>
You wish to understand the relationship between the age of the driver and the (logarithm of the) claim cost. Let <span class="math inline">\(x_i\)</span> be the logarithm of the <span class="math inline">\(i^{\text{th}}\)</span> claim cost and <span class="math inline">\(z_i\)</span> be the age associated with the <span class="math inline">\(i^{\text{th}}\)</span> claim. Further assume the following linear relationship between the two quantities:</p>
<p><span class="math display">\[
x_i = \alpha + \beta z_{i} + \varepsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_i\)</span> is normally distributed with mean zero and variance <span class="math inline">\(\sigma^2\)</span>. Find the posterior density of the three parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> using the Gibbs sampler.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.4.1" href="javascript:toggleEX('toggleExample.8.4.1','displayExample.8.4.1');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.4.1" style="display: none">
<p><strong>Solution.</strong>
Let us begin by visualizing the data. Figure <a href="ChBayes.html#fig:ChBayesGibbs1">9.12</a> reports the logarithm of the claim cost as a function of the drivers age. At first sight, it seems that the relationship between the claim cost and age is negative, so we should expect a negative <span class="math inline">\(\beta\)</span>, generally speaking.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesGibbs1"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesGibbs1-1.png" alt="Logarithm of the claim cost as a function of the drivers age" width="80%" />
<p class="caption">
Figure 9.12: <strong>Logarithm of the claim cost as a function of the drivers age</strong>
</p>
</div>
<p>Let us now turn to Bayesian inference via Gibbs sampling to find the posterior distribution of the three parameters of interest. We will use 10,000 iterations and discard the first 5,000 iterations (i.e., burn-in period). For our prior distributions, we use weakly informative priors by setting <span class="math inline">\(\theta_{\alpha} = \tfrac{1}{n} \sum_{i=1}^n x_i = \overline{x}\)</span>, <span class="math inline">\(\theta_{\beta} = 0\)</span>, <span class="math inline">\(\tau_{\alpha}^2=\tau_{\beta}^2 = 10\)</span>, <span class="math inline">\(n_{\sigma} = 1\)</span>, and <span class="math inline">\(\theta_{\sigma} = 0.1\)</span>. The initial values of the parameters are set to: <span class="math inline">\(\alpha^{(0)} = \overline{x}\)</span>, <span class="math inline">\(\beta^{(0)} = 0\)</span>, and <span class="math inline">\(\sigma^{2\,(0)} = \tfrac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2\)</span>.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="ChBayes.html#cb147-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb147-2"><a href="ChBayes.html#cb147-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;invgamma&quot;</span>)</span>
<span id="cb147-3"><a href="ChBayes.html#cb147-3" aria-hidden="true" tabindex="-1"></a>dataOhlsson <span class="ot">&lt;-</span> dataOhlsson[dataOhlsson<span class="sc">$</span>skadkost <span class="sc">&gt;</span> <span class="dv">0</span>, ]</span>
<span id="cb147-4"><a href="ChBayes.html#cb147-4" aria-hidden="true" tabindex="-1"></a>dataOhlsson<span class="sc">$</span>logskadkost <span class="ot">&lt;-</span> <span class="fu">log</span>(dataOhlsson<span class="sc">$</span>skadkost)</span>
<span id="cb147-5"><a href="ChBayes.html#cb147-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-6"><a href="ChBayes.html#cb147-6" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> dataOhlsson<span class="sc">$</span>logskadkost</span>
<span id="cb147-7"><a href="ChBayes.html#cb147-7" aria-hidden="true" tabindex="-1"></a>z <span class="ot">&lt;-</span> dataOhlsson<span class="sc">$</span>agarald</span>
<span id="cb147-8"><a href="ChBayes.html#cb147-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-9"><a href="ChBayes.html#cb147-9" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb147-10"><a href="ChBayes.html#cb147-10" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb147-11"><a href="ChBayes.html#cb147-11" aria-hidden="true" tabindex="-1"></a>Mstar <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb147-12"><a href="ChBayes.html#cb147-12" aria-hidden="true" tabindex="-1"></a>thetaa <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb147-13"><a href="ChBayes.html#cb147-13" aria-hidden="true" tabindex="-1"></a>tau2a <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb147-14"><a href="ChBayes.html#cb147-14" aria-hidden="true" tabindex="-1"></a>thetab <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb147-15"><a href="ChBayes.html#cb147-15" aria-hidden="true" tabindex="-1"></a>tau2b <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb147-16"><a href="ChBayes.html#cb147-16" aria-hidden="true" tabindex="-1"></a>nsigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb147-17"><a href="ChBayes.html#cb147-17" aria-hidden="true" tabindex="-1"></a>thetasigma <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb147-18"><a href="ChBayes.html#cb147-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-19"><a href="ChBayes.html#cb147-19" aria-hidden="true" tabindex="-1"></a>alphas <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb147-20"><a href="ChBayes.html#cb147-20" aria-hidden="true" tabindex="-1"></a>betas <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb147-21"><a href="ChBayes.html#cb147-21" aria-hidden="true" tabindex="-1"></a>sigma2s <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb147-22"><a href="ChBayes.html#cb147-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-23"><a href="ChBayes.html#cb147-23" aria-hidden="true" tabindex="-1"></a>alphas[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(x)</span>
<span id="cb147-24"><a href="ChBayes.html#cb147-24" aria-hidden="true" tabindex="-1"></a>betas[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb147-25"><a href="ChBayes.html#cb147-25" aria-hidden="true" tabindex="-1"></a>sigma2s[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">var</span>(x)</span>
<span id="cb147-26"><a href="ChBayes.html#cb147-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-27"><a href="ChBayes.html#cb147-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb147-28"><a href="ChBayes.html#cb147-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate alpha</span></span>
<span id="cb147-29"><a href="ChBayes.html#cb147-29" aria-hidden="true" tabindex="-1"></a>    den_alpha <span class="ot">&lt;-</span> n <span class="sc">*</span> tau2a <span class="sc">+</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb147-30"><a href="ChBayes.html#cb147-30" aria-hidden="true" tabindex="-1"></a>    mean_alpha <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> (<span class="fu">sum</span>(x <span class="sc">-</span> betas[m <span class="sc">-</span> <span class="dv">1</span>] <span class="sc">*</span> z)) <span class="sc">*</span> (n <span class="sc">*</span> tau2a)<span class="sc">/</span>den_alpha <span class="sc">+</span> thetaa <span class="sc">*</span></span>
<span id="cb147-31"><a href="ChBayes.html#cb147-31" aria-hidden="true" tabindex="-1"></a>        sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_alpha</span>
<span id="cb147-32"><a href="ChBayes.html#cb147-32" aria-hidden="true" tabindex="-1"></a>    var_alpha <span class="ot">&lt;-</span> tau2a <span class="sc">*</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_alpha</span>
<span id="cb147-33"><a href="ChBayes.html#cb147-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-34"><a href="ChBayes.html#cb147-34" aria-hidden="true" tabindex="-1"></a>    alphas[m] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mean_alpha, <span class="at">sd =</span> <span class="fu">sqrt</span>(var_alpha))</span>
<span id="cb147-35"><a href="ChBayes.html#cb147-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-36"><a href="ChBayes.html#cb147-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate beta</span></span>
<span id="cb147-37"><a href="ChBayes.html#cb147-37" aria-hidden="true" tabindex="-1"></a>    den_beta <span class="ot">&lt;-</span> tau2b <span class="sc">*</span> <span class="fu">sum</span>(z<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb147-38"><a href="ChBayes.html#cb147-38" aria-hidden="true" tabindex="-1"></a>    mean_beta <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>n) <span class="sc">*</span> (<span class="fu">sum</span>(z <span class="sc">*</span> (x <span class="sc">-</span> alphas[m]))) <span class="sc">*</span> (n <span class="sc">*</span> tau2b)<span class="sc">/</span>den_beta <span class="sc">+</span> thetab <span class="sc">*</span></span>
<span id="cb147-39"><a href="ChBayes.html#cb147-39" aria-hidden="true" tabindex="-1"></a>        sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_beta</span>
<span id="cb147-40"><a href="ChBayes.html#cb147-40" aria-hidden="true" tabindex="-1"></a>    var_beta <span class="ot">&lt;-</span> tau2b <span class="sc">*</span> sigma2s[m <span class="sc">-</span> <span class="dv">1</span>]<span class="sc">/</span>den_beta</span>
<span id="cb147-41"><a href="ChBayes.html#cb147-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-42"><a href="ChBayes.html#cb147-42" aria-hidden="true" tabindex="-1"></a>    betas[m] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> mean_beta, <span class="at">sd =</span> <span class="fu">sqrt</span>(var_beta))</span>
<span id="cb147-43"><a href="ChBayes.html#cb147-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-44"><a href="ChBayes.html#cb147-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate sigma</span></span>
<span id="cb147-45"><a href="ChBayes.html#cb147-45" aria-hidden="true" tabindex="-1"></a>    shape_sigma <span class="ot">&lt;-</span> (nsigma <span class="sc">+</span> n)<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb147-46"><a href="ChBayes.html#cb147-46" aria-hidden="true" tabindex="-1"></a>    scale_sigma <span class="ot">&lt;-</span> (thetasigma <span class="sc">+</span> <span class="fu">sum</span>((x <span class="sc">-</span> alphas[m] <span class="sc">-</span> betas[m] <span class="sc">*</span> z)<span class="sc">^</span><span class="dv">2</span>))<span class="sc">/</span><span class="dv">2</span></span>
<span id="cb147-47"><a href="ChBayes.html#cb147-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-48"><a href="ChBayes.html#cb147-48" aria-hidden="true" tabindex="-1"></a>    sigma2s[m] <span class="ot">&lt;-</span> <span class="fu">rinvgamma</span>(<span class="dv">1</span>, <span class="at">shape =</span> shape_sigma, <span class="at">scale =</span> scale_sigma)</span>
<span id="cb147-49"><a href="ChBayes.html#cb147-49" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Once we have the posterior parameter samples, we can get multiple quantities of interest. For instance, the posterior mean of parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span> are <span class="math inline">\(9.843\)</span>, <span class="math inline">\(-0.0208\)</span>, and <span class="math inline">\(3.517 \times 10^{-6}\)</span>, respectively.</p>
<pre><code>The posterior mean for coefficient alpha is 9.842986</code></pre>
<pre><code>The posterior mean for coefficient beta is -0.02080261</code></pre>
<pre><code>The posterior mean for the variance parameter is 0.000003516722</code></pre>
<p>We can also get histograms of the posterior distribution for <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, and <span class="math inline">\(\sigma^2\)</span>; Figure <a href="ChBayes.html#fig:ChBayesGibbs2">9.13</a> reports histograms for the three parameters. The uncertainty around each parameter is very small.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesGibbs2"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesGibbs2-1.png" alt="Histogram of the posterior distribution for parameters \(\alpha\) (top panel), \(\beta\) (middle panel), and \(\sigma^2\) (bottom panel)" width="80%" />
<p class="caption">
Figure 9.13: <strong>Histogram of the posterior distribution for parameters <span class="math inline">\(\alpha\)</span> (top panel), <span class="math inline">\(\beta\)</span> (middle panel), and <span class="math inline">\(\sigma^2\)</span> (bottom panel)</strong>
</p>
</div>
<p>The top panel of Figure <a href="ChBayes.html#fig:ChBayesGibbs5">9.14</a> reports a plot of the post-burn-in values of <span class="math inline">\(\alpha\)</span> as a function of the iteration number; this type of plot is known as a trace plot in the literature. These samples are not impacted by the initial parameter value that was selected. Indeed, after about 2030 iterations, the posterior parameter values obtained by the Gibbs sampler are very close to their posterior means. For instance, the bottom panel of Figure <a href="ChBayes.html#fig:ChBayesGibbs5">9.14</a> shows a plot of the first 50 values of <span class="math inline">\(\alpha\)</span> as a function of the iteration number.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesGibbs5"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesGibbs5-1.png" alt="Trace plot of \(\alpha\) for the post-burn-in iterations (top panel) and for the first 50 iterations (bottom panel)" width="80%" />
<p class="caption">
Figure 9.14: <strong>Trace plot of <span class="math inline">\(\alpha\)</span> for the post-burn-in iterations (top panel) and for the first 50 iterations (bottom panel)</strong>
</p>
</div>
</div>
<hr />
</div>
</div>
<div id="the-metropolishastings-algorithm" class="section level3 hasAnchor" number="9.4.3">
<h3><span class="header-section-number">9.4.3</span> The MetropolisHastings Algorithm<a href="ChBayes.html#the-metropolishastings-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gibbs sampling works well when the full conditional distribution for each parameter in the model can be found and is of a common form. This, unfortunately, is not always possible, meaning that we need to rely on other computational tools to find the posterior distribution of the parameters. One very popular method that copes with the shortcomings of Gibbs method is the MetropolisHastings sampler.</p>
<p>Let us assume that the current value of the first model parameter is <span class="math inline">\(\theta_1^{(0)}\)</span>. From this current value, we now wish to find a new value for this parameter. To do so, we propose a new value for this parameter, <span class="math inline">\(\theta^*_1\)</span>, from a candidate (or proposal) density <span class="math inline">\(q\left( \theta_1^* \,\middle|\, \theta_1^{(0)} \right)\)</span>. Since this proposal has nothing to do with the posterior distribution of the parameter, we should not keep all candidates in our final samplewe only accept those samples that are representative of the posterior distribution of interest. To determine whether we accept or reject the candidate, we compute a so-called acceptance ratio <span class="math inline">\(\alpha\left( \theta_1^{(0)}, \theta_1^* \right)\)</span> using</p>
<p><span class="math display">\[
\alpha\left(\theta_1^{(0)},\theta_1^*\right) = \frac{h\left(\theta_1^*\vphantom{\theta_1^{(1)}}\right) q\left( \theta_1^{(0)} \,\middle|\,\theta_1^*  \right)}{h\left(\theta_1^{(1)}\right) q\left( \theta_1^* \,\middle|\, \theta_1^{(0)} \right)}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
h(\theta_1) = f_{\mathbf{X}\,|\,\Theta_1 =  \,\theta_1, \, \pmb{\Theta}_{\backslash1}=\,\pmb{\theta}_{\backslash1}}(\mathbf{x}) \, f_{\Theta_1,\pmb{\Theta}_{\backslash1}}\left(\theta_1,\pmb{\theta}_{\backslash1}\right)
\]</span></p>
<p>and <span class="math inline">\(\pmb{\theta}_{\backslash 1}\)</span> represents all parameters except for the first one. Then, we accept the proposed value <span class="math inline">\(\theta_1^*\)</span> with probability <span class="math inline">\(\alpha\left( \theta_1^{(0)}, \theta_1^* \right)\)</span> and reject it with probability <span class="math inline">\(1-\alpha\left( \theta_1^{(0)}, \theta_1^* \right)\)</span>. Specifically,</p>
<p><span class="math display">\[
\theta_1^{(1)} = \left\{
\begin{array}{ll}
\theta_1^* &amp; \text{with probability }\,\alpha\left(\theta_1^{(0)},\theta_1^*\right) \\
\theta_1^{(0)} &amp;\text{with probability }\,1-\alpha\left(\theta_1^{(0)},\theta_1^*\right)
\end{array}
\right.
\]</span></p>
<p>Then, we can repeat the same process for all other parameters to obtain <span class="math inline">\(\theta_2^{(1)}\)</span> to <span class="math inline">\(\theta_k^{(1)}\)</span>, while replacing the parameters <span class="math inline">\(\theta_{\backslash i}\)</span> by their most current values in the chain. Once we have updated all values, we can repeat this process for all <span class="math inline">\(m\)</span> in <span class="math inline">\(\{2,3,...,M\}\)</span>, similar to the iterative process used in the Gibbs sampler.</p>
<p><strong>Special Case: Symmetric Proposal Distribution.</strong> If a proposal distribution is symmetric, then</p>
<p><span class="math display">\[
q\left( \theta_i^{(m)} \,\middle|\,\theta_i^*  \right) = q\left( \theta_i^*  \,\middle|\,\theta_i^{(m)} \right),
\]</span></p>
<p>and those terms cancel out, leaving</p>
<p><span class="math display">\[
\alpha\left(\theta_i^{(m)},\theta_1^*\right) = \frac{h\left(\theta_i^*\vphantom{\theta_1^{(1)}}\right)}{h\left(\theta_i^{(m)}\right) }.
\]</span></p>
<p>This special case is called the Metropolis algorithm.</p>
<hr />
<p>The MetropolisHastings sampler requires a lot of fine-tuning, generally speaking, because the experimenter needs to select one proposal distributions for each parameter. A common approach is to assume a normal proposal distribution centered at the previous value; that is,</p>
<p><span class="math display">\[
\Theta_i^* \sim \text{Normal}\left( \theta_i^{(m-1)} , \sigma_i^2 \right),
\]</span></p>
<p>at step <span class="math inline">\(m\)</span>, where <span class="math inline">\(\sigma_i^2\)</span> is the variance of the <span class="math inline">\(i^{\text{th}}\)</span> parameters proposal distribution.</p>
<hr />
<p><strong>Example 8.4.2. Impact of Proposal Density on the Acceptance Rate.</strong>
Assume that each policyholders claim count (frequency) is distributed as a Poisson random variable such that</p>
<p><span class="math display">\[
p_{N_i\,|\,\Lambda=\lambda}(n_i) = \frac{\lambda^{n_i}e^{-\lambda}}{n_i!},
\]</span></p>
<p>where <span class="math inline">\(n_i\)</span> is the number of claims associated with the <span class="math inline">\(i^{\text{th}}\)</span> policyholder. Further assume a noninformative, flat prior over <span class="math inline">\([0,\infty]\)</span>; that is,</p>
<p><span class="math display">\[
f_{\Lambda}(\lambda) \propto 1, \quad \lambda \in [0,\infty].
\]</span></p>
<p>Find the posterior distribution of the parameter using 1,000 iterations of the MetropolisHastings sampler assuming the claim count data of the Singapore Insurance Data (see Example 8.1.4 for more details). Use a normal proposal with small (<span class="math inline">\(1\times 10^{-7}\)</span>), moderate (<span class="math inline">\(1\times 10^{-4}\)</span>), and large (<span class="math inline">\(1 \times 10^{-1}\)</span>) values as the proposal variance in your tests and comment on the differences.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.4.2" href="javascript:toggleEX('toggleExample.8.4.2','displayExample.8.4.2');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.4.2" style="display: none">
<p><strong>Solution.</strong>
Starting from the the likelihood function and the prior distribution, we have that</p>
<p><span class="math display">\[
h(\lambda) \propto \prod_{i=1}^N \frac{\lambda^{n_i}e^{-\lambda}}{n_i!}.
\]</span></p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="ChBayes.html#cb151-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb151-2"><a href="ChBayes.html#cb151-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-3"><a href="ChBayes.html#cb151-3" aria-hidden="true" tabindex="-1"></a><span class="co"># First variance: 1 x 10^-7</span></span>
<span id="cb151-4"><a href="ChBayes.html#cb151-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb151-5"><a href="ChBayes.html#cb151-5" aria-hidden="true" tabindex="-1"></a>sigma21 <span class="ot">&lt;-</span> <span class="fl">0.0000001</span></span>
<span id="cb151-6"><a href="ChBayes.html#cb151-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-7"><a href="ChBayes.html#cb151-7" aria-hidden="true" tabindex="-1"></a>lambdas1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb151-8"><a href="ChBayes.html#cb151-8" aria-hidden="true" tabindex="-1"></a>lambdas1[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb151-9"><a href="ChBayes.html#cb151-9" aria-hidden="true" tabindex="-1"></a>accept1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb151-10"><a href="ChBayes.html#cb151-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-11"><a href="ChBayes.html#cb151-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb151-12"><a href="ChBayes.html#cb151-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb151-13"><a href="ChBayes.html#cb151-13" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb151-14"><a href="ChBayes.html#cb151-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-15"><a href="ChBayes.html#cb151-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb151-16"><a href="ChBayes.html#cb151-16" aria-hidden="true" tabindex="-1"></a>    lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma21))</span>
<span id="cb151-17"><a href="ChBayes.html#cb151-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb151-18"><a href="ChBayes.html#cb151-18" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb151-19"><a href="ChBayes.html#cb151-19" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb151-20"><a href="ChBayes.html#cb151-20" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb151-21"><a href="ChBayes.html#cb151-21" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb151-22"><a href="ChBayes.html#cb151-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-23"><a href="ChBayes.html#cb151-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb151-24"><a href="ChBayes.html#cb151-24" aria-hidden="true" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb151-25"><a href="ChBayes.html#cb151-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb151-26"><a href="ChBayes.html#cb151-26" aria-hidden="true" tabindex="-1"></a>        lambdas1[m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb151-27"><a href="ChBayes.html#cb151-27" aria-hidden="true" tabindex="-1"></a>        accept1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb151-28"><a href="ChBayes.html#cb151-28" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb151-29"><a href="ChBayes.html#cb151-29" aria-hidden="true" tabindex="-1"></a>        lambdas1[m] <span class="ot">&lt;-</span> lambdas1[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb151-30"><a href="ChBayes.html#cb151-30" aria-hidden="true" tabindex="-1"></a>        accept1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb151-31"><a href="ChBayes.html#cb151-31" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb151-32"><a href="ChBayes.html#cb151-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb151-33"><a href="ChBayes.html#cb151-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-34"><a href="ChBayes.html#cb151-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Second variance: 1 x 10^-4</span></span>
<span id="cb151-35"><a href="ChBayes.html#cb151-35" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb151-36"><a href="ChBayes.html#cb151-36" aria-hidden="true" tabindex="-1"></a>sigma22 <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb151-37"><a href="ChBayes.html#cb151-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-38"><a href="ChBayes.html#cb151-38" aria-hidden="true" tabindex="-1"></a>lambdas2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb151-39"><a href="ChBayes.html#cb151-39" aria-hidden="true" tabindex="-1"></a>lambdas2[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb151-40"><a href="ChBayes.html#cb151-40" aria-hidden="true" tabindex="-1"></a>accept2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb151-41"><a href="ChBayes.html#cb151-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-42"><a href="ChBayes.html#cb151-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb151-43"><a href="ChBayes.html#cb151-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb151-44"><a href="ChBayes.html#cb151-44" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb151-45"><a href="ChBayes.html#cb151-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-46"><a href="ChBayes.html#cb151-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb151-47"><a href="ChBayes.html#cb151-47" aria-hidden="true" tabindex="-1"></a>    lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma22))</span>
<span id="cb151-48"><a href="ChBayes.html#cb151-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb151-49"><a href="ChBayes.html#cb151-49" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb151-50"><a href="ChBayes.html#cb151-50" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb151-51"><a href="ChBayes.html#cb151-51" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb151-52"><a href="ChBayes.html#cb151-52" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb151-53"><a href="ChBayes.html#cb151-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-54"><a href="ChBayes.html#cb151-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb151-55"><a href="ChBayes.html#cb151-55" aria-hidden="true" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb151-56"><a href="ChBayes.html#cb151-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb151-57"><a href="ChBayes.html#cb151-57" aria-hidden="true" tabindex="-1"></a>        lambdas2[m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb151-58"><a href="ChBayes.html#cb151-58" aria-hidden="true" tabindex="-1"></a>        accept2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb151-59"><a href="ChBayes.html#cb151-59" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb151-60"><a href="ChBayes.html#cb151-60" aria-hidden="true" tabindex="-1"></a>        lambdas2[m] <span class="ot">&lt;-</span> lambdas2[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb151-61"><a href="ChBayes.html#cb151-61" aria-hidden="true" tabindex="-1"></a>        accept2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb151-62"><a href="ChBayes.html#cb151-62" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb151-63"><a href="ChBayes.html#cb151-63" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb151-64"><a href="ChBayes.html#cb151-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-65"><a href="ChBayes.html#cb151-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Third variance: 1 x 10^-1</span></span>
<span id="cb151-66"><a href="ChBayes.html#cb151-66" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb151-67"><a href="ChBayes.html#cb151-67" aria-hidden="true" tabindex="-1"></a>sigma23 <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb151-68"><a href="ChBayes.html#cb151-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-69"><a href="ChBayes.html#cb151-69" aria-hidden="true" tabindex="-1"></a>lambdas3 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb151-70"><a href="ChBayes.html#cb151-70" aria-hidden="true" tabindex="-1"></a>lambdas3[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count)</span>
<span id="cb151-71"><a href="ChBayes.html#cb151-71" aria-hidden="true" tabindex="-1"></a>accept3 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb151-72"><a href="ChBayes.html#cb151-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-73"><a href="ChBayes.html#cb151-73" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb151-74"><a href="ChBayes.html#cb151-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb151-75"><a href="ChBayes.html#cb151-75" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas3[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb151-76"><a href="ChBayes.html#cb151-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-77"><a href="ChBayes.html#cb151-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb151-78"><a href="ChBayes.html#cb151-78" aria-hidden="true" tabindex="-1"></a>    lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas3[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma23))</span>
<span id="cb151-79"><a href="ChBayes.html#cb151-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb151-80"><a href="ChBayes.html#cb151-80" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb151-81"><a href="ChBayes.html#cb151-81" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb151-82"><a href="ChBayes.html#cb151-82" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb151-83"><a href="ChBayes.html#cb151-83" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb151-84"><a href="ChBayes.html#cb151-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb151-85"><a href="ChBayes.html#cb151-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb151-86"><a href="ChBayes.html#cb151-86" aria-hidden="true" tabindex="-1"></a>    alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb151-87"><a href="ChBayes.html#cb151-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb151-88"><a href="ChBayes.html#cb151-88" aria-hidden="true" tabindex="-1"></a>        lambdas3[m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb151-89"><a href="ChBayes.html#cb151-89" aria-hidden="true" tabindex="-1"></a>        accept3[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb151-90"><a href="ChBayes.html#cb151-90" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb151-91"><a href="ChBayes.html#cb151-91" aria-hidden="true" tabindex="-1"></a>        lambdas3[m] <span class="ot">&lt;-</span> lambdas3[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb151-92"><a href="ChBayes.html#cb151-92" aria-hidden="true" tabindex="-1"></a>        accept3[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb151-93"><a href="ChBayes.html#cb151-93" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb151-94"><a href="ChBayes.html#cb151-94" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesMH1"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesMH1-1.png" alt="Trace plots based on three different proposals: \(\sigma^2 = 1 \times 10^{-7}\) (top panel), \(\sigma^2=1\times 10^{-4}\) (middle panel), and \(\sigma^2 = 1 \times 10^{-1}\) (bottom panel)" width="80%" />
<p class="caption">
Figure 9.15: <strong>Trace plots based on three different proposals: <span class="math inline">\(\sigma^2 = 1 \times 10^{-7}\)</span> (top panel), <span class="math inline">\(\sigma^2=1\times 10^{-4}\)</span> (middle panel), and <span class="math inline">\(\sigma^2 = 1 \times 10^{-1}\)</span> (bottom panel)</strong>
</p>
</div>
<p>Different variance parameters lead to different results. In this example, if <span class="math inline">\(\sigma^2\)</span> is too small, then the experimenter tends to draw samples that are very similar from one iteration to the other. This increases the acceptance rate (i.e., the rate at which we accept the proposal), but also means that the chain is travelling slowly around the posterior distribution. This ultimately imply that it will take longer chains to visit the whole posterior distribution. One way to see this issue in practice is by computing autocorrelation coefficients for the sample of parameter (more details on this in Section <a href="ChBayes.html#ChBayes:SubsecDiag">9.4.4</a>). The top panel of Figure <a href="ChBayes.html#fig:ChBayesMH1">9.15</a> indeed shows this strong autocorrelation and slow travelling around the posterior distribution.</p>
<p>On the other hand, if <span class="math inline">\(\sigma^2\)</span> is too large, then the proposal are seldom accepted, and the chain will tend to stickexhibiting long period for which the chain stays constant. For instance, the case with large proposal variance above leads to an acceptance rate of 1.2%, which is very low. The bottom panel of Figure <a href="ChBayes.html#fig:ChBayesMH1">9.15</a> reports this issue.</p>
<p>The moderate proposal variance case reports an acceptance rate of 32.4%, which is not too high nor too low. The general behavior of this chain resembles that of a hairy caterpillara good signmeaning that the mixing seems adequate and that we accept a decent amount of proposed values.</p>
<p>Finding the right proposal variance values for problems of interest requires some fine-tuning. As a general guideline, experimenters should target acceptance rates between 20% and 50%.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesMH2"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesMH2-1.png" alt="Posterior densities based on three different proposals: \(\sigma^2 = 1 \times 10^{-7}\) (top panel), \(\sigma^2=1\times 10^{-4}\) (middle panel), and \(\sigma^2 = 1 \times 10^{-1}\) (bottom panel)" width="80%" />
<p class="caption">
Figure 9.16: <strong>Posterior densities based on three different proposals: <span class="math inline">\(\sigma^2 = 1 \times 10^{-7}\)</span> (top panel), <span class="math inline">\(\sigma^2=1\times 10^{-4}\)</span> (middle panel), and <span class="math inline">\(\sigma^2 = 1 \times 10^{-1}\)</span> (bottom panel)</strong>
</p>
</div>
<p>Using the wrong proposal distribution can have an impact on the posterior distribution, as shown in Figure <a href="ChBayes.html#fig:ChBayesMH2">9.16</a>. A small variance takes a long time to travel throughout the posterior distribution, whereas a large variance tends to stick.</p>
</div>
<hr />
<p><strong>Example 8.4.3. Impact of Initial Parameters.</strong>
Consider the motorcycle insurance data from Wasa used in Example 8.4.1. We wish to model the claim amount from motorcycle losses with a gamma distribution; that is,</p>
<p><span class="math display">\[
f_{X_i\,|\,\Theta = \theta,\, A = \alpha}(x_i) = \frac{1}{\theta^{\alpha}\Gamma(\alpha)} x_i^{\alpha-1} e^{-\frac{x_i}{\theta}},
\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i^{\text{th}}\)</span> claim amount. We assume that the prior distributions for both <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\alpha\)</span> are noninformative and flat; that is,</p>
<p><span class="math display">\[
f_{\Theta , A}( \theta,\alpha) \propto 1, \quad \theta \in [0,\infty], \quad \alpha \in [0,\infty].
\]</span></p>
<p>Find the posterior distribution of the parameter using 1,000 iterations of the MetropolisHastings sampler. Use a normal proposal with a proposal variance <span class="math inline">\(5 \times 10^7\)</span> for <span class="math inline">\(\theta\)</span> and <span class="math inline">\(1 \times 10^{-2}\)</span> for <span class="math inline">\(\alpha\)</span>, and rely on <span class="math inline">\(\theta^{(0)} = 50,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 0.5\)</span> to start the MetropolisHastings sampler. Redo the experiment with <span class="math inline">\(\theta^{(0)} = 10,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 2.5\)</span>.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.4.3" href="javascript:toggleEX('toggleExample.8.4.3','displayExample.8.4.3');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.4.3" style="display: none">
<p><strong>Solution.</strong>
Starting from the the likelihood function and the prior distribution, we have that</p>
<p><span class="math display">\[
h(\theta,\alpha) \propto \prod_{i=1}^N \frac{1}{\theta^{\alpha}\Gamma(\alpha)} x_i^{\alpha-1} e^{-\frac{x_i}{\theta}}.
\]</span></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="ChBayes.html#cb152-1" aria-hidden="true" tabindex="-1"></a>dataOhlsson <span class="ot">&lt;-</span> dataOhlsson[dataOhlsson<span class="sc">$</span>skadkost <span class="sc">&gt;</span> <span class="dv">0</span>, ]</span>
<span id="cb152-2"><a href="ChBayes.html#cb152-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> dataOhlsson<span class="sc">$</span>skadkost</span>
<span id="cb152-3"><a href="ChBayes.html#cb152-3" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb152-4"><a href="ChBayes.html#cb152-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-5"><a href="ChBayes.html#cb152-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb152-6"><a href="ChBayes.html#cb152-6" aria-hidden="true" tabindex="-1"></a>sigma2theta <span class="ot">&lt;-</span> <span class="dv">50000000</span></span>
<span id="cb152-7"><a href="ChBayes.html#cb152-7" aria-hidden="true" tabindex="-1"></a>sigma2alpha <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb152-8"><a href="ChBayes.html#cb152-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-9"><a href="ChBayes.html#cb152-9" aria-hidden="true" tabindex="-1"></a><span class="co"># First set of initial values</span></span>
<span id="cb152-10"><a href="ChBayes.html#cb152-10" aria-hidden="true" tabindex="-1"></a>thetas1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb152-11"><a href="ChBayes.html#cb152-11" aria-hidden="true" tabindex="-1"></a>thetas1[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb152-12"><a href="ChBayes.html#cb152-12" aria-hidden="true" tabindex="-1"></a>alphas1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb152-13"><a href="ChBayes.html#cb152-13" aria-hidden="true" tabindex="-1"></a>alphas1[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb152-14"><a href="ChBayes.html#cb152-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-15"><a href="ChBayes.html#cb152-15" aria-hidden="true" tabindex="-1"></a>accepttheta1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb152-16"><a href="ChBayes.html#cb152-16" aria-hidden="true" tabindex="-1"></a>acceptalpha1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb152-17"><a href="ChBayes.html#cb152-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-18"><a href="ChBayes.html#cb152-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb152-19"><a href="ChBayes.html#cb152-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Let us start with theta Compute logarithm of h for past value</span></span>
<span id="cb152-20"><a href="ChBayes.html#cb152-20" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">shape =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-21"><a href="ChBayes.html#cb152-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-22"><a href="ChBayes.html#cb152-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb152-23"><a href="ChBayes.html#cb152-23" aria-hidden="true" tabindex="-1"></a>    thetastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> thetas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2theta))</span>
<span id="cb152-24"><a href="ChBayes.html#cb152-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb152-25"><a href="ChBayes.html#cb152-25" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetastar, <span class="at">shape =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-26"><a href="ChBayes.html#cb152-26" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-27"><a href="ChBayes.html#cb152-27" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb152-28"><a href="ChBayes.html#cb152-28" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-29"><a href="ChBayes.html#cb152-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-30"><a href="ChBayes.html#cb152-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb152-31"><a href="ChBayes.html#cb152-31" aria-hidden="true" tabindex="-1"></a>    alphatheta <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb152-32"><a href="ChBayes.html#cb152-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alphatheta <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb152-33"><a href="ChBayes.html#cb152-33" aria-hidden="true" tabindex="-1"></a>        thetas1[m] <span class="ot">&lt;-</span> thetastar</span>
<span id="cb152-34"><a href="ChBayes.html#cb152-34" aria-hidden="true" tabindex="-1"></a>        accepttheta1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb152-35"><a href="ChBayes.html#cb152-35" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-36"><a href="ChBayes.html#cb152-36" aria-hidden="true" tabindex="-1"></a>        thetas1[m] <span class="ot">&lt;-</span> thetas1[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb152-37"><a href="ChBayes.html#cb152-37" aria-hidden="true" tabindex="-1"></a>        accepttheta1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb152-38"><a href="ChBayes.html#cb152-38" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-39"><a href="ChBayes.html#cb152-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-40"><a href="ChBayes.html#cb152-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># And then deal with alpha Compute logarithm of h for past value</span></span>
<span id="cb152-41"><a href="ChBayes.html#cb152-41" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m], <span class="at">shape =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-42"><a href="ChBayes.html#cb152-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-43"><a href="ChBayes.html#cb152-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb152-44"><a href="ChBayes.html#cb152-44" aria-hidden="true" tabindex="-1"></a>    alphastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2alpha))</span>
<span id="cb152-45"><a href="ChBayes.html#cb152-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb152-46"><a href="ChBayes.html#cb152-46" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m], <span class="at">shape =</span> alphastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-47"><a href="ChBayes.html#cb152-47" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-48"><a href="ChBayes.html#cb152-48" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb152-49"><a href="ChBayes.html#cb152-49" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-50"><a href="ChBayes.html#cb152-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-51"><a href="ChBayes.html#cb152-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb152-52"><a href="ChBayes.html#cb152-52" aria-hidden="true" tabindex="-1"></a>    alphaalpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb152-53"><a href="ChBayes.html#cb152-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alphaalpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb152-54"><a href="ChBayes.html#cb152-54" aria-hidden="true" tabindex="-1"></a>        alphas1[m] <span class="ot">&lt;-</span> alphastar</span>
<span id="cb152-55"><a href="ChBayes.html#cb152-55" aria-hidden="true" tabindex="-1"></a>        acceptalpha1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb152-56"><a href="ChBayes.html#cb152-56" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-57"><a href="ChBayes.html#cb152-57" aria-hidden="true" tabindex="-1"></a>        alphas1[m] <span class="ot">&lt;-</span> alphas1[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb152-58"><a href="ChBayes.html#cb152-58" aria-hidden="true" tabindex="-1"></a>        acceptalpha1[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb152-59"><a href="ChBayes.html#cb152-59" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-60"><a href="ChBayes.html#cb152-60" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb152-61"><a href="ChBayes.html#cb152-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-62"><a href="ChBayes.html#cb152-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Second set of initial values</span></span>
<span id="cb152-63"><a href="ChBayes.html#cb152-63" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb152-64"><a href="ChBayes.html#cb152-64" aria-hidden="true" tabindex="-1"></a>thetas2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb152-65"><a href="ChBayes.html#cb152-65" aria-hidden="true" tabindex="-1"></a>thetas2[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb152-66"><a href="ChBayes.html#cb152-66" aria-hidden="true" tabindex="-1"></a>alphas2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb152-67"><a href="ChBayes.html#cb152-67" aria-hidden="true" tabindex="-1"></a>alphas2[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">2.5</span></span>
<span id="cb152-68"><a href="ChBayes.html#cb152-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-69"><a href="ChBayes.html#cb152-69" aria-hidden="true" tabindex="-1"></a>accepttheta2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb152-70"><a href="ChBayes.html#cb152-70" aria-hidden="true" tabindex="-1"></a>acceptalpha2 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, M)</span>
<span id="cb152-71"><a href="ChBayes.html#cb152-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-72"><a href="ChBayes.html#cb152-72" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb152-73"><a href="ChBayes.html#cb152-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Let us start with theta Compute logarithm of h for past value</span></span>
<span id="cb152-74"><a href="ChBayes.html#cb152-74" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">shape =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-75"><a href="ChBayes.html#cb152-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-76"><a href="ChBayes.html#cb152-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb152-77"><a href="ChBayes.html#cb152-77" aria-hidden="true" tabindex="-1"></a>    thetastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> thetas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2theta))</span>
<span id="cb152-78"><a href="ChBayes.html#cb152-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb152-79"><a href="ChBayes.html#cb152-79" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetastar, <span class="at">shape =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-80"><a href="ChBayes.html#cb152-80" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-81"><a href="ChBayes.html#cb152-81" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb152-82"><a href="ChBayes.html#cb152-82" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-83"><a href="ChBayes.html#cb152-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-84"><a href="ChBayes.html#cb152-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb152-85"><a href="ChBayes.html#cb152-85" aria-hidden="true" tabindex="-1"></a>    alphatheta <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb152-86"><a href="ChBayes.html#cb152-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alphatheta <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb152-87"><a href="ChBayes.html#cb152-87" aria-hidden="true" tabindex="-1"></a>        thetas2[m] <span class="ot">&lt;-</span> thetastar</span>
<span id="cb152-88"><a href="ChBayes.html#cb152-88" aria-hidden="true" tabindex="-1"></a>        accepttheta2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb152-89"><a href="ChBayes.html#cb152-89" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-90"><a href="ChBayes.html#cb152-90" aria-hidden="true" tabindex="-1"></a>        thetas2[m] <span class="ot">&lt;-</span> thetas2[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb152-91"><a href="ChBayes.html#cb152-91" aria-hidden="true" tabindex="-1"></a>        accepttheta2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb152-92"><a href="ChBayes.html#cb152-92" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-93"><a href="ChBayes.html#cb152-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-94"><a href="ChBayes.html#cb152-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># And then deal with alpha Compute logarithm of h for past value</span></span>
<span id="cb152-95"><a href="ChBayes.html#cb152-95" aria-hidden="true" tabindex="-1"></a>    loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas1[m], <span class="at">shape =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-96"><a href="ChBayes.html#cb152-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-97"><a href="ChBayes.html#cb152-97" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposed parameter and compute logarithm of h for proposed value</span></span>
<span id="cb152-98"><a href="ChBayes.html#cb152-98" aria-hidden="true" tabindex="-1"></a>    alphastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2alpha))</span>
<span id="cb152-99"><a href="ChBayes.html#cb152-99" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (thetastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb152-100"><a href="ChBayes.html#cb152-100" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dgamma</span>(x, <span class="at">scale =</span> thetas2[m], <span class="at">shape =</span> alphastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb152-101"><a href="ChBayes.html#cb152-101" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-102"><a href="ChBayes.html#cb152-102" aria-hidden="true" tabindex="-1"></a>        loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb152-103"><a href="ChBayes.html#cb152-103" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-104"><a href="ChBayes.html#cb152-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb152-105"><a href="ChBayes.html#cb152-105" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb152-106"><a href="ChBayes.html#cb152-106" aria-hidden="true" tabindex="-1"></a>    alphaalpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb152-107"><a href="ChBayes.html#cb152-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (alphaalpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb152-108"><a href="ChBayes.html#cb152-108" aria-hidden="true" tabindex="-1"></a>        alphas2[m] <span class="ot">&lt;-</span> alphastar</span>
<span id="cb152-109"><a href="ChBayes.html#cb152-109" aria-hidden="true" tabindex="-1"></a>        acceptalpha2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb152-110"><a href="ChBayes.html#cb152-110" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb152-111"><a href="ChBayes.html#cb152-111" aria-hidden="true" tabindex="-1"></a>        alphas2[m] <span class="ot">&lt;-</span> alphas2[m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb152-112"><a href="ChBayes.html#cb152-112" aria-hidden="true" tabindex="-1"></a>        acceptalpha2[m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb152-113"><a href="ChBayes.html#cb152-113" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb152-114"><a href="ChBayes.html#cb152-114" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesInit2"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesInit2-1.png" alt="Trace plots based on two different starting parameter sets: \(\theta^{(0)} = 50,000\) and \(\alpha^{(0)} = 0.5\) (left panels), and \(\theta^{(0)} = 10,000\) and \(\alpha^{(0)} = 2.5\) (right panels)" width="80%" />
<p class="caption">
Figure 9.17: <strong>Trace plots based on two different starting parameter sets: <span class="math inline">\(\theta^{(0)} = 50,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 0.5\)</span> (left panels), and <span class="math inline">\(\theta^{(0)} = 10,000\)</span> and <span class="math inline">\(\alpha^{(0)} = 2.5\)</span> (right panels)</strong>
</p>
</div>
<p>Clearly, from Figure <a href="ChBayes.html#fig:ChBayesInit2">9.17</a>, the initial parameter value matters: for the first set, the starting value is close to the posterior mode, meaning that the final sample does not depend much on the starting value. For the second set, on the other hand, it takes about 200 iterations to get closer to where most of the density resides. Having a burn-in in the case of MetropolisHastings sampler is therefore a good idea to reduce the impact of initial guesses on the final posterior distribution.</p>
</div>
<hr />
<p>In the next subsection, we learn a few methods and metrics to diagnose the convergence of the Markov chains generated via MCMC methods.</p>
</div>
<div id="ChBayes:SubsecDiag" class="section level3 hasAnchor" number="9.4.4">
<h3><span class="header-section-number">9.4.4</span> Markov Chain Diagnostics<a href="ChBayes.html#ChBayes:SubsecDiag" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many different tuning parameters in MCMC schemes, and they all have an impact on the convergence of the Markov chains generated by these methods. To understand the impact of these choices on the chains (e.g., number of iterations, length of burn-in, proposal distribution), we introduce a few methods to analyze their convergence.</p>
<div id="examining-trace-plots-and-autocorrelation" class="section level4 hasAnchor" number="9.4.4.1">
<h4><span class="header-section-number">9.4.4.1</span> Examining Trace Plots and Autocorrelation<a href="ChBayes.html#examining-trace-plots-and-autocorrelation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Trace Plot.</strong> The most elementary tool to assess whether MCMC chains have converged to the posterior distribution is the trace plot. As mentioned above, a trace plot displays the sequence of samples as a function of the iteration number, with the sample value on the <span class="math inline">\(y\)</span>-axis and the iteration number on the <span class="math inline">\(x\)</span>-axis. If the chain has converged, the trace plot should show a stable sequence of samples around the true posterior distribution that looks like a hairy caterpillar. However, if the chain has not yet converged, the trace plot may show a sequence of samples that still appear to be changing or have not yet settled into a stable pattern.</p>
<p>In addition to assessing convergence, trace plots can also be used to diagnose potential problems with MCMC algorithms, such as poor mixing or autocorrelation. For example, if the trace plot shows long periods of no change followed by abrupt jumps, this may indicate poor mixing and suggest that the MCMC algorithm needs to be adjusted or a different method should be used.</p>
<p><strong>Lag-1 Autocorrelation.</strong> Another quantity that might be helpful is the lag-1 autocorrelationthe correlation between consecutive samples in a given chain:</p>
<p><span class="math display">\[
\mathrm{Cov}\left[ \theta_i^{(m)}, \theta_i^{(m-1)} \right].
\]</span></p>
<p>Note that if the autocorrelation is too high, it can indicate that the chain is not mixing well and is not sampling the posterior distribution effectively. This can result in poor convergence, longer run times, and decreased precision of the estimates obtained from the MCMC algorithm.</p>
<p>In addition to examining trace plots and computing autocorrelation coefficients, we can use other, more formal tools to evaluate whether the chains obtained are reliable and have converged.</p>
</div>
<div id="comparing-parallel-chains" class="section level4 hasAnchor" number="9.4.4.2">
<h4><span class="header-section-number">9.4.4.2</span> Comparing Parallel Chains<a href="ChBayes.html#comparing-parallel-chains" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>GelmanRubin Statistic</strong> Another way to assess convergence is to run multiple chains in parallel from different starting points and check if their behavior is similar. In addition to comparing their trace plots, the chains can be compared by using a statistical testthe GelmanRubin test of <span class="citation">Gelman and Rubin (<a href="#ref-gelman1992inference" role="doc-biblioref">1992</a>)</span>. The latter test compares the within-chain variance to the between-chain variance; to calculate the statistic, we need to generate a small number of chains (say, <span class="math inline">\(R\)</span>), each for <span class="math inline">\(M-M^*\)</span> post-burn-in iterations.</p>
<p>If the chains have converged, the within-chain variance should be similar to the between-chain variance. Assuming the parameter of interest is <span class="math inline">\(\theta_i\)</span>, the within-chain variance is</p>
<p><span class="math display">\[
W = \frac{1}{R(M-M^*-1)} \sum_{r=1}^R \sum_{m=M^*+1}^{M} \left( \theta_{i,r}^{(m)} - \overline{\theta}_{i,r} \right)^2 ,
\]</span></p>
<p>where <span class="math inline">\(\theta_{i,r}^{(m)}\)</span> is the <span class="math inline">\(m^{\text{th}}\)</span> draw of <span class="math inline">\(\theta_i\)</span> in the <span class="math inline">\(r^{\text{th}}\)</span> chain and <span class="math inline">\(\overline{\theta}_{i,r}\)</span> is the sample mean of <span class="math inline">\(\theta_i\)</span> for the <span class="math inline">\(r^{\text{th}}\)</span> chain. The between-chain variance is given by</p>
<p><span class="math display">\[
B = \frac{M-M^*}{R-1} \sum_{r=1}^R \left( \overline{\theta}_{i,r} - \overline{\theta}_{i} \right),
\]</span></p>
<p>where <span class="math inline">\(\overline{\theta}_{i}\)</span> is the overall sample mean of <span class="math inline">\(\theta_i\)</span> from all chains. The GelmanRubin statistic is</p>
<p><span class="math display">\[
\sqrt{\left( \frac{M-M^*-1}{M-M^*} + \frac{R+1}{R(M-M^*)} \frac{B}{W} \right) \frac{\text{df}}{\text{df}-2} },
\]</span></p>
<p>where <span class="math inline">\(\text{df}\)</span> is the degrees of freedom from Students <span class="math inline">\(t\)</span>-distribution that approximates the posterior distribution. The statistic should produce a value close to 1 if the chain has converged. On the other hand, if the statistic value is greater than 1.1 or 1.2, this indicates that the chains may not have converged, and further analysis may be needed to determine why the chains are not mixing well.</p>
</div>
<div id="calculating-effective-sample-sizes" class="section level4 hasAnchor" number="9.4.4.3">
<h4><span class="header-section-number">9.4.4.3</span> Calculating Effective Sample Sizes<a href="ChBayes.html#calculating-effective-sample-sizes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Effective Sample Size</strong> The effective sample size (ESS) is a measure of the number of independent samples obtained from an MCMC chain. Recall that in an MCMC chain, each sample is correlated with the previous sample; as a result, the effective number of independent samples is usually much smaller than the total number of samples generated by the MCMC algorithm. The ESS takes this correlation into account and provides an estimate of the number of independent samples that are equivalent to the correlated samples in the chain.</p>
<p>In general, a higher effective sample size indicates that the MCMC algorithm has produced more independent samples and is more likely to have accurately sampled the posterior distribution. A lower effective sample size, on the other hand, suggests that the MCMC algorithm may require further tuning or optimization to produce reliable posterior estimates.</p>
<p>The function <code>multiESS</code>of the R package <code>mcmcse</code> contains a function that gives the ESS of a multivariate Markov chain as described in <span class="citation">Vats, Flegal, and Jones (<a href="#ref-vats2019multivariate" role="doc-biblioref">2019</a>)</span>. The package also includes an estimate of the minimum ESS required for a specified relative tolerance level (see function <code>minESS</code>).</p>
<p>We now apply these various diagnostics to an example.</p>
<hr />
<p><strong>Example 8.4.4. Markov Chain Diagnostics.</strong>
Consider the setup of Example 8.4.2. Using chains of 51,000 iterations and a burn-in of 1,000 iterations, calculate the various Markov chain diagnostics mentioned above.</p>
<h5 style="text-align: center;">
<a id="displayExample.8.4.4" href="javascript:toggleEX('toggleExample.8.4.4','displayExample.8.4.4');"><i><strong>Show Example Solution</strong></i></a>
</h5>
<div id="toggleExample.8.4.4" style="display: none">
<p><strong>Solution.</strong>
Let us begin by generating five chains.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="ChBayes.html#cb153-1" aria-hidden="true" tabindex="-1"></a>M <span class="ot">&lt;-</span> <span class="dv">51000</span></span>
<span id="cb153-2"><a href="ChBayes.html#cb153-2" aria-hidden="true" tabindex="-1"></a>Mstar <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb153-3"><a href="ChBayes.html#cb153-3" aria-hidden="true" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb153-4"><a href="ChBayes.html#cb153-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-5"><a href="ChBayes.html#cb153-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb153-6"><a href="ChBayes.html#cb153-6" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="fl">0.0001</span></span>
<span id="cb153-7"><a href="ChBayes.html#cb153-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-8"><a href="ChBayes.html#cb153-8" aria-hidden="true" tabindex="-1"></a>lambdas <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">ncol =</span> M <span class="sc">+</span> <span class="dv">1</span>, <span class="at">nrow =</span> R)</span>
<span id="cb153-9"><a href="ChBayes.html#cb153-9" aria-hidden="true" tabindex="-1"></a>accept <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">data =</span> <span class="cn">NA</span>, <span class="at">ncol =</span> M, <span class="at">nrow =</span> R)</span>
<span id="cb153-10"><a href="ChBayes.html#cb153-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-11"><a href="ChBayes.html#cb153-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb153-12"><a href="ChBayes.html#cb153-12" aria-hidden="true" tabindex="-1"></a>    lambdas[r, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fl">0.000001</span>, <span class="fu">mean</span>(sgautonb<span class="sc">$</span>Clm_Count) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>,</span>
<span id="cb153-13"><a href="ChBayes.html#cb153-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">sd =</span> <span class="fl">0.1</span>))</span>
<span id="cb153-14"><a href="ChBayes.html#cb153-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-15"><a href="ChBayes.html#cb153-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>(M <span class="sc">+</span> <span class="dv">1</span>)) {</span>
<span id="cb153-16"><a href="ChBayes.html#cb153-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute logarithm of h for past value</span></span>
<span id="cb153-17"><a href="ChBayes.html#cb153-17" aria-hidden="true" tabindex="-1"></a>        loghpast <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdas[r, m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb153-18"><a href="ChBayes.html#cb153-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-19"><a href="ChBayes.html#cb153-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate proposed parameter and compute logarithm of h for proposed</span></span>
<span id="cb153-20"><a href="ChBayes.html#cb153-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># value</span></span>
<span id="cb153-21"><a href="ChBayes.html#cb153-21" aria-hidden="true" tabindex="-1"></a>        lambdastar <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> lambdas[r, m <span class="sc">-</span> <span class="dv">1</span>], <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2))</span>
<span id="cb153-22"><a href="ChBayes.html#cb153-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (lambdastar <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb153-23"><a href="ChBayes.html#cb153-23" aria-hidden="true" tabindex="-1"></a>            loghstar <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">dpois</span>(sgautonb<span class="sc">$</span>Clm_Count, <span class="at">lambda =</span> lambdastar, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb153-24"><a href="ChBayes.html#cb153-24" aria-hidden="true" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb153-25"><a href="ChBayes.html#cb153-25" aria-hidden="true" tabindex="-1"></a>            loghstar <span class="ot">&lt;-</span> <span class="sc">-</span><span class="cn">Inf</span></span>
<span id="cb153-26"><a href="ChBayes.html#cb153-26" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb153-27"><a href="ChBayes.html#cb153-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb153-28"><a href="ChBayes.html#cb153-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute acceptance probability and copy new parameter value</span></span>
<span id="cb153-29"><a href="ChBayes.html#cb153-29" aria-hidden="true" tabindex="-1"></a>        alpha <span class="ot">&lt;-</span> <span class="fu">exp</span>(loghstar <span class="sc">-</span> loghpast)</span>
<span id="cb153-30"><a href="ChBayes.html#cb153-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (alpha <span class="sc">&gt;</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">1</span>)) {</span>
<span id="cb153-31"><a href="ChBayes.html#cb153-31" aria-hidden="true" tabindex="-1"></a>            lambdas[r, m] <span class="ot">&lt;-</span> lambdastar</span>
<span id="cb153-32"><a href="ChBayes.html#cb153-32" aria-hidden="true" tabindex="-1"></a>            accept[r, m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb153-33"><a href="ChBayes.html#cb153-33" aria-hidden="true" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb153-34"><a href="ChBayes.html#cb153-34" aria-hidden="true" tabindex="-1"></a>            lambdas[r, m] <span class="ot">&lt;-</span> lambdas[r, m <span class="sc">-</span> <span class="dv">1</span>]</span>
<span id="cb153-35"><a href="ChBayes.html#cb153-35" aria-hidden="true" tabindex="-1"></a>            accept[r, m <span class="sc">-</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb153-36"><a href="ChBayes.html#cb153-36" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb153-37"><a href="ChBayes.html#cb153-37" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb153-38"><a href="ChBayes.html#cb153-38" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb153-39"><a href="ChBayes.html#cb153-39" aria-hidden="true" tabindex="-1"></a><span class="fu">save</span>(lambdas, accept, sgautonb, alpha, lambdastar, loghpast, loghstar, m, M, Mstar,</span>
<span id="cb153-40"><a href="ChBayes.html#cb153-40" aria-hidden="true" tabindex="-1"></a>    r, R, sigma2, <span class="at">file =</span> <span class="st">&quot;../IntermediateCalcs/BayesChap/Example844.Rdata&quot;</span>)</span></code></pre></div>
<p>Figure <a href="ChBayes.html#fig:ChBayesDiag2">9.18</a> reports the trace plot for the first chain: it indeed looks like a hairy caterpillar, which is a good sign.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ChBayesDiag2"></span>
<img src="LossDataAnalytics_files/figure-html/ChBayesDiag2-1.png" alt="Trace plot for parameter \(\lambda\)" width="80%" />
<p class="caption">
Figure 9.18: <strong>Trace plot for parameter <span class="math inline">\(\lambda\)</span></strong>
</p>
</div>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="ChBayes.html#cb154-1" aria-hidden="true" tabindex="-1"></a>autocorr <span class="ot">&lt;-</span> <span class="fu">acf</span>(lambdas[<span class="dv">1</span>, (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M], <span class="at">lag.max =</span> <span class="dv">1</span>, <span class="at">plot =</span> <span class="cn">FALSE</span>)</span>
<span id="cb154-2"><a href="ChBayes.html#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The lag-1 autocorrelation coefficient is&quot;</span>, autocorr<span class="sc">$</span>acf[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>The lag-1 autocorrelation coefficient is 0.6515109</code></pre>
<p>The autocorrelation is also mild at 65%, again pointing towards good convergence behavior.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="ChBayes.html#cb156-1" aria-hidden="true" tabindex="-1"></a>W <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb156-2"><a href="ChBayes.html#cb156-2" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb156-3"><a href="ChBayes.html#cb156-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>R) {</span>
<span id="cb156-4"><a href="ChBayes.html#cb156-4" aria-hidden="true" tabindex="-1"></a>    W <span class="ot">&lt;-</span> W <span class="sc">+</span> <span class="dv">1</span><span class="sc">/</span>(R <span class="sc">*</span> (M <span class="sc">-</span> Mstar <span class="sc">-</span> <span class="dv">1</span>)) <span class="sc">*</span> <span class="fu">sum</span>((lambdas[r, (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M] <span class="sc">-</span> <span class="fu">mean</span>(lambdas[r,</span>
<span id="cb156-5"><a href="ChBayes.html#cb156-5" aria-hidden="true" tabindex="-1"></a>        (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb156-6"><a href="ChBayes.html#cb156-6" aria-hidden="true" tabindex="-1"></a>    B <span class="ot">&lt;-</span> B <span class="sc">+</span> (M <span class="sc">-</span> Mstar)<span class="sc">/</span>(R <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">sum</span>((<span class="fu">mean</span>(lambdas[r, (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M]) <span class="sc">-</span> <span class="fu">mean</span>(lambdas[,</span>
<span id="cb156-7"><a href="ChBayes.html#cb156-7" aria-hidden="true" tabindex="-1"></a>        (Mstar <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>M]))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb156-8"><a href="ChBayes.html#cb156-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb156-9"><a href="ChBayes.html#cb156-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming that df/(df-2) tends to 1</span></span>
<span id="cb156-10"><a href="ChBayes.html#cb156-10" aria-hidden="true" tabindex="-1"></a>GR <span class="ot">&lt;-</span> <span class="fu">sqrt</span>((M <span class="sc">-</span> Mstar <span class="sc">-</span> <span class="dv">1</span>)<span class="sc">/</span>(M <span class="sc">-</span> Mstar) <span class="sc">+</span> (R <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">/</span>(R <span class="sc">*</span> (M <span class="sc">-</span> Mstar)) <span class="sc">*</span> B<span class="sc">/</span>W)</span>
<span id="cb156-11"><a href="ChBayes.html#cb156-11" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The Gelman-Rubin statistic is&quot;</span>, GR)</span></code></pre></div>
<pre><code>The Gelman-Rubin statistic is 1.000127</code></pre>
<p>The GelmanRubin statistic is very close to 1 in this case, meaning that the chain converged.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="ChBayes.html#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;mcmcse&quot;</span>)</span>
<span id="cb158-2"><a href="ChBayes.html#cb158-2" aria-hidden="true" tabindex="-1"></a>ess <span class="ot">&lt;-</span> <span class="fu">multiESS</span>(<span class="fu">matrix</span>(lambdas[<span class="dv">1</span>, ]))</span>
<span id="cb158-3"><a href="ChBayes.html#cb158-3" aria-hidden="true" tabindex="-1"></a>mess <span class="ot">&lt;-</span> <span class="fu">minESS</span>(<span class="at">p =</span> <span class="dv">1</span>)</span>
<span id="cb158-4"><a href="ChBayes.html#cb158-4" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;The ESS is&quot;</span>, ess, <span class="st">&quot;and the minimum ESS is&quot;</span>, mess)</span></code></pre></div>
<pre><code>The ESS is 9927.299 and the minimum ESS is 6146</code></pre>
<p>The last diagnostic refers to the ESS, and its comparison to the minimum ESS. In our case, the ESS is about 9,927, and the minimum ESS is 6,146. Since our ESS is above the minimum, we know we have a large enough sample to adequately capture the posterior distribution of <span class="math inline">\(\lambda\)</span>.</p>
</div>
<hr />
</div>
</div>
</div>
<div id="ChBayes:SecFurther" class="section level2 hasAnchor" number="9.5">
<h2><span class="header-section-number">9.5</span> Further Resources and Contributors<a href="ChBayes.html#ChBayes:SecFurther" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many great books exist on Bayesian statistics and MCMC schemes. We refer the interested reader to <span class="citation">Bernardo and Smith (<a href="#ref-bernardo2009bayesian" role="doc-biblioref">2009</a>)</span> and <span class="citation">Robert and Casella (<a href="#ref-robert1999monte" role="doc-biblioref">1999</a>)</span> for an advanced treatment of these topics.</p>
<div id="contributors-8" class="section level3 unnumbered hasAnchor">
<h3>Contributors<a href="ChBayes.html#contributors-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><strong>Jean-Franois Bgin</strong>, Simon Fraser University, is the principal author of the initial version of this chapter. Email: <a href="mailto:jbegin@sfu.ca" class="email">jbegin@sfu.ca</a> for chapter comments and suggested improvements.</li>
<li>Chapter reviewers include: Brian Hartman.</li>
</ul>

</div>
</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bernardo2009bayesian" class="csl-entry">
Bernardo, Jos M, and Adrian FM Smith. 2009. <em>Bayesian Theory</em>. John Wiley &amp; Sons: New York, NY, United States of America.
</div>
<div id="ref-cowles2013applied" class="csl-entry">
Cowles, Mary Kathryn. 2013. <em>Applied Bayesian Statistics: With <span>R</span> and <span>OpenBUGS</span> Examples</em>. Springer Science &amp; Business Media: New York, NY, United States of America.
</div>
<div id="ref-gelfand1990sampling" class="csl-entry">
Gelfand, Alan E, and Adrian FM Smith. 1990. <span>Sampling-Based Approaches to Calculating Marginal Densities.</span> <em>Journal of the American Statistical Association</em> 85 (410): 398409.
</div>
<div id="ref-gelman1992inference" class="csl-entry">
Gelman, Andrew, and Donald B Rubin. 1992. <span>Inference from Iterative Simulation Using Multiple Sequences.</span> <em>Statistical Science</em>, 45772.
</div>
<div id="ref-hastings1970monte" class="csl-entry">
Hastings, WK. 1970. <span>Monte Carlo Sampling Methods Using Markov Chains and Their Applications.</span> <em>Biometrika</em> 57 (1): 97109.
</div>
<div id="ref-metropolis1953equation" class="csl-entry">
Metropolis, Nicholas, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. 1953. <span>Equation of State Calculations by Fast Computing Machines.</span> <em>Journal of Chemical Physics</em> 21 (6): 108792.
</div>
<div id="ref-meyers1994quantifying" class="csl-entry">
Meyers, Glenn. 1994. <span>Quantifying the Uncertainty in Claim Severity Estimates for an Excess Layer When Using the Single Parameter Pareto.</span> In <em>Proceedings of the Casualty Actuarial Society</em>, 81:91122.
</div>
<div id="ref-o1936history" class="csl-entry">
ODonnell, Terence. 1936. <em>History of Life Insurance in Its Formative Years</em>. American Conservation Company: Chicago, IL, United States of America.
</div>
<div id="ref-robert1999monte" class="csl-entry">
Robert, Christian P, and George Casella. 1999. <em>Monte Carlo Statistical Methods</em>. Springer: New York, NY, United States of America.
</div>
<div id="ref-vats2019multivariate" class="csl-entry">
Vats, Dootika, James M Flegal, and Galin L Jones. 2019. <span>Multivariate Output Analysis for Markov Chain Monte Carlo.</span> <em>Biometrika</em> 106 (2): 32137.
</div>
<div id="ref-wolny2014package" class="csl-entry">
Wolny-Dominiak, Alicja, and Michal Trzesiok. 2014. <span>Package <span>insuranceData</span>.</span> The Comprehensive R Archive Network.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="9">
<li id="fn9"><p>Each coin toss can be seen as a Bernoulli random variable, meaning that their sum is a binomial with parameters <span class="math inline">\(q=0.5\)</span> and <span class="math inline">\(m=5\)</span>. See <span style="color:red">Chapter 19.1</span> for more details.<a href="ChBayes.html#fnref9" class="footnote-back"></a></p></li>
<li id="fn10"><p>Specifically, we use a uniform over <span class="math inline">\([0,1]\)</span> for our prior distribution. As explained in Section <a href="ChBayes.html#ChBayes:SubsecPrior">9.2.3</a>, this type of prior is said to be noninformative.<a href="ChBayes.html#fnref10" class="footnote-back"></a></p></li>
<li id="fn11"><p>This is also an application of the betabinomial conjugate family that will be explained in Section <a href="ChBayes.html#ChBayes:SubsecBetaBin">9.3.1</a><a href="ChBayes.html#fnref11" class="footnote-back"></a></p></li>
<li id="fn12"><p>There is also a rich history blending prior information with data in loss modeling and in actuarial science, generally speaking; it is known as credibility. For more details on experience rating using credibility theory, see <span style="color:red">Chapter 11</span>.<a href="ChBayes.html#fnref12" class="footnote-back"></a></p></li>
<li id="fn13"><p>The law of total probability states that the total probability of an event <span class="math inline">\(B\)</span> is equal to the sum of the probabilities of <span class="math inline">\(B\)</span> occurring under different conditions, weighted by the probabilities of those conditions. In the case where there are only two different conditions (let us say <span class="math inline">\(A\)</span> and <span class="math inline">\(A^{\text{c}}\)</span>), we simply need to consider these two conditions. In all generality, however, we would need to consider more possibilities if the sample space cannot be divided into only two events.<a href="ChBayes.html#fnref13" class="footnote-back"></a></p></li>
<li id="fn14"><p>The data are from the General Insurance Association of Singapore, an organization consisting of non-life insurers in Singapore. These data contains the number of car accidents for <span class="math inline">\(n=7{,}483\)</span> auto insurance policies with several categorical explanatory variables and the exposure for each policy.<a href="ChBayes.html#fnref14" class="footnote-back"></a></p></li>
<li id="fn15"><p>For the sake of simplicity, we only consider one parameter in our derivation here. Note that, later, we will consider cases with more than one parameter and that this extension does not change the bulk of our results and derivations.<a href="ChBayes.html#fnref15" class="footnote-back"></a></p></li>
<li id="fn16"><p>Here, we assume that the domain of the beta is <span class="math inline">\([0,1]\)</span>, meaning that <span class="math inline">\(\theta = 1\)</span>. For more details, see <span style="color:red">Appendix D</span>.<a href="ChBayes.html#fnref16" class="footnote-back"></a></p></li>
<li id="fn17"><p>Conjugate families for the normal distribution with unknown <span class="math inline">\(\sigma^2\)</span> can also be derived. For the sake of simplicity, we will only focus on the case with known variance parameter in this book.<a href="ChBayes.html#fnref17" class="footnote-back"></a></p></li>
<li id="fn18"><p>For an overview of the theory behind MCMC methods, see <span class="citation">Robert and Casella (<a href="#ref-robert1999monte" role="doc-biblioref">1999</a>)</span>.<a href="ChBayes.html#fnref18" class="footnote-back"></a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ChapSimulation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ChapPremiumFoundations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["LossDataAnalytics.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
