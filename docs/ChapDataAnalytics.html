<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to Data Analytics | Loss Data Analytics</title>
  <meta name="description" content="Chapter 2 Introduction to Data Analytics | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to Data Analytics | Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 2 Introduction to Data Analytics | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  <meta name="github-repo" content="https://github.com/openacttexts/Loss-Data-Analytics" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to Data Analytics | Loss Data Analytics" />
  
  <meta name="twitter:description" content="Chapter 2 Introduction to Data Analytics | Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ChapIntro.html"/>
<link rel="next" href="ChapFrequency-Modeling.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<!-- Mathjax Version 2-->
<script type='text/x-mathjax-config'>
		MathJax.Hub.Config({
			extensions: ['tex2jax.js'],
			jax: ['input/TeX', 'output/HTML-CSS'],
			tex2jax: {
				inlineMath: [ ['$','$'], ['\\(','\\)'] ],
				displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
				processEscapes: true
			},
			'HTML-CSS': { availableFonts: ['TeX'] }
		});
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"> </script>


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="https://unpkg.com/survey-jquery/survey.jquery.min.js"></script>
<link href="https://unpkg.com/survey-jquery/modern.min.css" type="text/css" rel="stylesheet">
<script src="https://unpkg.com/showdown/dist/showdown.min.js"></script>


<script>
function markdownConverterEWF() {  
//Create showdown markdown converter
var converter = new showdown.Converter();
converter.setOption('ghCompatibleHeaderId', true);
survey
    .onTextMarkdown
    .add(function (survey, options) {
        //convert the markdown text to html
        var str = converter.makeHtml(options.text);
        //remove root paragraphs <p></p>
        str = str.substring(3);
        str = str.substring(0, str.length - 4);
        //set html
        options.html = str;
        MathJax.Hub.Queue(['Typeset',MathJax.Hub, 'options']);
    });  
};

// Quiz Header info
const jsonHeader = { 
    showProgressBar: "bottom",
    showTimerPanel: "none",
    maxTimeToFinishPage: 10000,
    maxTimeToFinish: 25000,
    firstPageIsStarted: true,
    startSurveyText: "Start Quiz" //,
//    title: "Does This Make Sense?"
}


// One and Two question quizzes
function jsonSummary1EWF(json) {  
let jsonEnd1 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
};  
return jsonEnd1;
};


function jsonSummary2EWF(json) {  
let jsonEnd2 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
};  
return jsonEnd2;
};

// Three, four, and five question quizzes
function jsonSummary3EWF(json) {  
let jsonEnd3 = { 
completedHtml: 
json["pages"][1]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][1]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][1]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][2]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][2]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][2]["questions"][0]["correctAnswer"]
+"<br>"+
json["pages"][3]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][3]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][3]["questions"][0]["correctAnswer"]
};  
return jsonEnd3;
};

function jsonSummary4EWF(json) {  
jsonEnd4 = jsonSummary3EWF(json);
jsonEnd4.completedHtml = jsonEnd4.completedHtml +  
"<br>"+
json["pages"][4]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][4]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][4]["questions"][0]["correctAnswer"]
;  
return jsonEnd4;
};

function jsonSummary5EWF(json) {  
jsonEnd5 = jsonSummary4EWF(json);
jsonEnd5.completedHtml = jsonEnd5.completedHtml +  
"<br>"+
json["pages"][5]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][5]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][5]["questions"][0]["correctAnswer"]
;  
return jsonEnd5;
};

function jsonSummary6EWF(json) {  
jsonEnd6 = jsonSummary5EWF(json);
jsonEnd6.completedHtml = jsonEnd6.completedHtml +  
"<br>"+
json["pages"][6]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][6]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][6]["questions"][0]["correctAnswer"]
;  
return jsonEnd6;
};

function jsonSummary7EWF(json) {  
jsonEnd7 = jsonSummary6EWF(json);
jsonEnd7.completedHtml = jsonEnd7.completedHtml +  
"<br>"+
json["pages"][7]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][7]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][7]["questions"][0]["correctAnswer"]
;  
return jsonEnd7;
};

function jsonSummary8EWF(json) {  
jsonEnd8 = jsonSummary7EWF(json);
jsonEnd8.completedHtml = jsonEnd8.completedHtml +  
"<br>"+
json["pages"][8]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][8]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][8]["questions"][0]["correctAnswer"]
;  
return jsonEnd8;
};

function jsonSummary9EWF(json) {  
jsonEnd9 = jsonSummary8EWF(json);
jsonEnd9.completedHtml = jsonEnd9.completedHtml +  
"<br>"+
json["pages"][9]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][9]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][9]["questions"][0]["correctAnswer"]
;  
return jsonEnd9;
};


function jsonSummary10EWF(json) {  
jsonEnd10 = jsonSummary9EWF(json);
jsonEnd10.completedHtml = jsonEnd10.completedHtml +  
"<br>"+
json["pages"][10]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][10]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][10]["questions"][0]["correctAnswer"]
;  
return jsonEnd10;
};


function jsonSummary11EWF(json) {  
jsonEnd11 = jsonSummary10EWF(json);
jsonEnd11.completedHtml = jsonEnd11.completedHtml +  
"<br>"+
json["pages"][11]["questions"][0]["name"]+ "<br>"+
"<i>Question: </i>"+json["pages"][11]["questions"][0]["title"]+"<br>"+
"<i>Answer: </i>"+json["pages"][11]["questions"][0]["correctAnswer"]
;  
return jsonEnd11;
};

Survey.StylesManager.applyTheme("modern");

</script>  
<!-- This completes the code for the quizzes -->

<!-- Various toggle functions used throughout --> 
<script language="javascript">
function toggle(id1,id2) {
	var ele = document.getElementById(id1); var text = document.getElementById(id2);
	if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Solution";}
		else {ele.style.display = "block"; text.innerHTML = "Hide Solution";}}
function togglecode(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show R Code";}
      else {ele.style.display = "block"; text.innerHTML = "Hide R Code";}}
function toggleEX(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Example";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Example";}}
function toggleTheory(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Theory";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Theory";}}
function toggleQuiz(id1,id2) {
   var ele = document.getElementById(id1); var text = document.getElementById(id2);
   if (ele.style.display == "block") {ele.style.display = "none"; text.innerHTML = "Show Quiz Solution";}
      else {ele.style.display = "block"; text.innerHTML = "Hide Quiz Solution";}}      
</script>

<!-- A few functions for revealing definitions -->
<script language="javascript">
<!--   $( function() {
    $("#tabs").tabs();
  } ); -->

$(document).ready(function(){
    $('[data-toggle="tooltip"]').tooltip();
});

$(document).ready(function(){
    $('[data-toggle="popover"]').popover(); 
});
</script>

<script language="javascript">
function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
}

// Get the element with id="defaultOpen" and click on it
document.getElementById("defaultOpen").click();
</script>

<script>

/* update total correct if #webex-total_correct exists */
update_total_correct = function() {
  console.log("webex: update total_correct");

  if (t = document.getElementById("webex-total_correct")) {
    var correct = document.getElementsByClassName("webex-correct").length;
    var solvemes = document.getElementsByClassName("webex-solveme").length;
    var radiogroups = document.getElementsByClassName("webex-radiogroup").length;
    var selects = document.getElementsByClassName("webex-select").length;
    
    t.innerHTML = correct + " of " + (solvemes + radiogroups + selects) + " correct";
  }
}

/* webex-solution button toggling function */
b_func = function() {
  console.log("webex: toggle hide");
  
  var cl = this.parentElement.classList;
  if (cl.contains('open')) {
    cl.remove("open");
  } else {
    cl.add("open");
  }
}

/* function for checking solveme answers */
solveme_func = function(e) {
  console.log("webex: check solveme");

  var real_answers = JSON.parse(this.dataset.answer);
  var my_answer = this.value;
  var cl = this.classList;
  if (cl.contains("ignorecase")) {
    my_answer = my_answer.toLowerCase();
  }
  if (cl.contains("nospaces")) {
    my_answer = my_answer.replace(/ /g, "")
  }

  if (my_answer == "") {
    cl.remove("webex-correct");
    cl.remove("webex-incorrect");
  } else if (real_answers.includes(my_answer)) {
    cl.add("webex-correct");
    cl.remove("webex-incorrect");
  } else {
    cl.add("webex-incorrect");
    cl.remove("webex-correct");
  }

  // match numeric answers within a specified tolerance
  if(this.dataset.tol > 0){
    var tol = JSON.parse(this.dataset.tol);
    var matches = real_answers.map(x => Math.abs(x - my_answer) < tol)
    if (matches.reduce((a, b) => a + b, 0) > 0) {
      cl.add("webex-correct");
    } else {
      cl.remove("webex-correct");
    }
  }

  // added regex bit
  if (cl.contains("regex")){
    answer_regex = RegExp(real_answers.join("|"))
    if (answer_regex.test(my_answer)) {
      cl.add("webex-correct");
    }
  }

  update_total_correct();
}

/* function for checking select answers */
select_func = function(e) {
  console.log("webex: check select");
  
  var cl = this.classList
  
  /* add style */
  cl.remove("webex-incorrect");
  cl.remove("webex-correct");
  if (this.value == "answer") {
    cl.add("webex-correct");
  } else if (this.value != "blank") {
    cl.add("webex-incorrect");
  }
  
  update_total_correct();
}

/* function for checking radiogroups answers */
radiogroups_func = function(e) {
  console.log("webex: check radiogroups");

  var checked_button = document.querySelector('input[name=' + this.id + ']:checked');
  var cl = checked_button.parentElement.classList;
  var labels = checked_button.parentElement.parentElement.children;
  
  /* get rid of styles */
  for (i = 0; i < labels.length; i++) {
    labels[i].classList.remove("webex-incorrect");
    labels[i].classList.remove("webex-correct");
  }
  
  /* add style */
  if (checked_button.value == "answer") {
    cl.add("webex-correct");
  } else {
    cl.add("webex-incorrect");
  }
  
  update_total_correct();
}

window.onload = function() {
  console.log("onload");
  /* set up solution buttons */
  var buttons = document.getElementsByTagName("button");

  for (var i = 0; i < buttons.length; i++) {
    if (buttons[i].parentElement.classList.contains('webex-solution')) {
      buttons[i].onclick = b_func;
    }
  }

  /* set up webex-solveme inputs */
  var solveme = document.getElementsByClassName("webex-solveme");

  for (var i = 0; i < solveme.length; i++) {
    /* make sure input boxes don't auto-anything */
    solveme[i].setAttribute("autocomplete","off");
    solveme[i].setAttribute("autocorrect", "off");
    solveme[i].setAttribute("autocapitalize", "off");
    solveme[i].setAttribute("spellcheck", "false");
    solveme[i].value = "";

    /* adjust answer for ignorecase or nospaces */
    var cl = solveme[i].classList;
    var real_answer = solveme[i].dataset.answer;
    if (cl.contains("ignorecase")) {
      real_answer = real_answer.toLowerCase();
    }
    if (cl.contains("nospaces")) {
      real_answer = real_answer.replace(/ /g, "");
    }
    solveme[i].dataset.answer = real_answer;

    /* attach checking function */
    solveme[i].onkeyup = solveme_func;
    solveme[i].onchange = solveme_func;
  }
  
  /* set up radiogroups */
  var radiogroups = document.getElementsByClassName("webex-radiogroup");
  for (var i = 0; i < radiogroups.length; i++) {
    radiogroups[i].onchange = radiogroups_func;
  }
  
  /* set up selects */
  var selects = document.getElementsByClassName("webex-select");
  for (var i = 0; i < selects.length; i++) {
    selects[i].onchange = select_func;
  }

  update_total_correct();
}

</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="includeWebex/webex.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i>Contributors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#reviewers"><i class="fa fa-check"></i>Reviewers</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#other-collaborators"><i class="fa fa-check"></i>Other Collaborators</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version"><i class="fa fa-check"></i>Version</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#for-our-readers"><i class="fa fa-check"></i>For our Readers</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ChapIntro.html"><a href="ChapIntro.html"><i class="fa fa-check"></i><b>1</b> Loss Data and Insurance Activities</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Data Driven Insurance Activities</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="ChapIntro.html"><a href="ChapIntro.html#nature-and-relevance-of-insurance"><i class="fa fa-check"></i><b>1.1.1</b> Nature and Relevance of Insurance</a></li>
<li class="chapter" data-level="1.1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:DataDriven"><i class="fa fa-check"></i><b>1.1.2</b> Why Data Driven?</a></li>
<li class="chapter" data-level="1.1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:PredModApps"><i class="fa fa-check"></i><b>1.2</b> Insurance Company Operations</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ChapIntro.html"><a href="ChapIntro.html#initiating-insurance"><i class="fa fa-check"></i><b>1.2.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.2.2" data-path="ChapIntro.html"><a href="ChapIntro.html#renewing-insurance"><i class="fa fa-check"></i><b>1.2.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.2.3" data-path="ChapIntro.html"><a href="ChapIntro.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.2.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.2.4" data-path="ChapIntro.html"><a href="ChapIntro.html#S:Reserving"><i class="fa fa-check"></i><b>1.2.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ChapIntro.html"><a href="ChapIntro.html#S:LGPIF"><i class="fa fa-check"></i><b>1.3</b> Case Study: Wisconsin Property Fund</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="ChapIntro.html"><a href="ChapIntro.html#S:OutComes"><i class="fa fa-check"></i><b>1.3.1</b> Fund Claims Variables: Frequency and Severity</a></li>
<li class="chapter" data-level="1.3.2" data-path="ChapIntro.html"><a href="ChapIntro.html#S:FundVariables"><i class="fa fa-check"></i><b>1.3.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.3.3" data-path="ChapIntro.html"><a href="ChapIntro.html#fund-operations"><i class="fa fa-check"></i><b>1.3.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="ChapIntro.html"><a href="ChapIntro.html#exercises"><i class="fa fa-check"></i><b>1.4</b> Exercises</a></li>
<li class="chapter" data-level="1.5" data-path="ChapIntro.html"><a href="ChapIntro.html#Intro-further-reading-and-resources"><i class="fa fa-check"></i><b>1.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html"><i class="fa fa-check"></i><b>2</b> Introduction to Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Elements"><i class="fa fa-check"></i><b>2.1</b> Elements of Data Analytics</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#key-data-analytic-concepts"><i class="fa fa-check"></i><b>2.1.1</b> Key Data Analytic Concepts</a></li>
<li class="chapter" data-level="2.1.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:DataAlgorithm"><i class="fa fa-check"></i><b>2.1.2</b> Data versus Algorithmic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:Process"><i class="fa fa-check"></i><b>2.2</b> Data Analysis Process</a></li>
<li class="chapter" data-level="2.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:SingleVarAnalytics"><i class="fa fa-check"></i><b>2.3</b> Single Variable Analytics</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:VarTypes"><i class="fa fa-check"></i><b>2.3.1</b> Variable Types</a></li>
<li class="chapter" data-level="2.3.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:EDACDA"><i class="fa fa-check"></i><b>2.3.2</b> Exploratory versus Confirmatory</a></li>
<li class="chapter" data-level="2.3.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#model-construction"><i class="fa fa-check"></i><b>2.3.3</b> Model Construction</a></li>
<li class="chapter" data-level="2.3.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#model-selection"><i class="fa fa-check"></i><b>2.3.4</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:ManyVarAnalytics"><i class="fa fa-check"></i><b>2.4</b> Analytics with Many Variables</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#supervised-and-unsupervised-learning"><i class="fa fa-check"></i><b>2.4.1</b> Supervised and Unsupervised Learning</a></li>
<li class="chapter" data-level="2.4.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#algorithmic-modeling"><i class="fa fa-check"></i><b>2.4.2</b> Algorithmic Modeling</a></li>
<li class="chapter" data-level="2.4.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Data Modeling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:DataLearn"><i class="fa fa-check"></i><b>2.5</b> Data</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-types"><i class="fa fa-check"></i><b>2.5.1</b> Data Types</a></li>
<li class="chapter" data-level="2.5.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-structures-and-storage"><i class="fa fa-check"></i><b>2.5.2</b> Data Structures and Storage</a></li>
<li class="chapter" data-level="2.5.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#data-cleaning"><i class="fa fa-check"></i><b>2.5.3</b> Data Cleaning</a></li>
<li class="chapter" data-level="2.5.4" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#Sec:BigDataAnalysis"><i class="fa fa-check"></i><b>2.5.4</b> Big Data Analysis</a></li>
<li class="chapter" data-level="2.5.5" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#ethical-issues"><i class="fa fa-check"></i><b>2.5.5</b> Ethical Issues</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#DS:further-reading-and-resources"><i class="fa fa-check"></i><b>2.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#S:MultiEDA"><i class="fa fa-check"></i><b>2.6.1</b> Technical Supplement: Multivariate Exploratory Analysis</a></li>
<li class="chapter" data-level="2.6.2" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#tree-based-models"><i class="fa fa-check"></i><b>2.6.2</b> Tree-based Models</a></li>
<li class="chapter" data-level="2.6.3" data-path="ChapDataAnalytics.html"><a href="ChapDataAnalytics.html#technical-supplement-some-r-functions"><i class="fa fa-check"></i><b>2.6.3</b> Technical Supplement: Some R Functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html"><i class="fa fa-check"></i><b>3</b> Frequency Modeling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:frequency-distributions"><i class="fa fa-check"></i><b>3.1</b> Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>3.1.1</b> How Frequency Augments Severity Information</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:basic-frequency-distributions"><i class="fa fa-check"></i><b>3.2</b> Basic Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:foundations"><i class="fa fa-check"></i><b>3.2.1</b> Foundations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:generating-functions"><i class="fa fa-check"></i><b>3.2.2</b> Moment and Probability Generating Functions</a></li>
<li class="chapter" data-level="3.2.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:important-frequency-distributions"><i class="fa fa-check"></i><b>3.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:the-a-b-0-class"><i class="fa fa-check"></i><b>3.3</b> The (<em>a</em>, <em>b</em>, 0) Class</a></li>
<li class="chapter" data-level="3.4" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:estimating-frequency-distributions"><i class="fa fa-check"></i><b>3.4</b> Estimating Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:parameter-estimation"><i class="fa fa-check"></i><b>3.4.1</b> Parameter Estimation</a></li>
<li class="chapter" data-level="3.4.2" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:frequency-distributions-mle"><i class="fa fa-check"></i><b>3.4.2</b> Frequency Distributions MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:other-frequency-distributions"><i class="fa fa-check"></i><b>3.5</b> Other Frequency Distributions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:zero-truncation-or-modification"><i class="fa fa-check"></i><b>3.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:mixture-distributions"><i class="fa fa-check"></i><b>3.6</b> Mixture Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:goodness-of-fit"><i class="fa fa-check"></i><b>3.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="3.8" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:exercises"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#Freq-further-reading-and-resources"><i class="fa fa-check"></i><b>3.9</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="3.9.1" data-path="ChapFrequency-Modeling.html"><a href="ChapFrequency-Modeling.html#S:rcode"><i class="fa fa-check"></i><b>3.9.1</b> TS 3.A. R Code for Plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ChapSeverity.html"><a href="ChapSeverity.html"><i class="fa fa-check"></i><b>4</b> Modeling Loss Severity</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:BasicQuantities"><i class="fa fa-check"></i><b>4.1</b> Basic Distributional Quantities</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Chap3Moments"><i class="fa fa-check"></i><b>4.1.1</b> Moments</a></li>
<li class="chapter" data-level="4.1.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LS:Quantiles"><i class="fa fa-check"></i><b>4.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="4.1.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#moment-generating-function"><i class="fa fa-check"></i><b>4.1.3</b> Moment Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:ContinuousDistn"><i class="fa fa-check"></i><b>4.2</b> Continuous Distributions for Modeling Loss Severity</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Loss:Gamma"><i class="fa fa-check"></i><b>4.2.1</b> Gamma Distribution</a></li>
<li class="chapter" data-level="4.2.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#pareto-distribution"><i class="fa fa-check"></i><b>4.2.2</b> Pareto Distribution</a></li>
<li class="chapter" data-level="4.2.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LS:Weibull"><i class="fa fa-check"></i><b>4.2.3</b> Weibull Distribution</a></li>
<li class="chapter" data-level="4.2.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>4.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#MethodsCreation"><i class="fa fa-check"></i><b>4.3</b> Methods of Creating New Distributions</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>4.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="4.3.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>4.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="4.3.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:LossSev:Raising"><i class="fa fa-check"></i><b>4.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="4.3.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#exponentiation"><i class="fa fa-check"></i><b>4.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="4.3.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#finite-mixtures"><i class="fa fa-check"></i><b>4.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="4.3.6" data-path="ChapSeverity.html"><a href="ChapSeverity.html#continuous-mixtures"><i class="fa fa-check"></i><b>4.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:CoverageModifications"><i class="fa fa-check"></i><b>4.4</b> Coverage Modifications</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:PolicyDeduct"><i class="fa fa-check"></i><b>4.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="4.4.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:PolicyLimits"><i class="fa fa-check"></i><b>4.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="4.4.3" data-path="ChapSeverity.html"><a href="ChapSeverity.html#coinsurance-and-inflation"><i class="fa fa-check"></i><b>4.4.3</b> Coinsurance and Inflation</a></li>
<li class="chapter" data-level="4.4.4" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Chap3Reinsurance"><i class="fa fa-check"></i><b>4.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:MaxLikeEstimation"><i class="fa fa-check"></i><b>4.5</b> Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="ChapSeverity.html"><a href="ChapSeverity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>4.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="4.5.2" data-path="ChapSeverity.html"><a href="ChapSeverity.html#S:Loss:MLEModified"><i class="fa fa-check"></i><b>4.5.2</b> Maximum Likelihood Estimators using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="ChapSeverity.html"><a href="ChapSeverity.html#LM-further-reading-and-resources"><i class="fa fa-check"></i><b>4.6</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html"><i class="fa fa-check"></i><b>5</b> Model Selection and Estimation</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:NonParInf"><i class="fa fa-check"></i><b>5.1</b> Nonparametric Estimation</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:NonParEst"><i class="fa fa-check"></i><b>5.1.1</b> Nonparametric Methods</a></li>
<li class="chapter" data-level="5.1.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#starting-values"><i class="fa fa-check"></i><b>5.1.2</b> Starting Values</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModifiedData"><i class="fa fa-check"></i><b>5.2</b> Estimation using Modified Data</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModifiedData1"><i class="fa fa-check"></i><b>5.2.1</b> Parametric Estimation using Modified Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:NonParModified"><i class="fa fa-check"></i><b>5.2.2</b> Nonparametric Estimation using Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ModelSelection"><i class="fa fa-check"></i><b>5.3</b> Model Selection</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:ToolsModelSelection"><i class="fa fa-check"></i><b>5.3.1</b> Tools for Model Selection and Diagnostics</a></li>
<li class="chapter" data-level="5.3.2" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Iterative:Selection"><i class="fa fa-check"></i><b>5.3.2</b> Iterative Model Selection</a></li>
<li class="chapter" data-level="5.3.3" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Tools:Stats:Likelihood"><i class="fa fa-check"></i><b>5.3.3</b> Model Selection Based on a Training Dataset</a></li>
<li class="chapter" data-level="5.3.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#model-selection-based-on-a-test-dataset"><i class="fa fa-check"></i><b>5.3.4</b> Model Selection Based on a Test Dataset</a></li>
<li class="chapter" data-level="5.3.5" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Cross-Validation"><i class="fa fa-check"></i><b>5.3.5</b> Model Selection Based on Cross-Validation</a></li>
<li class="chapter" data-level="5.3.6" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#S:MS:Modified-Data"><i class="fa fa-check"></i><b>5.3.6</b> Model Selection for Modified Data</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="ChapModelSelection.html"><a href="ChapModelSelection.html#MS:further-reading-and-resources"><i class="fa fa-check"></i><b>5.4</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html"><i class="fa fa-check"></i><b>6</b> Aggregate Loss Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#introduction"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#individual-risk-model"><i class="fa fa-check"></i><b>6.2</b> Individual Risk Model</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#moments-and-distribution"><i class="fa fa-check"></i><b>6.2.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="6.2.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#aggregate-loss-distribution"><i class="fa fa-check"></i><b>6.2.2</b> Aggregate Loss Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:AggLoss:CRM"><i class="fa fa-check"></i><b>6.3</b> Collective Risk Model</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#moments-and-distribution-1"><i class="fa fa-check"></i><b>6.3.1</b> Moments and Distribution</a></li>
<li class="chapter" data-level="6.3.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#stop-loss-insurance"><i class="fa fa-check"></i><b>6.3.2</b> Stop-loss Insurance</a></li>
<li class="chapter" data-level="6.3.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#closed-form-distributions"><i class="fa fa-check"></i><b>6.3.3</b> Closed-form Distributions</a></li>
<li class="chapter" data-level="6.3.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:AggLoss:Tweedie"><i class="fa fa-check"></i><b>6.3.4</b> Tweedie Distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#computing-the-aggregate-claims-distribution"><i class="fa fa-check"></i><b>6.4</b> Computing the Aggregate Claims Distribution</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#recursive-method"><i class="fa fa-check"></i><b>6.4.1</b> Recursive Method</a></li>
<li class="chapter" data-level="6.4.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#simulation"><i class="fa fa-check"></i><b>6.4.2</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#effects-of-coverage-modifications"><i class="fa fa-check"></i><b>6.5</b> Effects of Coverage Modifications</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#impact-of-exposure-on-frequency"><i class="fa fa-check"></i><b>6.5.1</b> Impact of Exposure on Frequency</a></li>
<li class="chapter" data-level="6.5.2" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#S:MS:DedImpactClmFreq"><i class="fa fa-check"></i><b>6.5.2</b> Impact of Deductibles on Claim Frequency</a></li>
<li class="chapter" data-level="6.5.3" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#impact-of-policy-modifications-on-aggregate-claims"><i class="fa fa-check"></i><b>6.5.3</b> Impact of Policy Modifications on Aggregate Claims</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#AL-further-reading-and-resources"><i class="fa fa-check"></i><b>6.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-6.a.1.-individual-risk-model-properties"><i class="fa fa-check"></i>TS 6.A.1. Individual Risk Model Properties</a></li>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-6.a.2.-relationship-between-probability-generating-functions-of-x_i-and-x_it"><i class="fa fa-check"></i>TS 6.A.2. Relationship Between Probability Generating Functions of <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_i^T\)</span></a></li>
<li class="chapter" data-level="" data-path="ChapAggLossModels.html"><a href="ChapAggLossModels.html#ts-6.a.3.-moment-generating-function-of-aggregate-loss-s_n-in-example-6.3.8"><i class="fa fa-check"></i>TS 6.A.3. Moment Generating Function of Aggregate Loss <span class="math inline">\(S_N\)</span> in Example 6.3.8</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ChapSimulation.html"><a href="ChapSimulation.html"><i class="fa fa-check"></i><b>7</b> Simulation and Resampling</a>
<ul>
<li class="chapter" data-level="7.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:SimulationFundamentals"><i class="fa fa-check"></i><b>7.1</b> Simulation Fundamentals</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>7.1.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="7.1.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:InverseTransform"><i class="fa fa-check"></i><b>7.1.2</b> Inverse Transform Method</a></li>
<li class="chapter" data-level="7.1.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#simulation-precision"><i class="fa fa-check"></i><b>7.1.3</b> Simulation Precision</a></li>
<li class="chapter" data-level="7.1.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:SimulationStatInference"><i class="fa fa-check"></i><b>7.1.4</b> Simulation and Statistical Inference</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Bootstrap"><i class="fa fa-check"></i><b>7.2</b> Bootstrapping and Resampling</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#bootstrap-foundations"><i class="fa fa-check"></i><b>7.2.1</b> Bootstrap Foundations</a></li>
<li class="chapter" data-level="7.2.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:Sim:Precision"><i class="fa fa-check"></i><b>7.2.2</b> Bootstrap Precision: Bias, Standard Deviation, and Mean Square Error</a></li>
<li class="chapter" data-level="7.2.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#confidence-intervals"><i class="fa fa-check"></i><b>7.2.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="7.2.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:ParametricBootStrap"><i class="fa fa-check"></i><b>7.2.4</b> Parametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:CrossValidation"><i class="fa fa-check"></i><b>7.3</b> Cross-Validation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>7.3.1</b> k-Fold Cross-Validation</a></li>
<li class="chapter" data-level="7.3.2" data-path="ChapSimulation.html"><a href="ChapSimulation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i><b>7.3.2</b> Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="7.3.3" data-path="ChapSimulation.html"><a href="ChapSimulation.html#cross-validation-and-bootstrap"><i class="fa fa-check"></i><b>7.3.3</b> Cross-Validation and Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="ChapSimulation.html"><a href="ChapSimulation.html#S:ImportanceSampling"><i class="fa fa-check"></i><b>7.4</b> Importance Sampling</a></li>
<li class="chapter" data-level="7.5" data-path="ChapSimulation.html"><a href="ChapSimulation.html#Simulation:further-reading-and-resources"><i class="fa fa-check"></i><b>7.5</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="ChapSimulation.html"><a href="ChapSimulation.html#ts-7.a.-bootstrap-applications-in-predictive-modeling"><i class="fa fa-check"></i><b>7.5.1</b> TS 7.A. Bootstrap Applications in Predictive Modeling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html"><i class="fa fa-check"></i><b>8</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#S:IntroBayes"><i class="fa fa-check"></i><b>8.0.1</b> Introduction to Bayesian Inference</a></li>
<li class="chapter" data-level="8.0.2" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#bayesian-model"><i class="fa fa-check"></i><b>8.0.2</b> Bayesian Model</a></li>
<li class="chapter" data-level="8.0.3" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#bayesian-inference"><i class="fa fa-check"></i><b>8.0.3</b> Bayesian Inference</a></li>
<li class="chapter" data-level="8.0.4" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#S:ConjugateDistributions"><i class="fa fa-check"></i><b>8.0.4</b> Conjugate Distributions</a></li>
<li class="chapter" data-level="8.1" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#S:MCMC"><i class="fa fa-check"></i><b>8.1</b> Monte Carlo Markov Chain (MCMC)</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#metropolis-hastings"><i class="fa fa-check"></i><b>8.1.1</b> Metropolis Hastings</a></li>
<li class="chapter" data-level="8.1.2" data-path="ChapBayesInference.html"><a href="ChapBayesInference.html#gibbs-sampler"><i class="fa fa-check"></i><b>8.1.2</b> Gibbs Sampler</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html"><i class="fa fa-check"></i><b>9</b> Premium Foundations</a>
<ul>
<li class="chapter" data-level="9.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:IntroductionRatemaking"><i class="fa fa-check"></i><b>9.1</b> Introduction to Ratemaking</a></li>
<li class="chapter" data-level="9.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:AggRateMaking"><i class="fa fa-check"></i><b>9.2</b> Aggregate Ratemaking Methods</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:PurePremium"><i class="fa fa-check"></i><b>9.2.1</b> Pure Premium Method</a></li>
<li class="chapter" data-level="9.2.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:LossRatio"><i class="fa fa-check"></i><b>9.2.2</b> Loss Ratio Method</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:PricingPrinciples"><i class="fa fa-check"></i><b>9.3</b> Pricing Principles</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#premium-principles"><i class="fa fa-check"></i><b>9.3.1</b> Premium Principles</a></li>
<li class="chapter" data-level="9.3.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#properties-of-premium-principles"><i class="fa fa-check"></i><b>9.3.2</b> Properties of Premium Principles</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:HeterogeneousRisks"><i class="fa fa-check"></i><b>9.4</b> Heterogeneous Risks</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:ExposureToRisk"><i class="fa fa-check"></i><b>9.4.1</b> Exposure to Risk</a></li>
<li class="chapter" data-level="9.4.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:RatingFactors"><i class="fa fa-check"></i><b>9.4.2</b> Rating Factors</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:TrendDevelopment"><i class="fa fa-check"></i><b>9.5</b> Development and Trending</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#exposures-and-premiums"><i class="fa fa-check"></i><b>9.5.1</b> Exposures and Premiums</a></li>
<li class="chapter" data-level="9.5.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#losses-claims-and-payments"><i class="fa fa-check"></i><b>9.5.2</b> Losses, Claims, and Payments</a></li>
<li class="chapter" data-level="9.5.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:CompareMethods"><i class="fa fa-check"></i><b>9.5.3</b> Comparing Pure Premium and Loss Ratio Methods</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#S:GiniStatistic"><i class="fa fa-check"></i><b>9.6</b> Selecting a Premium</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#classic-lorenz-curve"><i class="fa fa-check"></i><b>9.6.1</b> Classic Lorenz Curve</a></li>
<li class="chapter" data-level="9.6.2" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#performance-curve-and-a-gini-statistic"><i class="fa fa-check"></i><b>9.6.2</b> Performance Curve and a Gini Statistic</a></li>
<li class="chapter" data-level="9.6.3" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#out-of-sample-validation"><i class="fa fa-check"></i><b>9.6.3</b> Out-of-Sample Validation</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#further-resources-and-contributors"><i class="fa fa-check"></i><b>9.7</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapPremiumFoundations.html"><a href="ChapPremiumFoundations.html#ts-9.a.-rate-regulation"><i class="fa fa-check"></i>TS 9.A. Rate Regulation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html"><i class="fa fa-check"></i><b>10</b> Risk Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:Introduction"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:PoissonRegression"><i class="fa fa-check"></i><b>10.2</b> Poisson Regression Model</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:RC:Need.Poi.reg"><i class="fa fa-check"></i><b>10.2.1</b> Need for Poisson Regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#poisson-regression"><i class="fa fa-check"></i><b>10.2.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="10.2.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#incorporating-exposure"><i class="fa fa-check"></i><b>10.2.3</b> Incorporating Exposure</a></li>
<li class="chapter" data-level="10.2.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#exercises-4"><i class="fa fa-check"></i><b>10.2.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#S:CatVarMultiTarriff"><i class="fa fa-check"></i><b>10.3</b> Categorical Variables and Multiplicative Tariff</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#rating-factors-and-tariff"><i class="fa fa-check"></i><b>10.3.1</b> Rating Factors and Tariff</a></li>
<li class="chapter" data-level="10.3.2" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#multiplicative-tariff-model"><i class="fa fa-check"></i><b>10.3.2</b> Multiplicative Tariff Model</a></li>
<li class="chapter" data-level="10.3.3" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#poisson-regression-for-multiplicative-tariff"><i class="fa fa-check"></i><b>10.3.3</b> Poisson Regression for Multiplicative Tariff</a></li>
<li class="chapter" data-level="10.3.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#numerical-examples"><i class="fa fa-check"></i><b>10.3.4</b> Numerical Examples</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#RC:further-reading-and-resources"><i class="fa fa-check"></i><b>10.4</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-10.a.-estimating-poisson-regression-models"><i class="fa fa-check"></i>TS 10.A. Estimating Poisson Regression Models</a></li>
<li class="chapter" data-level="" data-path="ChapRiskClass.html"><a href="ChapRiskClass.html#ts-10.b.-selecting-rating-factors"><i class="fa fa-check"></i>TS 10.B. Selecting Rating Factors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ChapCredibility.html"><a href="ChapCredibility.html"><i class="fa fa-check"></i><b>11</b> Experience Rating Using Credibility Theory</a>
<ul>
<li class="chapter" data-level="11.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#introduction-to-applications-of-credibility-theory"><i class="fa fa-check"></i><b>11.1</b> Introduction to Applications of Credibility Theory</a></li>
<li class="chapter" data-level="11.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#limited-fluctuation-credibility"><i class="fa fa-check"></i><b>11.2</b> Limited Fluctuation Credibility</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:frequency"><i class="fa fa-check"></i><b>11.2.1</b> Full Credibility for Claim Frequency</a></li>
<li class="chapter" data-level="11.2.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-aggregate-losses-and-pure-premium"><i class="fa fa-check"></i><b>11.2.2</b> Full Credibility for Aggregate Losses and Pure Premium</a></li>
<li class="chapter" data-level="11.2.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-for-severity"><i class="fa fa-check"></i><b>11.2.3</b> Full Credibility for Severity</a></li>
<li class="chapter" data-level="11.2.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#partial-credibility"><i class="fa fa-check"></i><b>11.2.4</b> Partial Credibility</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Cred:Buhlmann"><i class="fa fa-check"></i><b>11.3</b> Bhlmann Credibility</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:EPV-VHM-Z"><i class="fa fa-check"></i><b>11.3.1</b> Credibility <em>Z</em>, <em>EPV</em>, and <em>VHM</em></a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#bhlmann-straub-credibility"><i class="fa fa-check"></i><b>11.4</b> Bhlmann-Straub Credibility</a></li>
<li class="chapter" data-level="11.5" data-path="ChapCredibility.html"><a href="ChapCredibility.html#S:Cred:BayesInf"><i class="fa fa-check"></i><b>11.5</b> Bayesian Inference and Bhlmann Credibility</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#Sec:Cred:gammaPoisson"><i class="fa fa-check"></i><b>11.5.1</b> Gamma-Poisson Model</a></li>
<li class="chapter" data-level="11.5.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#beta-binomial-model"><i class="fa fa-check"></i><b>11.5.2</b> Beta-Binomial Model</a></li>
<li class="chapter" data-level="11.5.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#exact-credibility"><i class="fa fa-check"></i><b>11.5.3</b> Exact Credibility</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="ChapCredibility.html"><a href="ChapCredibility.html#estimating-credibility-parameters"><i class="fa fa-check"></i><b>11.6</b> Estimating Credibility Parameters</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="ChapCredibility.html"><a href="ChapCredibility.html#full-credibility-standard-for-limited-fluctuation-credibility"><i class="fa fa-check"></i><b>11.6.1</b> Full Credibility Standard for Limited Fluctuation Credibility</a></li>
<li class="chapter" data-level="11.6.2" data-path="ChapCredibility.html"><a href="ChapCredibility.html#nonparametric-estimation-for-bhlmann-and-bhlmann-straub-models"><i class="fa fa-check"></i><b>11.6.2</b> Nonparametric Estimation for Bhlmann and Bhlmann-Straub Models</a></li>
<li class="chapter" data-level="11.6.3" data-path="ChapCredibility.html"><a href="ChapCredibility.html#semiparametric-estimation-for-bhlmann-and-bhlmann-straub-models"><i class="fa fa-check"></i><b>11.6.3</b> Semiparametric Estimation for Bhlmann and Bhlmann-Straub Models</a></li>
<li class="chapter" data-level="11.6.4" data-path="ChapCredibility.html"><a href="ChapCredibility.html#balancing-credibility-estimators"><i class="fa fa-check"></i><b>11.6.4</b> Balancing Credibility Estimators</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="ChapCredibility.html"><a href="ChapCredibility.html#Cred-further-reading-and-resources"><i class="fa fa-check"></i><b>11.7</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html"><i class="fa fa-check"></i><b>12</b> Insurance Portfolio Management including Reinsurance</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#introduction-to-insurance-portfolios"><i class="fa fa-check"></i><b>12.1</b> Introduction to Insurance Portfolios</a></li>
<li class="chapter" data-level="12.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Tails"><i class="fa fa-check"></i><b>12.2</b> Tails of Distributions</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#classification-based-on-moments"><i class="fa fa-check"></i><b>12.2.1</b> Classification Based on Moments</a></li>
<li class="chapter" data-level="12.2.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#comparison-based-on-limiting-tail-behavior"><i class="fa fa-check"></i><b>12.2.2</b> Comparison Based on Limiting Tail Behavior</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:RiskMeasure"><i class="fa fa-check"></i><b>12.3</b> Risk Measures</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#coherent-risk-measures"><i class="fa fa-check"></i><b>12.3.1</b> Coherent Risk Measures</a></li>
<li class="chapter" data-level="12.3.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#value-at-risk"><i class="fa fa-check"></i><b>12.3.2</b> Value-at-Risk</a></li>
<li class="chapter" data-level="12.3.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#tail-value-at-risk"><i class="fa fa-check"></i><b>12.3.3</b> Tail Value-at-Risk</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:Reinsurance"><i class="fa fa-check"></i><b>12.4</b> Reinsurance</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:ProportionalRe"><i class="fa fa-check"></i><b>12.4.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="12.4.2" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:NonProportionalRe"><i class="fa fa-check"></i><b>12.4.2</b> Non-Proportional Reinsurance</a></li>
<li class="chapter" data-level="12.4.3" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#S:AdditionalRe"><i class="fa fa-check"></i><b>12.4.3</b> Additional Reinsurance Treaties</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="ChapPortMgt.html"><a href="ChapPortMgt.html#further-resources-and-contributors-1"><i class="fa fa-check"></i><b>12.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html"><i class="fa fa-check"></i><b>13</b> Loss Reserving</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:motivation"><i class="fa fa-check"></i><b>13.1</b> Motivation</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:claim-types"><i class="fa fa-check"></i><b>13.1.1</b> Closed, IBNR, and RBNS Claims</a></li>
<li class="chapter" data-level="13.1.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#why-reserving"><i class="fa fa-check"></i><b>13.1.2</b> Why Reserving?</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Data"><i class="fa fa-check"></i><b>13.2</b> Loss Reserve Data</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#from-micro-to-macro"><i class="fa fa-check"></i><b>13.2.1</b> From Micro to Macro</a></li>
<li class="chapter" data-level="13.2.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#run-off-triangles"><i class="fa fa-check"></i><b>13.2.2</b> Run-off Triangles</a></li>
<li class="chapter" data-level="13.2.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#loss-reserve-notation"><i class="fa fa-check"></i><b>13.2.3</b> Loss Reserve Notation</a></li>
<li class="chapter" data-level="13.2.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Rcode"><i class="fa fa-check"></i><b>13.2.4</b> R Code to Summarize Loss Reserve Data</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:Chain-ladder"><i class="fa fa-check"></i><b>13.3</b> The Chain-Ladder Method</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:DeterministicCL"><i class="fa fa-check"></i><b>13.3.1</b> The Deterministic Chain-Ladder</a></li>
<li class="chapter" data-level="13.3.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#macks-distribution-free-chain-ladder-model"><i class="fa fa-check"></i><b>13.3.2</b> Macks Distribution-Free Chain-Ladder Model</a></li>
<li class="chapter" data-level="13.3.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#r-code-for-chain-ladder-predictions"><i class="fa fa-check"></i><b>13.3.3</b> R code for Chain-Ladder Predictions</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#S:GLMs"><i class="fa fa-check"></i><b>13.4</b> GLMs and Bootstrap for Loss Reserves</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-specification"><i class="fa fa-check"></i><b>13.4.1</b> Model Specification</a></li>
<li class="chapter" data-level="13.4.2" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#model-estimation-and-prediction"><i class="fa fa-check"></i><b>13.4.2</b> Model Estimation and Prediction</a></li>
<li class="chapter" data-level="13.4.3" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#bootstrap"><i class="fa fa-check"></i><b>13.4.3</b> Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="ChapLossReserves.html"><a href="ChapLossReserves.html#LossRe:further-reading-and-resources"><i class="fa fa-check"></i><b>13.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html"><i class="fa fa-check"></i><b>14</b> Experience Rating using Bonus-Malus</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:Intro"><i class="fa fa-check"></i><b>14.1</b> Introduction</a></li>
<li class="chapter" data-level="14.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:NCD"><i class="fa fa-check"></i><b>14.2</b> <em>NCD</em> System in Several Countries</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#ncd-system-in-malaysia"><i class="fa fa-check"></i><b>14.2.1</b> <em>NCD</em> System in Malaysia</a></li>
<li class="chapter" data-level="14.2.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#ncd-systems-in-other-countries"><i class="fa fa-check"></i><b>14.2.2</b> NCD Systems in Other Countries</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:BMS"><i class="fa fa-check"></i><b>14.3</b> <em>BMS</em> and Markov Chain Model</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#transition-probability"><i class="fa fa-check"></i><b>14.3.1</b> Transition Probability</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:ERBM:StatDist"><i class="fa fa-check"></i><b>14.4</b> <em>BMS</em> and Stationary Distribution</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution"><i class="fa fa-check"></i><b>14.4.1</b> Stationary Distribution</a></li>
<li class="chapter" data-level="14.4.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-code-for-a-stationary-distribution"><i class="fa fa-check"></i><b>14.4.2</b> <code>R</code> Code for a Stationary Distribution</a></li>
<li class="chapter" data-level="14.4.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-evolution"><i class="fa fa-check"></i><b>14.4.3</b> Premium Evolution</a></li>
<li class="chapter" data-level="14.4.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-program-for-premium-evolution"><i class="fa fa-check"></i><b>14.4.4</b> <code>R</code> Program for Premium Evolution</a></li>
<li class="chapter" data-level="14.4.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#convergence-rate"><i class="fa fa-check"></i><b>14.4.5</b> Convergence Rate</a></li>
<li class="chapter" data-level="14.4.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#r-program-for-convergence-rate"><i class="fa fa-check"></i><b>14.4.6</b> <code>R</code> Program for Convergence Rate</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:PremRtg"><i class="fa fa-check"></i><b>14.5</b> <em>BMS</em> and Premium Rating</a>
<ul>
<li class="chapter" data-level="14.5.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#premium-rating"><i class="fa fa-check"></i><b>14.5.1</b> Premium Rating</a></li>
<li class="chapter" data-level="14.5.2" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#a-priori-risk-classification"><i class="fa fa-check"></i><b>14.5.2</b> A Priori Risk Classification</a></li>
<li class="chapter" data-level="14.5.3" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#modelling-of-residual-heterogeneity"><i class="fa fa-check"></i><b>14.5.3</b> Modelling of Residual Heterogeneity</a></li>
<li class="chapter" data-level="14.5.4" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#stationary-distribution-allowing-for-residual-heterogeneity"><i class="fa fa-check"></i><b>14.5.4</b> Stationary Distribution Allowing for Residual Heterogeneity</a></li>
<li class="chapter" data-level="14.5.5" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#determination-of-optimal-relativities"><i class="fa fa-check"></i><b>14.5.5</b> Determination of Optimal Relativities</a></li>
<li class="chapter" data-level="14.5.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#numerical-illustrations"><i class="fa fa-check"></i><b>14.5.6</b> Numerical Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#S:Further"><i class="fa fa-check"></i><b>14.6</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="14.6.1" data-path="ChapBonusMalus.html"><a href="ChapBonusMalus.html#further-reading-and-references-1"><i class="fa fa-check"></i><b>14.6.1</b> Further Reading and References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html"><i class="fa fa-check"></i><b>15</b> Dependence Modeling</a>
<ul>
<li class="chapter" data-level="15.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#multivariate-variables"><i class="fa fa-check"></i><b>15.1</b> Multivariate Variables</a></li>
<li class="chapter" data-level="15.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Measures"><i class="fa fa-check"></i><b>15.2</b> Classic Measures of Scalar Associations</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#association-measures-for-quantitative-variables"><i class="fa fa-check"></i><b>15.2.1</b> Association Measures for Quantitative Variables</a></li>
<li class="chapter" data-level="15.2.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#rank-based-measures"><i class="fa fa-check"></i><b>15.2.2</b> Rank Based Measures</a></li>
<li class="chapter" data-level="15.2.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#nominal-variables"><i class="fa fa-check"></i><b>15.2.3</b> Nominal Variables</a></li>
<li class="chapter" data-level="15.2.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ordinal-variables"><i class="fa fa-check"></i><b>15.2.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="15.2.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#interval-variables"><i class="fa fa-check"></i><b>15.2.5</b> Interval Variables</a></li>
<li class="chapter" data-level="15.2.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#discrete-and-continuous-variables"><i class="fa fa-check"></i><b>15.2.6</b> Discrete and Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:Copula"><i class="fa fa-check"></i><b>15.3</b> Introduction to Copulas</a></li>
<li class="chapter" data-level="15.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopAppl"><i class="fa fa-check"></i><b>15.4</b> Application Using Copulas</a>
<ul>
<li class="chapter" data-level="15.4.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#data-description"><i class="fa fa-check"></i><b>15.4.1</b> Data Description</a></li>
<li class="chapter" data-level="15.4.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#marginal-models"><i class="fa fa-check"></i><b>15.4.2</b> Marginal Models</a></li>
<li class="chapter" data-level="15.4.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#probability-integral-transformation"><i class="fa fa-check"></i><b>15.4.3</b> Probability Integral Transformation</a></li>
<li class="chapter" data-level="15.4.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#joint-modeling-with-copula-function"><i class="fa fa-check"></i><b>15.4.4</b> Joint Modeling with Copula Function</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopTyp"><i class="fa fa-check"></i><b>15.5</b> Types of Copulas</a>
<ul>
<li class="chapter" data-level="15.5.1" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#normal-gaussian-copulas"><i class="fa fa-check"></i><b>15.5.1</b> Normal (Gaussian) Copulas</a></li>
<li class="chapter" data-level="15.5.2" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#t--and-elliptical-copulas"><i class="fa fa-check"></i><b>15.5.2</b> <em>t</em>- and Elliptical Copulas</a></li>
<li class="chapter" data-level="15.5.3" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#archimedean-copulas"><i class="fa fa-check"></i><b>15.5.3</b> Archimedean Copulas</a></li>
<li class="chapter" data-level="15.5.4" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#properties-of-copulas"><i class="fa fa-check"></i><b>15.5.4</b> Properties of Copulas</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#S:CopImp"><i class="fa fa-check"></i><b>15.6</b> Why is Dependence Modeling Important?</a></li>
<li class="chapter" data-level="15.7" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#Dep:further-reading-and-resources"><i class="fa fa-check"></i><b>15.7</b> Further Resources and Contributors</a>
<ul>
<li class="chapter" data-level="" data-path="ChapDependenceModel.html"><a href="ChapDependenceModel.html#ts-15.a.-other-classic-measures-of-scalar-associations"><i class="fa fa-check"></i>TS 15.A. Other Classic Measures of Scalar Associations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="CAppA.html"><a href="CAppA.html"><i class="fa fa-check"></i><b>16</b> Appendix A: Review of Statistical Inference</a>
<ul>
<li class="chapter" data-level="16.1" data-path="CAppA.html"><a href="CAppA.html#S:AppA:BASIC"><i class="fa fa-check"></i><b>16.1</b> Basic Concepts</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="CAppA.html"><a href="CAppA.html#random-sampling"><i class="fa fa-check"></i><b>16.1.1</b> Random Sampling</a></li>
<li class="chapter" data-level="16.1.2" data-path="CAppA.html"><a href="CAppA.html#sampling-distribution"><i class="fa fa-check"></i><b>16.1.2</b> Sampling Distribution</a></li>
<li class="chapter" data-level="16.1.3" data-path="CAppA.html"><a href="CAppA.html#central-limit-theorem"><i class="fa fa-check"></i><b>16.1.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="CAppA.html"><a href="CAppA.html#S:AppA:PE"><i class="fa fa-check"></i><b>16.2</b> Point Estimation and Properties</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="CAppA.html"><a href="CAppA.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>16.2.1</b> Method of Moments Estimation</a></li>
<li class="chapter" data-level="16.2.2" data-path="CAppA.html"><a href="CAppA.html#S:AppA:MLE"><i class="fa fa-check"></i><b>16.2.2</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="CAppA.html"><a href="CAppA.html#S:AppA:IE"><i class="fa fa-check"></i><b>16.3</b> Interval Estimation</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="CAppA.html"><a href="CAppA.html#S:AppA:IE:ED"><i class="fa fa-check"></i><b>16.3.1</b> Exact Distribution for Normal Sample Mean</a></li>
<li class="chapter" data-level="16.3.2" data-path="CAppA.html"><a href="CAppA.html#large-sample-properties-of-mle"><i class="fa fa-check"></i><b>16.3.2</b> Large-sample Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="16.3.3" data-path="CAppA.html"><a href="CAppA.html#confidence-interval"><i class="fa fa-check"></i><b>16.3.3</b> Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT"><i class="fa fa-check"></i><b>16.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="CAppA.html"><a href="CAppA.html#basic-concepts"><i class="fa fa-check"></i><b>16.4.1</b> Basic Concepts</a></li>
<li class="chapter" data-level="16.4.2" data-path="CAppA.html"><a href="CAppA.html#student-t-test-based-on-mle"><i class="fa fa-check"></i><b>16.4.2</b> Student-<em>t</em> test based on <em>mle</em></a></li>
<li class="chapter" data-level="16.4.3" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT:LRT"><i class="fa fa-check"></i><b>16.4.3</b> Likelihood Ratio Test</a></li>
<li class="chapter" data-level="16.4.4" data-path="CAppA.html"><a href="CAppA.html#S:AppA:HT:IC"><i class="fa fa-check"></i><b>16.4.4</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="CAppB.html"><a href="CAppB.html"><i class="fa fa-check"></i><b>17</b> Appendix B: Iterated Expectations</a>
<ul>
<li class="chapter" data-level="17.1" data-path="CAppB.html"><a href="CAppB.html#S:AppB:CD"><i class="fa fa-check"></i><b>17.1</b> Conditional Distribution and Conditional Expectation</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="CAppB.html"><a href="CAppB.html#conditional-distribution"><i class="fa fa-check"></i><b>17.1.1</b> Conditional Distribution</a></li>
<li class="chapter" data-level="17.1.2" data-path="CAppB.html"><a href="CAppB.html#conditional-expectation-and-conditional-variance"><i class="fa fa-check"></i><b>17.1.2</b> Conditional Expectation and Conditional Variance</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="CAppB.html"><a href="CAppB.html#S:AppB:IE"><i class="fa fa-check"></i><b>17.2</b> Iterated Expectations and Total Variance</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="CAppB.html"><a href="CAppB.html#S:AppB:LIE"><i class="fa fa-check"></i><b>17.2.1</b> Law of Iterated Expectations</a></li>
<li class="chapter" data-level="17.2.2" data-path="CAppB.html"><a href="CAppB.html#law-of-total-variance"><i class="fa fa-check"></i><b>17.2.2</b> Law of Total Variance</a></li>
<li class="chapter" data-level="17.2.3" data-path="CAppB.html"><a href="CAppB.html#application"><i class="fa fa-check"></i><b>17.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="CAppB.html"><a href="CAppB.html#S:AppConjugateDistributions"><i class="fa fa-check"></i><b>17.3</b> Conjugate Distributions</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="CAppB.html"><a href="CAppB.html#linear-exponential-family"><i class="fa fa-check"></i><b>17.3.1</b> Linear Exponential Family</a></li>
<li class="chapter" data-level="17.3.2" data-path="CAppB.html"><a href="CAppB.html#S:IterExp:Conjugate"><i class="fa fa-check"></i><b>17.3.2</b> Conjugate Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="CAppC.html"><a href="CAppC.html"><i class="fa fa-check"></i><b>18</b> Appendix C: Maximum Likelihood Theory</a>
<ul>
<li class="chapter" data-level="18.1" data-path="CAppC.html"><a href="CAppC.html#S:AppC:LF"><i class="fa fa-check"></i><b>18.1</b> Likelihood Function</a>
<ul>
<li class="chapter" data-level="18.1.1" data-path="CAppC.html"><a href="CAppC.html#likelihood-and-log-likelihood-functions"><i class="fa fa-check"></i><b>18.1.1</b> Likelihood and Log-likelihood Functions</a></li>
<li class="chapter" data-level="18.1.2" data-path="CAppC.html"><a href="CAppC.html#properties-of-likelihood-functions"><i class="fa fa-check"></i><b>18.1.2</b> Properties of Likelihood Functions</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="CAppC.html"><a href="CAppC.html#S:AppC:MLE"><i class="fa fa-check"></i><b>18.2</b> Maximum Likelihood Estimators</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="CAppC.html"><a href="CAppC.html#definition-and-derivation-of-mle"><i class="fa fa-check"></i><b>18.2.1</b> Definition and Derivation of <em>MLE</em></a></li>
<li class="chapter" data-level="18.2.2" data-path="CAppC.html"><a href="CAppC.html#asymptotic-properties-of-mle"><i class="fa fa-check"></i><b>18.2.2</b> Asymptotic Properties of <em>MLE</em></a></li>
<li class="chapter" data-level="18.2.3" data-path="CAppC.html"><a href="CAppC.html#use-of-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>18.2.3</b> Use of Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="CAppC.html"><a href="CAppC.html#S:AppC:SI"><i class="fa fa-check"></i><b>18.3</b> Statistical Inference Based on Maximum Likelihood Estimation</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="CAppC.html"><a href="CAppC.html#hypothesis-testing"><i class="fa fa-check"></i><b>18.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="18.3.2" data-path="CAppC.html"><a href="CAppC.html#S:AppC:MLEModelVal"><i class="fa fa-check"></i><b>18.3.2</b> <em>MLE</em> and Model Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html"><i class="fa fa-check"></i><b>19</b> Appendix D: Summary of Distributions</a>
<ul>
<li class="chapter" data-level="19.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:DiscreteDistributions"><i class="fa fa-check"></i><b>19.1</b> Discrete Distributions</a>
<ul>
<li class="chapter" data-level="19.1.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab0-class"><i class="fa fa-check"></i><b>19.1.1</b> The <em>(a,b,0)</em> Class</a></li>
<li class="chapter" data-level="19.1.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#the-ab1-class"><i class="fa fa-check"></i><b>19.1.2</b> The <em>(a,b,1)</em> Class</a></li>
</ul></li>
<li class="chapter" data-level="19.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#S:ContinuousDistributions"><i class="fa fa-check"></i><b>19.2</b> Continuous Distributions</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#one-parameter-distributions"><i class="fa fa-check"></i><b>19.2.1</b> One Parameter Distributions</a></li>
<li class="chapter" data-level="19.2.2" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#two-parameter-distributions"><i class="fa fa-check"></i><b>19.2.2</b> Two Parameter Distributions</a></li>
<li class="chapter" data-level="19.2.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#three-parameter-distributions"><i class="fa fa-check"></i><b>19.2.3</b> Three Parameter Distributions</a></li>
<li class="chapter" data-level="19.2.4" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#four-parameter-distribution"><i class="fa fa-check"></i><b>19.2.4</b> Four Parameter Distribution</a></li>
<li class="chapter" data-level="19.2.5" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#other-distributions"><i class="fa fa-check"></i><b>19.2.5</b> Other Distributions</a></li>
<li class="chapter" data-level="19.2.6" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#distributions-with-finite-support"><i class="fa fa-check"></i><b>19.2.6</b> Distributions with Finite Support</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="ChapSummaryDistributions.html"><a href="ChapSummaryDistributions.html#limited-expected-values"><i class="fa fa-check"></i><b>19.3</b> Limited Expected Values</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html"><i class="fa fa-check"></i><b>20</b> Appendix E: Conventions for Notation</a>
<ul>
<li class="chapter" data-level="20.1" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:General"><i class="fa fa-check"></i><b>20.1</b> General Conventions</a></li>
<li class="chapter" data-level="20.2" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Abbreviations"><i class="fa fa-check"></i><b>20.2</b> Abbreviations</a></li>
<li class="chapter" data-level="20.3" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:StatSymbols"><i class="fa fa-check"></i><b>20.3</b> Common Statistical Symbols and Operators</a></li>
<li class="chapter" data-level="20.4" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#S:Symbols"><i class="fa fa-check"></i><b>20.4</b> Common Mathematical Symbols and Functions</a></li>
<li class="chapter" data-level="20.5" data-path="ChapNotationConvention.html"><a href="ChapNotationConvention.html#further-readings"><i class="fa fa-check"></i><b>20.5</b> Further Readings</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="DataResources.html"><a href="DataResources.html"><i class="fa fa-check"></i><b>21</b> Appendix. Data Resources</a>
<ul>
<li class="chapter" data-level="21.1" data-path="DataResources.html"><a href="DataResources.html#S:WiscPropFundA"><i class="fa fa-check"></i><b>21.1</b> Wisconsin Property Fund</a></li>
<li class="chapter" data-level="21.2" data-path="DataResources.html"><a href="DataResources.html#Sec:DataTravel"><i class="fa fa-check"></i><b>21.2</b> ANU Corporate Travel Data</a></li>
<li class="chapter" data-level="21.3" data-path="DataResources.html"><a href="DataResources.html#Sec:DataGPA"><i class="fa fa-check"></i><b>21.3</b> ANU Group Personal Accident Data</a></li>
<li class="chapter" data-level="21.4" data-path="DataResources.html"><a href="DataResources.html#Sec:DataAuto"><i class="fa fa-check"></i><b>21.4</b> ANU Motor Vehicle Data</a></li>
<li class="chapter" data-level="21.5" data-path="DataResources.html"><a href="DataResources.html#spanish-personal-insurance-data"><i class="fa fa-check"></i><b>21.5</b> Spanish Personal Insurance Data</a></li>
<li class="chapter" data-level="21.6" data-path="DataResources.html"><a href="DataResources.html#r-package-casdatasets"><i class="fa fa-check"></i><b>21.6</b> R Package CASdatasets</a></li>
<li class="chapter" data-level="21.7" data-path="DataResources.html"><a href="DataResources.html#other-data-sources"><i class="fa fa-check"></i><b>21.7</b> Other Data Sources</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/OpenActTexts/Loss-Data-Analytics" target="blank">Loss Data Analytics on GitHub</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ChapDataAnalytics" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Data Analytics<a href="ChapDataAnalytics.html#ChapDataAnalytics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Chapter Preview</em>. This introduction to data analytic concepts emphasizes aspects that are likely to be of use within insurance activities. Because data analytics is used in many different fields with differing terminologies, we start in Section <a href="ChapDataAnalytics.html#S:Elements">2.1</a> simply describing the basic ingredients, or elements, of data analytics. Then, Section <a href="ChapDataAnalytics.html#S:Process">2.2</a> outlines a process in which an analyst is likely to employ these elements with a focus on analyzing data associated with insurance activities. Development of data analytics in many fields heavily emphasizes considerations of multiple variables (or big data) almost to the exclusion of the consideration of a single variable, such as an insurance loss on a particular type of coverage. So, Section <a href="ChapDataAnalytics.html#S:SingleVarAnalytics">2.3</a> introduces what we coin single variable analytics that includes a description of variable types, exploratory versus confirmatory analysis, and elements of model construction and selection that can be done in the context of a single variable. Building on this, Section <a href="ChapDataAnalytics.html#S:ManyVarAnalytics">2.4</a> continues the introduction to analytics relying on the roles of supervised and unsupervised learning, concepts that require the presence of many variables.</p>
<p>The final section of the chapter, Section <a href="ChapDataAnalytics.html#S:DataLearn">2.5</a>, provides a broader introduction to data considerations that go beyond this book for budding analysts who wish to use this chapter to build a foundation for further studies of data analytics. In the same way, the technical supplements also introduce other standard ingredients of data analytics such as principal components, cluster analysis, and tree-based regression models that are not needed in this book but are important in a broader analytics context.</p>
<div id="S:Elements" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Elements of Data Analytics<a href="ChapDataAnalytics.html#S:Elements" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In this section, you learn how to describe the essential ingredients of data analytics</p>
<ul>
<li>consisting of several key concepts, and</li>
<li>two fundamental approaches, data and algorithmic modeling.</li>
</ul>
<hr />
<p><strong>Data analysis</strong> involves inspecting, cleansing, transforming, and modeling data to discover useful information to suggest conclusions and make decisions. Data analysis has a long history. In 1962, statistician John Tukey defined data analysis as:</p>
<blockquote>
<p>procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.</p>
<p> <span class="citation">(<a href="#ref-tukey1962data" role="doc-biblioref">Tukey 1962</a>)</span></p>
</blockquote>
<div id="key-data-analytic-concepts" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Key Data Analytic Concepts<a href="ChapDataAnalytics.html#key-data-analytic-concepts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Underpinning the elements of data analytics are the following key concepts:</p>
<ul>
<li><strong>Data Driven</strong>. As described in Section <a href="ChapIntro.html#S:DataDriven">1.1.2</a>, the conclusions and decisions made through a data analytic process depend heavily on data inputs. In comparison, econometricians have long recognized the difference between a data-driven model and a structural model, the latter being one that represents an explicit interplay between economic theory and stochastic models, <span class="citation"><a href="#ref-goldberger1972structural" role="doc-biblioref">Goldberger</a> (<a href="#ref-goldberger1972structural" role="doc-biblioref">1972</a>)</span> .</li>
<li><strong>EDA</strong> - exploratory data analysis - and <strong>CDA</strong> - confirmatory data analysis. Although some techniques overlap, e.g., taking the average of a dataset, these two approaches to analyzing data have different purposes. The purpose of EDA is to reveal aspects or patterns in the data without reference to any particular model. In contrast, CDA techniques use data to substantiate, or confirm, aspects or patterns in a model. See Section <a href="ChapDataAnalytics.html#S:EDACDA">2.3.2</a> for further discussions.</li>
<li><strong>Estimation</strong> and <strong>Prediction</strong>. Recall the traditional triad of statistical inference: hypothesis testing, parameter estimation, and prediction. Medical statisticians test the efficacy of a new drug and econometricians estimate parameters of an economic relationship. In insurance, one also uses hypothesis testing and parameter estimation. Moreover, predictions of yet to be realized random outcomes are critical for financial risk management (e.g., pricing) of existing risks in future periods, as well as not yet observed risks in a current period, cf. <span class="citation"><a href="#ref-frees2015analytics" role="doc-biblioref">Frees</a> (<a href="#ref-frees2015analytics" role="doc-biblioref">2015</a>)</span>.</li>
<li><strong>Model Complexity,</strong> <strong>Parsimony,</strong> and <strong>Interpretability</strong>. A model is a mathematical representation of reality that, in statistics, is calibrated using a data set. One concern is the <em>complexity</em> of the model where the complexity may involve the number of parameters used to define the model, the number of variables upon which it relies, and the intricacies of relationships among the parameters and variables. As a rule of thumb, we will see that the more complex is the model, the better it fares in fitting a set of data (and hence at estimation) but the worse it fares in predicting new outcomes. Other things being equal, a model with fewer parameters is said to be <em>parsimonious</em> and hence less complex. Moreover, a parsimonious model is typically easier to interpret than a comparable model that is more complex. Complexity hinders our ability to understand the inner workings of a model, its interpretability, and will be a key ingredient in our comparisons of data versus algorithmic models in Section <a href="ChapDataAnalytics.html#S:DataAlgorithm">2.1.2</a>.</li>
<li><strong>Parametric</strong> and <strong>Nonparametric</strong> models. Many models, including stochastic distributions, are known with the exception of a limited number of quantities known as parameters. For example, the mean and variance are parameters that determine a normal distribution. In contrast, other models may not rely on parameters; these are simply known as <em>nonparametric</em> models. Naturally, there is also a host of models that rely on parameters for some parts of the distribution and are distribution-free for other portions; these are referred to as <em>semi-parametric</em> models. Parametric and nonparametric approaches have different strengths and limitations; neither is strictly better than the other. We start the discussion in Section <a href="ChapDataAnalytics.html#S:ParametricNonP">2.3.3.1</a> to explain under what circumstances you might prefer one approach to another.</li>
<li><strong>Robustness</strong> refers to the health of a model to unanticipated deviations in model assumptions or the data used to calibrate the model. When interpreting findings, it is natural to ask questions about how the results react to changes in assumptions or data, that is, the robustness of the results.</li>
<li><strong>Computational Statistics</strong>. Historically, statistical modeling relied extensively on summary statistics that were not only easy to interpret but also easy to compute. With modern-day computing power, definitions of easy to compute have altered drastically paving the way for measures that were once deemed far too computationally intensive to be of practical use. Moreover, ideas of subsampling and resampling data (e.g., through cross-validation and bootstrapping) have introduced new methods for understanding statistical sampling errors and a models predictive capabilities.</li>
<li><strong>Big Data</strong>. This is about the process of using special methods and tools that can extract information rapidly from massive data. Examples of big data include text documents, videos, and audio files that are also known as <em>unstructured</em> data. <a href="#tab:2.1">Table 2.1</a> summarizes new types of data sources that lead to new data. As part of the analytics trends, different types of algorithms lead to new software for handling new types of data. See Section <a href="ChapDataAnalytics.html#Sec:BigDataAnalysis">2.5.4</a> for further discussions.</li>
</ul>
<p><a id=tab:2.1></a></p>
<p>Table 2.1. <strong>Analytic Trends</strong> (from <span class="citation"><a href="#ref-Frees2019" role="doc-biblioref">Frees and Gao</a> (<a href="#ref-Frees2019" role="doc-biblioref">2019</a>)</span>)</p>
<p><span class="math display">\[
{\small
\begin{array}{l|l}
    \hline
\textbf{Data Sources}                   &amp; \textbf{Algorithms}\\ \hline
\text{Mobile devices}                   &amp; \text{Statistical learning} \\
\text{Auto telematics}                  &amp; \text{Artificial intelligence}\\
\text{Home sensors (Internet of Things)}&amp;  \text{Structural models}\\
\text{Drones, micro satellites}         &amp;\\ \hline
\textbf{Data}                           &amp; \textbf{Software}  \\ \hline
\text{Big data (text, speech, image, video)}   &amp; \text{Text analysis, semantics}  \\
\text{Behavioral data (including social media)}&amp; \text{Voice recognition}\\
\text{Credit, trading, financial data}         &amp; \text{Image recognition}  \\
                                               &amp; \text{Video recognition} \\    \hline
{{\tiny \textit{Source}:\text{Stephen Mildenhall, Personal Communication}}} &amp;  \\ \hline
\end{array}
}
\]</span></p>
</div>
<div id="S:DataAlgorithm" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Data versus Algorithmic Modeling<a href="ChapDataAnalytics.html#S:DataAlgorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are two cultures for the use of statistical modeling to reach conclusions from data: the data modeling culture and the algorithmic modeling culture. In the data modeling culture, data are assumed to be generated by a given stochastic model. In the algorithmic modeling culture, the data mechanism is treated as unknown and algorithmic models are used.</p>
<p><a href="#" class="tooltip" style="color:green"><em>Data modeling</em><span style="font-size:8pt">Assumes data generated comes from a stochastic data model</span></a> allows statisticians to analyze data and acquire information about the data mechanisms. However, <span class="citation"><a href="#ref-breiman2001modeling" role="doc-biblioref">Breiman</a> (<a href="#ref-breiman2001modeling" role="doc-biblioref">2001</a>)</span> argued that the focus on data modeling in the statistical community has led to some side effects such as:</p>
<ul>
<li>It produced irrelevant theory and questionable scientific conclusions.</li>
<li>It kept statisticians from using algorithmic models that might be more suitable.</li>
<li>It restricted the ability of statisticians to deal with a wide range of problems.</li>
</ul>
<p><a href="#" class="tooltip" style="color:green"><em>Algorithmic modeling</em><span style="font-size:8pt">Assumes data generated comes from unknown algorithmic models</span></a> was used by industrial statisticians long time ago. Sadly, the development of algorithmic methods was taken up by communities outside statistics. The goal of algorithmic modeling is <a href="#" class="tooltip" style="color:green"><em>predictive accuracy</em><span style="font-size:8pt">Quantitative measure of how well the explanatory variables predict the response outcome</span></a>. For some complex prediction problems, data models are not suitable. These prediction problems include voice recognition, image recognition, handwriting recognition, nonlinear time series prediction, and financial market prediction. The theory in algorithmic modeling focuses on the properties of algorithms, such as convergence and predictive accuracy.</p>
<div id="surveyElement2Data1">

</div>
<div id="surveyResult2Data1">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz2Data1.1" href="javascript:toggleQuiz
('display.Quiz2Data1.2','display.Quiz2Data1.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz2Data1.2" style="display: none">
<p id="Quiz2Data1Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz2Data1.js">
</script>
</div>
</div>
<div id="S:Process" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Data Analysis Process<a href="ChapDataAnalytics.html#S:Process" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In this section, you learn how to describe the data analysis process as five steps:</p>
<ul>
<li>scoping phase,</li>
<li>data splitting,</li>
<li>model development,</li>
<li>validation, and</li>
<li>determining implications.</li>
</ul>
<hr />
<p><a href="#tab:2.2">Table 2.2</a> outlines common steps used when analyzing data associated with insurance activities.</p>
<p><a id=tab:2.2></a></p>
<p>Table 2.2 <strong>Data Analysis Process for Insurance Activities</strong></p>
<p><span class="math display">\[
{\small
\begin{array}{c|c|c|c|c}\hline
\textbf{I. Scoping} &amp;\textbf{II. Data}&amp; \textbf{III. Model} &amp; \textbf{IV. Validation} &amp; 
\textbf{V. Determine} \\ 
\textbf{Phase} &amp;\textbf{ Splitting}&amp; \textbf{ Development} &amp;   &amp; 
\textbf{Implications} \\ \hline
\text{Use background} &amp;\text{Split the}&amp; \text{Select a candidate} &amp; \text{Repeat Phase III} &amp; 
\text{Use knowledge gained} \\
\text{knowledge and} &amp;\text{data into}&amp; \text{model} &amp; \text{ to determine several} &amp; 
\text{from exploring the data,} \\
 \text{theory to} &amp;\text{training}&amp; &amp;\text{candidate models}  &amp; 
\text{fitting and predicting} \\
 \text{define goals} &amp;\text{and testing}&amp;&amp;&amp; \text{the models to make} \\ 
&amp;\text{portions}&amp;&amp;&amp; \text{data-informed statements} \\ 
&amp;&amp;&amp;&amp; \text{about the project goals} \\ \\ 
\text{Prepare, collect,}&amp;&amp;\text{Select variables to} &amp;\text{Assess each model}  \\
\text{and revise data}&amp;&amp;\text{ be used with the} &amp; \text{using the testing}&amp;  \\
&amp;&amp;\text{candidate model} &amp; \text{portion of the data}&amp;  \\ 
&amp;&amp; &amp; \text{ to determine its}&amp;  \\ 
&amp;&amp; &amp; \text{predictive capabilities}&amp;  \\\\
\text{EDA}&amp;&amp;\text{Evaluate model fit}
&amp;&amp;\\
\text{Explore the data}&amp;&amp;\text{using training data}
&amp;&amp;\\ \\
&amp;&amp;\text{Use deviations from}\\
&amp;&amp;\text{ model fit to improve}\\
&amp;&amp;\text{suggested models}\\
\hline
\end{array}
}
\]</span></p>
<div id="i.-scoping-phase" class="section level4 unnumbered hasAnchor">
<h4>I. Scoping Phase<a href="ChapDataAnalytics.html#i.-scoping-phase" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Scoping, or problem formulation, can be divided into three components:</p>
<ul>
<li><strong>Use background knowledge and theory to define goals</strong>. Insurance activity projects are commonly motivated by business pursuits that have been formulated to be consistent with background knowledge such as market conditions and theory such as a persons attitude towards risk-taking.</li>
<li><strong>Prepare, collect, and revise data</strong>. Getting the right data that gives insights into questions at hand is typically the most time-consuming aspect of most projects. Section <a href="ChapDataAnalytics.html#S:DataLearn">2.5</a> delves more into the devilish details of data structures, quality, cleaning, and so forth.</li>
<li><strong>EDA</strong> - Exploring the data, without reference to any particular model, can reveal unsuspected aspects or patterns in the data.</li>
</ul>
<p>These three components can be performed <em>iteratively</em>. For example, a question may suggest collecting certain data types. Then, a preliminary analysis of the data raises additional questions of interest that can lead to seeking more data - this cycle can be repeated many times. Note that this iterative approach differs from the traditional scientific method whereby the analyst develops a hypothesis, collects data, and then employs the data to test the hypothesis.</p>
</div>
<div id="ii.-data-splitting" class="section level4 unnumbered hasAnchor">
<h4>II. Data Splitting<a href="ChapDataAnalytics.html#ii.-data-splitting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Although optional, splitting the data into training and testing portions has some important advantages. If the available dataset is sufficiently large, one can split the data into a portion used to calibrate one or more candidate models, the training portion, and another portion that can be used for testing, that is, evaluating the predictive capabilities of the model. The data splitting procedure guards against overfitting a model and emphasizes predictive aspects of a model. For many applications, the splitting is done randomly to mitigate unanticipated sources of bias. For some applications such as insurance, it is common to use data from an earlier time period to predict, or <em>forecast</em>, future behavior. For example, with the Section <a href="ChapIntro.html#S:LGPIF">1.3</a> Wisconsin Property Fund data, one might use 2006-2010 data for training and 2011 data for assessing predictions.</p>
<p>For large datasets, some analysts prefer to split the data into three portions, one for training (model estimation), one for validation (estimate prediction error for model selection), and one for testing (assessment of the generalization error of the final chosen model), c.f. <span class="citation"><a href="#ref-hastie2009elements" role="doc-biblioref">Hastie, Tibshirani, and Friedman</a> (<a href="#ref-hastie2009elements" role="doc-biblioref">2009</a>)</span> (Chapter 7). In contrast, for moderate and smaller datasets, it is common to use <a href="#" class="tooltip" style="color:green"><em>cross-validation</em><span style="font-size:8pt">A model validation procedure in which the data sample is partitioned into subsamples, where splits are formed by separately taking each subsample as the out-of-sample dataset.</span></a> techniques where one repeatedly splits the dataset into training and testing portions and then averages results over many applications. These techniques are described further in Chapter <a href="ChapSimulation.html#ChapSimulation">7</a>.</p>
</div>
<div id="iii.-model-development" class="section level4 unnumbered hasAnchor">
<h4>III. Model Development<a href="ChapDataAnalytics.html#iii.-model-development" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The objective of the model development phase is to consider different types of model and provide the best fit for each candidate model. As with the scoping phase, developing a model is an iterative procedure.</p>
<ul>
<li><strong>Select a candidate model.</strong> One starts with a model that, from the analysts perspective, is a likely candidate to be the recommended model. Although analysts will focus on familiar models, such as through their past applications of a model or its acceptance in industry, in principle one remains open to all types of models.</li>
<li><strong>Select variables to be used with the candidate model.</strong> For simpler situations, only a single outcome, or variable, is of interest. However, many (if not most) situations deal with multivariate outcomes and, as will be seen in Section <a href="ChapDataAnalytics.html#S:ManyVarAnalytics">2.4</a>, analysts give a great deal of thought as to which variables are considered inputs to a system and which variables can be treated as outcomes.</li>
<li><strong>Evaluate model fit on training data.</strong> Given a candidate model based on one or more selected variables, the next step is to calibrate the model based on the training data and evaluate the model fit. Many measures of model fit are available - analysts should focus on those likely to be consistent with the project goals and intended audience of the data analysis process.</li>
<li><strong>Use deviations from the model fit to suggest improvements to the candidate model.</strong> When comparing the training data to model fits, it may be that certain patterns are revealed that suggest model improvements. In regression analysis, this tactic is known as <em>diagnostic checking.</em></li>
</ul>
</div>
<div id="iv.-validation" class="section level4 unnumbered hasAnchor">
<h4>IV. Validation<a href="ChapDataAnalytics.html#iv.-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Repeat Phase III to determine several candidate models.</strong> There is a wealth of potential models from which an analyst can choose. Some are parametric, others non-parametric, and some a mixture between the two. Some focus on simplicity such as through linear relationships whereas others are much more complex. And so on. Through repeated applications of the Phase III process, it is customary to narrow the field of candidates down to a handful based on their fit to the training data.</li>
<li><strong>Assess each model use the testing portion of the data to determine its predictive capabilities.</strong> With the handful of models that perform the best in the model development phase, one assesses the predictive capabilities of each model. Specifically, each fitted model is used to make predictions with the predicted outcomes compared to the held-out test data. This comparison may also be done using cross-validation. Models are then compared based on their predictive capabilities.</li>
</ul>
</div>
<div id="v.-determine-implications" class="section level4 unnumbered hasAnchor">
<h4>V. Determine Implications<a href="ChapDataAnalytics.html#v.-determine-implications" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The scoping, model development, and validation phases all contribute to making data-informed statements about the project goals. Although most projects result in a single recommended model, each phase has the potential to lend powerful insights.</p>
<p>For data analytic projects associated with insurance activities, it is common to select the model with best predictive capabilities. However, analysts are also mindful of the intended audiences of their analyses, and it is also common to favor models that are simpler and easier to interpret. The relative importance of interpretability very much depends on the project goals. For example, a model devoted to enticing potential customers to view a webpage can be judged more on its predictive capabilities. In contrast, a model that provides the foundations for insurance prices typically undergoes scrutiny by regulators and consumer advocacy groups; here, interpretation plays an important role.</p>
<div id="surveyElement2Data2">

</div>
<div id="surveyResult2Data2">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz2Data2.1" href="javascript:toggleQuiz
('display.Quiz2Data2.2','display.Quiz2Data2.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz2Data2.2" style="display: none">
<p id="Quiz2Data2Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz2Data2.js">
</script>
</div>
</div>
<div id="S:SingleVarAnalytics" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Single Variable Analytics<a href="ChapDataAnalytics.html#S:SingleVarAnalytics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In this section, you learn how to describe analytics based on a single variable in terms of</p>
<ul>
<li>the type of variable,</li>
<li>exploratory versus confirmatory analyses,</li>
<li>model construction and</li>
<li>model selection.</li>
</ul>
<hr />
<p>Rather than starting with multiple variables consisting of inputs and outputs as is common in analytics, in this section we restrict considerations to a single variable. Single variable analytics is motivated by statistical data modeling. Moreover, as will be seen in Chapters 3-8, single variable analytics plays a prominent role in fundamental insurance and risk management applications.</p>
<div id="S:VarTypes" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Variable Types<a href="ChapDataAnalytics.html#S:VarTypes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section describes basic variable types traditionally encountered in statistical data analysis. Section <a href="ChapDataAnalytics.html#S:DataLearn">2.5</a> will provide a framework for more extensive types that include big data.</p>
<div id="S:QuaVar" class="section level4 hasAnchor" number="2.3.1.1">
<h4><span class="header-section-number">2.3.1.1</span> Qualitative Variables<a href="ChapDataAnalytics.html#S:QuaVar" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A <a href="#" class="tooltip" style="color:green"><em>qualitative</em><span style="font-size:8pt">This is a type of variable in which the measurement denotes membership in a set of groups, or categories</span></a>, or <a href="#" class="tooltip" style="color:green"><em>categorical variable</em><span style="font-size:8pt">A variable whose values are qualitative groups and can have no natural ordering (nominal) or an ordering (ordinal)</span></a> is one for which the measurement denotes membership in a set of groups, or categories. For example, if you were coding in which area of the country an insured resides, you might use 1 for the northern part, 2 for southern, and 3 for everything else. Any analysis of categorical variables should not depend on the labeling of the categories. For example, instead of using a 1,2,3 for north, south, other, one should arrive at the same set of summary statistics if I used a 2,1,3 coding instead, interchanging north and south.</p>
<p>In contrast, an <a href="#" class="tooltip" style="color:green"><em>ordinal variable</em><span style="font-size:8pt">This is a type of qualitative/ categorical variable which has two or more ordered categories.</span></a> is a variation of categorical variable for which an ordering exists. For example, with a survey to see how satisfied customers are with our claims servicing department, we might use a five point scale that ranges from 1 meaning dissatisfied to a 5 meaning satisfied. Ordinal variables provide a clear ordering of levels of a variable although the amount of separation between levels is unknown.</p>
<p>A <a href="#" class="tooltip" style="color:green"><em>binary variable</em><span style="font-size:8pt">Is a special type of categorical variable where there are only two categories.</span></a> is a special type of categorical variable where there are only two categories commonly taken to be 0 and 1.</p>
<p>Earlier, in the Section <a href="ChapIntro.html#S:LGPIF">1.3</a> case study, we saw in <a href="#tab:1.5">Table 1.5</a> several examples of qualitative variables. These included the categorical <code>EntityType</code> and binary variables <code>NoClaimCredit</code> and <code>Fire5</code>. We also treated <code>AlarmCredit</code> as a categorical variable although some analysts may wish to explore its use as an ordinal variable.</p>
</div>
<div id="S:QuanVar" class="section level4 hasAnchor" number="2.3.1.2">
<h4><span class="header-section-number">2.3.1.2</span> Quantitative Variables<a href="ChapDataAnalytics.html#S:QuanVar" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Unlike a qualitative variable, a <a href="#" class="tooltip" style="color:green"><em>quantitative variable</em><span style="font-size:8pt">A quantitative variable is a type of variable in which numerical level is a realization from some scale so that the distance between any two levels of the scale takes on meaning.</span></a> is one in which each numerical level is a realization from some scale so that the distance between any two levels of the scale takes on meaning. A <a href="#" class="tooltip" style="color:green"><em>continuous variable</em><span style="font-size:8pt">A continuous variable is a quantitative variable that can take on any value within a finite interval.</span></a> is one that can take on any value within a finite interval. For example, one could represent a <a href="#" class="tooltip" style="color:green"><em>policyholder</em><span style="font-size:8pt">Person in actual possession of insurance policy; policy owner.</span></a>s age, weight, or income, as continuous variables. In contrast, a
<a href="#" class="tooltip" style="color:green"><em>discrete variable</em><span style="font-size:8pt">A discrete variable is quantitative variable that takes on only a finite number of values in any finite interval.</span></a> is one that takes on only a finite number of values in any finite interval. For example, when examining a policyholders choice of <a href="#" class="tooltip" style="color:green"><em>deductibles</em><span style="font-size:8pt">A deductible is a parameter specified in the contract. Typically, losses below the deductible are paid by the policyholder whereas losses in excess of the deductible are the insurers responsibility (subject to policy limits and coninsurance).</span></a>, it may be that values of 0, 250, 500, and 1000 are the only possible outcomes. Like an ordinal variable, these represent distinct categories that are ordered. Unlike an ordinal variable, the numerical difference between levels takes on economic meaning. A special type of discrete variable is a <a href="#" class="tooltip" style="color:green"><em>count variable</em><span style="font-size:8pt">A count variable is a discrete variable with values on nonnegative integers.</span></a>, one with values on the nonnegative integers. For example, we will be particularly interested in the number of claims arising from a policy during a given period. Another interesting variation is an <a href="#" class="tooltip" style="color:green"><em>interval variable</em><span style="font-size:8pt">An ordinal variable with the additional property that the magnitudes of the differences between two values are meaningful</span></a>, one that gives a range of possible outcomes.</p>
<p>Earlier, in the Section <a href="ChapIntro.html#S:LGPIF">1.3</a> case study, we encountered several examples of quantitative variables. These included the deductible (in logarithmic dollars), total building and content coverage (in logarithmic dollars), claim severity and claim frequency.</p>
</div>
<div id="loss-data" class="section level4 hasAnchor" number="2.3.1.3">
<h4><span class="header-section-number">2.3.1.3</span> Loss Data<a href="ChapDataAnalytics.html#loss-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This introduction to data analytics is motivated by features of <strong>loss data</strong> that arise from, or are related to, obligations in insurance contracts. Loss data rarely arise from a bell-shaped normal distribution that has motivated the development of much of classical statistics. As a consequence, the treatment of data analytics in this text differs from that typically encountered in other introductions to data analytics.</p>
<p>What features of loss data warrant special treatment?</p>
<ul>
<li>We have already seen in the Section <a href="ChapIntro.html#S:LGPIF">1.3</a> case study that we will be concerned with the frequency of losses, a type of count variable.</li>
<li>Further, when a loss occurs, the interest is in the amount of the claim, a quantitative variable. This claim severity is commonly modeled using skewed and long-tailed distributions so that extremely large outcomes are associated with relatively large probabilities. Typically, the normal distribution is a poor choice for a loss distribution.</li>
<li>When a loss does occurs, often the analyst only observes a value that is modified by insurance contractual features such as deductibles, upper limits, and co-insurance parameters.</li>
<li>Loss data are frequently a <em>combination of discrete and continuous</em> components. For example, when we analyze the insured <a href="#" class="tooltip" style="color:green"><em>loss</em><span style="font-size:8pt">The amount of damages sustained by an individual or corporation, typically as the result of an insurable event.</span></a> of a policyholder, we will encounter a discrete outcome at zero, representing no insured loss, and a continuous amount for positive outcomes, representing the amount of the insured loss.</li>
</ul>
</div>
</div>
<div id="S:EDACDA" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Exploratory versus Confirmatory<a href="ChapDataAnalytics.html#S:EDACDA" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are two phases of data analysis: <a href="#" class="tooltip" style="color:green"><em>exploratory data analysis</em><span style="font-size:8pt">Approach to analyzing data sets to summarize their main characteristics, using visual methods, descriptive statistics, clustering, dimension reduction</span></a> (EDA) and <a href="#" class="tooltip" style="color:green"><em>confirmatory data analysis</em><span style="font-size:8pt">Process used to challenge assumptions about the data through hypothesis tests, significance testing, model estimation, prediction, confidence intervals, and inference</span></a> (CDA). <a href="#tab:2.2">Table 2.2</a> summarizes some differences between EDA and CDA. EDA is usually applied to observational data with the goal of looking for patterns and formulating hypotheses. In contrast, CDA is often applied to experimental data (i.e., data obtained by means of a formal design of experiments) with the goal of quantifying the extent to which discrepancies between the model and the data could be expected to occur by chance.</p>
<p><a id=tab:2.2></a></p>
<p>Table 2.2. <strong>Comparison of Exploratory Data Analysis and Confirmatory Data Analysis</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{lll} \hline
 &amp; \textbf{EDA} &amp; \textbf{CDA} \\\hline
\text{Data} &amp; \text{Observational data} &amp; \text{Experimental data}\\[3mm]
\text{Goal} &amp; \text{Pattern recognition,}  &amp; \text{Hypothesis testing,}  \\
&amp; \text{formulate hypotheses} &amp; \text{estimation, prediction} \\[3mm]
\text{Techniques} &amp; \text{Descriptive statistics,} &amp; \text{Traditional statistical tools of} \\
&amp; \text{visualization, clustering} &amp; \text{inference, significance, and}\\
&amp; &amp; \text{confidence} \\
\hline
\end{array}
}
\]</span></p>
<p>As we have seen in the Section <a href="ChapIntro.html#S:LGPIF">1.3</a> case study, the techniques for single variable EDA include descriptive statistics (e.g., mean, median, standard deviation, quantiles) and summaries of distributions such as through histograms. In contrast, the techniques for CDA include the traditional statistical tools of inference, significance, and confidence.</p>
</div>
<div id="model-construction" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Model Construction<a href="ChapDataAnalytics.html#model-construction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we learned in Section <a href="ChapDataAnalytics.html#S:DataAlgorithm">2.1.2</a>, models may have a stochastic basis from the statistical modeling paradigm or may simply be the result of an algorithm. When construction a model, it is helpful to think about how it is parameterized and to identify the purpose of constructing the model.</p>
<div id="S:ParametricNonP" class="section level4 hasAnchor" number="2.3.3.1">
<h4><span class="header-section-number">2.3.3.1</span> Parametric versus Nonparametric<a href="ChapDataAnalytics.html#S:ParametricNonP" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Data analysis models can be parametric or nonparametric. Parametric models are representations that are known up to a few terms known as <em>parameters</em>. These may be representations of a stochastic distribution or simply an algorithm used to predict data outcomes. Typically, data are used to determine the parameters and in this way calibrate the model. In contrast, nonparametric methods make no such assumption of a known functional form. As example, Section <a href="ChapModelSelection.html#S:MS:NonParInf">5.1</a> introduces nonparametric methods that do not assume distributions for the data and therefore are also called <em>distribution-free</em> methods.</p>
<p>Because a functional form is known with a parametric model, this approach works well went data size is relatively limited. This reasoning extends to the situation when one is considering many variables simultaneously so that the so-called curse of dimensionality effectively limits the sample size. For example if you are trying to determine the expected cost of automobile losses, you are likely to consider a drivers age, gender, driving location, type of vehicle, and dozens of other variables. Approaches that use some parametric relationships among these variables are common because a purely non-parametric approach would require data sets too large to be useful in practice.</p>
<p>Nonparametric methods are very valuable particularly at the exploratory stages of an analysis where one tries to understand the distribution of each variable. Because nonparametric methods make fewer assumptions, they can be more flexible, more <a href="#" class="tooltip" style="color:green"><em>robust</em><span style="font-size:8pt">Statistics which are more unaffected by outliers or small departures from model assumptions</span></a>, and more applicable to non-quantitative data. However, a drawback of nonparametric methods is that it is more difficult to extrapolate findings outside of the observed domain of the data, a key consideration in <em>predictive modeling</em>.</p>
</div>
<div id="S:expred" class="section level4 hasAnchor" number="2.3.3.2">
<h4><span class="header-section-number">2.3.3.2</span> Explanation versus Prediction<a href="ChapDataAnalytics.html#S:expred" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>There are two goals in data analysis: explanation and prediction. In some scientific areas such as economics, psychology, and environmental science, the focus of data analysis is to explain the causal relationships between the input variables and the response variable. In other scientific areas such as natural language processing, bioinformatics, and actuarial science, the focus of data analysis is to predict what the responses are going to be given the input variables.</p>
<p><span class="citation"><a href="#ref-shmueli2010model" role="doc-biblioref">Shmueli</a> (<a href="#ref-shmueli2010model" role="doc-biblioref">2010</a>)</span> discussed in detail the distinction between explanatory modeling and predictive modeling. <a href="#" class="tooltip" style="color:green"><em>Explanatory modeling</em><span style="font-size:8pt">Process where the modeling goal is to identify variables with meaningful and statistically significant relationships and test hypotheses</span></a> is commonly used for theory building and testing and is typically done as follows:</p>
<ul>
<li>State the prevailing theory.</li>
<li>State causal hypotheses, which are given in terms of theoretical constructs rather than measurable variables. A causal diagram is usually included to illustrate the hypothesized causal relationship between the theoretical constructs.</li>
<li>Operationalize constructs. In this step, previous literature and theoretical justification are used to build a bridge between theoretical constructs and observable measurements.</li>
<li>Collect data and build models alongside the statistical hypotheses, which are operationalized from the research hypotheses.</li>
<li>Reach research conclusions and recommend policy. The statistical conclusions are converted into research conclusions or policy recommendations.</li>
</ul>
<p>In contrast, <a href="#" class="tooltip" style="color:green"><em>predictive modeling</em><span style="font-size:8pt">Process where the modeling goal is to predict new observations</span></a> is the process of applying a statistical model or data mining algorithm to data for the purpose of predicting new or future observations. Predictions include point predictions, interval predictions, regions, distributions, and rankings of new observations. A predictive model can be any method that produces predictions.</p>
</div>
</div>
<div id="model-selection" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Model Selection<a href="ChapDataAnalytics.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Although hypothesis testing is one approach to model selection that is viable in many fields, it does have its drawbacks. For example, the asymmetry between the null and alternative hypotheses raises issues; hypothesis testing is biased towards a null hypothesis unless there is strong evidence to the contrary.</p>
<p>For modeling insurance activities, it is typically preferable to estimate the predictive power of various models and select a model with the best predictive power. The motivation for this is that we want good model selection methods achieve a balance between goodness of fit and parsimony. This is a trade-off because on the one hand, better fits to the data can be achieved by adding more parameters, making the model more complex and less parsimonious. On the other hand, models with fewer parameters (parsimonious) are attractive because of their simplicity and interpretability; they are also less subject to estimation variability and so can yield more accurate predictions, <span class="citation"><a href="#ref-ruppert2003semiparametric" role="doc-biblioref">Ruppert, Wand, and Carroll</a> (<a href="#ref-ruppert2003semiparametric" role="doc-biblioref">2003</a>)</span>.</p>
<p>One way of measuring this balance is through information criteria such as Akaikes Information Criterion (<a href="#" class="tooltip" style="color:green"><em>AIC</em><span style="font-size:8pt">A goodness of fit measure of a statistical model that describes how well it fits a set of observations.</span></a>) and the Bayesian Information Criterion (<a href="#" class="tooltip" style="color:green"><em>BIC</em><span style="font-size:8pt">Bayesian information criterion</span></a>). These measures each contain a component that summarizes how well the model fits the data, a <a href="#" class="tooltip" style="color:green"><em>goodness of fit</em><span style="font-size:8pt">A measure used to assess how well a statistical model fits the data, usually by summarizing the discrepancy between the observations and the expected values under the model.</span></a> piece, plus a component to penalize the complexity of the model.</p>
<p>Although attractive due to their simplicity, there are drawbacks to these measures. In particular, both rely on knowledge of the underlying distribution of the outcomes (or at least good estimates). A more robust approach is to split a data set in a portion that can used to calibrate a model, the <em>training</em> portion, and another portion used to quantify the predictive power of the model, the <em>test</em> portion. It is more robust in the sense that it does not rely on any distributional assumptions and can be used to validate general models.</p>
<p>The data splitting approach is attractive because it directly aligns with the concept of assessing predictive power and can be used in general, and complex, situations. However, it does introduce additional variability into the process by introducing extra randomness of the uncertainty of which observations fall into the training and testing portions. To mitigate this problem, it is common to use an approach known as <em>cross-validation.</em> To illustrate, suppose that one randomly partitions a dataset into five subsets of roughly equivalent sizes</p>
<p><span class="math display">\[
\fbox{Train} \ \ \ \fbox{Test} \ \ \ \fbox{Train} \ \ \ \fbox{Train} \ \ \ \fbox{Train}
\]</span></p>
<p>Then, based on the first, third, fourth, and fifth subsets, estimate a model, use this fitted model to predict outcomes in the second, and compare the predictions to the held-out values in the test portion. Repeat this process by selecting each subset as the test portion, with the others being used for training, and take an average over the comparison which results in a cross-validation statistic. Cross-validation is used widely in modeling insurance activities and is described in more detail in Chapter 5.</p>
<p><strong>Example 2.3.1. Under- and Over-Fitting.</strong> Suppose that we have a set of claims that potentially varies by a single categorical variable with six levels. For example, in the Section <a href="ChapIntro.html#S:LGPIF">1.3</a> case study there are six entity types. If each level is truly distinct, then in classical statistics one uses the level average to make predictions for future claims. Another option is to ignore information in the categorical variable and use the overall average to make predictions; this is known as a community-rating approach.</p>
<p>For illustrative purposes, we assume that two of the six levels differ from the others yet are the same. For example, the <a href="../docs/ChapIntro.html#tab:1.6">Table 1.6</a> summary statistics suggest that Schools and the Miscellaneous levels warrant a higher predicted claims amount than the other four levels. For illustrative purposes, we generated 100 claims that follow this pattern (using simulation techniques that will be described in Chapter <a href="ChapSimulation.html#ChapSimulation">7</a>).</p>
<p>Results are summarized in Table <a href="ChapDataAnalytics.html#tab:Example231">2.1</a> for three fitted models. These are the Community Rating corresponding to using the overall mean, the Two Levels corresponding to using two averages, and the Six Levels corresponding to using an average for each level of the categorical variable. The data set of size 100 was randomly split into five folds; for each fold, the other folds were used to train/estimate the model and then that fold was used to assess predictions. The first five rows of Table <a href="ChapDataAnalytics.html#tab:Example231">2.1</a> give the results of the root mean square error for each fold. The sixth row provides the average over the five folds and the last row gives a similar result for another goodness of fit statistic, the <span class="math inline">\(AIC\)</span>. This approach is known as cross-validation that will be described in greater detail in Chapters <a href="ChapModelSelection.html#ChapModelSelection">5</a> and <a href="ChapSimulation.html#ChapSimulation">7</a>.</p>
<p>Table <a href="ChapDataAnalytics.html#tab:Example231">2.1</a> shows that in each case the Two Level model has the lowest root mean square error and <span class="math inline">\(AIC\)</span>, indicating that it is the preferred model. The overfit model with six levels came in second and the underfit model, community rating, was a distant third. This analysis demonstrates techniques for selecting the appropriate model. Unlike analysis of real data, in this demonstration we enjoyed the additional luxury of knowing that we got things correct because we in fact generated the data - an approach that analysts often use to develop analytic procedures prior to utilizing the procedures on real data.</p>
<table>
<caption><span id="tab:Example231">Table 2.1: </span><strong>Under- and Over-Fitting of Models</strong></caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Community Rating</th>
<th align="right">Two Levels</th>
<th align="right">Six Levels</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Rmse - Fold 1</td>
<td align="right">1.318</td>
<td align="right">1.192</td>
<td align="right">1.239</td>
</tr>
<tr class="even">
<td align="left">Rmse - Fold 2</td>
<td align="right">1.034</td>
<td align="right">0.972</td>
<td align="right">1.023</td>
</tr>
<tr class="odd">
<td align="left">Rmse - Fold 3</td>
<td align="right">0.816</td>
<td align="right">0.660</td>
<td align="right">0.759</td>
</tr>
<tr class="even">
<td align="left">Rmse - Fold 4</td>
<td align="right">0.807</td>
<td align="right">0.796</td>
<td align="right">0.824</td>
</tr>
<tr class="odd">
<td align="left">Rmse - Fold 5</td>
<td align="right">0.886</td>
<td align="right">0.539</td>
<td align="right">0.671</td>
</tr>
<tr class="even">
<td align="left">Rmse - Average</td>
<td align="right">0.972</td>
<td align="right">0.832</td>
<td align="right">0.903</td>
</tr>
<tr class="odd">
<td align="left">AIC - Average</td>
<td align="right">227.171</td>
<td align="right">206.769</td>
<td align="right">211.333</td>
</tr>
</tbody>
</table>
<h5 style="text-align: center;">
<a id="displayCode.Example231.Hide" href="javascript:togglecode('toggleCode.Example231.Hide','displayCode.Example231.Hide');"><i><strong>Show Example 2.3.1 R Code</strong></i></a>
</h5>
<div id="toggleCode.Example231.Hide" style="display: none">
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="ChapDataAnalytics.html#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the Data</span></span>
<span id="cb30-2"><a href="ChapDataAnalytics.html#cb30-2" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">&lt;-</span> Metrics<span class="sc">::</span>rmse</span>
<span id="cb30-3"><a href="ChapDataAnalytics.html#cb30-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb30-4"><a href="ChapDataAnalytics.html#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb30-5"><a href="ChapDataAnalytics.html#cb30-5" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">6</span>, n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb30-6"><a href="ChapDataAnalytics.html#cb30-6" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>((u <span class="sc">==</span> <span class="dv">4</span>) <span class="sc">+</span> (u <span class="sc">==</span><span class="dv">5</span>)   )</span>
<span id="cb30-7"><a href="ChapDataAnalytics.html#cb30-7" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(u)</span>
<span id="cb30-8"><a href="ChapDataAnalytics.html#cb30-8" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">*</span> (x1<span class="sc">==</span><span class="dv">1</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb30-9"><a href="ChapDataAnalytics.html#cb30-9" aria-hidden="true" tabindex="-1"></a>xyData <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x1, x2, y)</span>
<span id="cb30-10"><a href="ChapDataAnalytics.html#cb30-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-11"><a href="ChapDataAnalytics.html#cb30-11" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(xyData)</span>
<span id="cb30-12"><a href="ChapDataAnalytics.html#cb30-12" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb30-13"><a href="ChapDataAnalytics.html#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="ChapDataAnalytics.html#cb30-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of folds</span></span>
<span id="cb30-15"><a href="ChapDataAnalytics.html#cb30-15" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb30-16"><a href="ChapDataAnalytics.html#cb30-16" aria-hidden="true" tabindex="-1"></a>splt <span class="ot">&lt;-</span> <span class="fu">split</span>(<span class="fu">sample</span>(n), <span class="dv">1</span><span class="sc">:</span>k) </span>
<span id="cb30-17"><a href="ChapDataAnalytics.html#cb30-17" aria-hidden="true" tabindex="-1"></a>Rmse.mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow=</span>k, <span class="at">ncol=</span><span class="dv">3</span>) <span class="ot">-&gt;</span> AIC.mat</span>
<span id="cb30-18"><a href="ChapDataAnalytics.html#cb30-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb30-19"><a href="ChapDataAnalytics.html#cb30-19" aria-hidden="true" tabindex="-1"></a>  test.id<span class="ot">&lt;-</span> splt[[i]] </span>
<span id="cb30-20"><a href="ChapDataAnalytics.html#cb30-20" aria-hidden="true" tabindex="-1"></a>  test   <span class="ot">&lt;-</span> xyData[test.id, ]</span>
<span id="cb30-21"><a href="ChapDataAnalytics.html#cb30-21" aria-hidden="true" tabindex="-1"></a>  train  <span class="ot">&lt;-</span> xyData[<span class="sc">-</span>test.id, ]</span>
<span id="cb30-22"><a href="ChapDataAnalytics.html#cb30-22" aria-hidden="true" tabindex="-1"></a>  model0 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> train)</span>
<span id="cb30-23"><a href="ChapDataAnalytics.html#cb30-23" aria-hidden="true" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1, <span class="at">data =</span> train)</span>
<span id="cb30-24"><a href="ChapDataAnalytics.html#cb30-24" aria-hidden="true" tabindex="-1"></a>  model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x2, <span class="at">data =</span> train)</span>
<span id="cb30-25"><a href="ChapDataAnalytics.html#cb30-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-26"><a href="ChapDataAnalytics.html#cb30-26" aria-hidden="true" tabindex="-1"></a>  Rmse.mat[i,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">rmse</span>(test<span class="sc">$</span>y, <span class="fu">predict</span>(model0, test))</span>
<span id="cb30-27"><a href="ChapDataAnalytics.html#cb30-27" aria-hidden="true" tabindex="-1"></a>  Rmse.mat[i,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">rmse</span>(test<span class="sc">$</span>y, <span class="fu">predict</span>(model1, test))</span>
<span id="cb30-28"><a href="ChapDataAnalytics.html#cb30-28" aria-hidden="true" tabindex="-1"></a>  Rmse.mat[i,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">rmse</span>(test<span class="sc">$</span>y, <span class="fu">predict</span>(model2, test))</span>
<span id="cb30-29"><a href="ChapDataAnalytics.html#cb30-29" aria-hidden="true" tabindex="-1"></a>  AIC.mat[i,<span class="dv">1</span>]  <span class="ot">&lt;-</span> <span class="fu">AIC</span>(model0)</span>
<span id="cb30-30"><a href="ChapDataAnalytics.html#cb30-30" aria-hidden="true" tabindex="-1"></a>  AIC.mat[i,<span class="dv">2</span>]  <span class="ot">&lt;-</span> <span class="fu">AIC</span>(model1)</span>
<span id="cb30-31"><a href="ChapDataAnalytics.html#cb30-31" aria-hidden="true" tabindex="-1"></a>  AIC.mat[i,<span class="dv">3</span>]  <span class="ot">&lt;-</span> <span class="fu">AIC</span>(model2)</span>
<span id="cb30-32"><a href="ChapDataAnalytics.html#cb30-32" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb30-33"><a href="ChapDataAnalytics.html#cb30-33" aria-hidden="true" tabindex="-1"></a>OutMat <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">round</span>(Rmse.mat, <span class="at">digits=</span><span class="dv">3</span>),</span>
<span id="cb30-34"><a href="ChapDataAnalytics.html#cb30-34" aria-hidden="true" tabindex="-1"></a>                <span class="fu">round</span>(<span class="fu">colMeans</span>(Rmse.mat), <span class="at">digits=</span><span class="dv">3</span>),</span>
<span id="cb30-35"><a href="ChapDataAnalytics.html#cb30-35" aria-hidden="true" tabindex="-1"></a>                <span class="fu">round</span>(<span class="fu">colMeans</span>(AIC.mat), <span class="at">digits=</span><span class="dv">3</span>) )</span>
<span id="cb30-36"><a href="ChapDataAnalytics.html#cb30-36" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(OutMat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Community Rating&quot;</span>, <span class="st">&quot;Two Levels&quot;</span>, <span class="st">&quot;Six Levels&quot;</span>)</span>
<span id="cb30-37"><a href="ChapDataAnalytics.html#cb30-37" aria-hidden="true" tabindex="-1"></a><span class="fu">row.names</span>(OutMat) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Rmse - Fold 1&quot;</span>, <span class="st">&quot;Rmse - Fold 2&quot;</span>, <span class="st">&quot;Rmse - Fold 3&quot;</span>, </span>
<span id="cb30-38"><a href="ChapDataAnalytics.html#cb30-38" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;Rmse - Fold 4&quot;</span>, <span class="st">&quot;Rmse - Fold 5&quot;</span>, <span class="st">&quot;Rmse - Average&quot;</span>, </span>
<span id="cb30-39"><a href="ChapDataAnalytics.html#cb30-39" aria-hidden="true" tabindex="-1"></a>                       <span class="st">&quot;AIC - Average&quot;</span>)</span>
<span id="cb30-40"><a href="ChapDataAnalytics.html#cb30-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-41"><a href="ChapDataAnalytics.html#cb30-41" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(OutMat, <span class="at">caption =</span> <span class="st">&quot;**Under- and Over-Fitting of Models**&quot;</span>, <span class="at">booktabs =</span> T) </span></code></pre></div>
<hr />
</div>
<div id="surveyElement2Data3">

</div>
<div id="surveyResult2Data3">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz2Data3.1" href="javascript:toggleQuiz
('display.Quiz2Data3.2','display.Quiz2Data3.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz2Data3.2" style="display: none">
<p id="Quiz2Data3Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz2Data3.js">
</script>
</div>
</div>
<div id="S:ManyVarAnalytics" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Analytics with Many Variables<a href="ChapDataAnalytics.html#S:ManyVarAnalytics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In this section, you learn how to describe analytics based on many variables in terms of</p>
<ul>
<li>supervised and unsupervised learning,</li>
<li>types of algorithmic models, including linear, ridge, and lasso regressions, as well as regularization, and</li>
<li>types of data models, including Poisson regressions and generalized linear models.</li>
</ul>
<hr />
<p>Just as with a single variable in Section <a href="ChapDataAnalytics.html#S:SingleVarAnalytics">2.3</a>, with many variables analysts follow the same structure of identifying variables, exploring data, constructing and selecting models. However, the potential applications becomes much richer when considering many variables. With many potential applications, it is natural that techniques for data analysis have developed in different but overlapping fields; these fields include statistics, machine learning, pattern recognition, and data mining.</p>
<ul>
<li>Statistics is a field that addresses reliable ways of gathering data and making inferences.</li>
<li>The term <a href="#" class="tooltip" style="color:green"><em>machine learning</em><span style="font-size:8pt">Study of algorithms and statistical models that perform a specific task without using explicit instructions, relying on patterns and inference</span></a> was coined by Samuel in 1959 <span class="citation">(<a href="#ref-samuel1959ml" role="doc-biblioref">Samuel 1959</a>)</span>. Originally, machine learning referred to the field of study where computers have the ability to learn without being explicitly programmed. Nowadays, machine learning has evolved to a broad field of study where computational methods use experience (i.e., the past information available for analysis) to improve performance or to make accurate predictions.</li>
<li>Originating in engineering, <a href="#" class="tooltip" style="color:green"><em>pattern recognition</em><span style="font-size:8pt">Automated recognition of patterns and regularities in data</span></a> is a field that is closely related to machine learning, which grew out of computer science. In fact, pattern recognition and machine learning can be considered to be two facets of the same field <span class="citation">(<a href="#ref-bishop2007" role="doc-biblioref">C. M. Bishop 2007</a>)</span>.</li>
<li><a href="#" class="tooltip" style="color:green"><em>Data mining</em><span style="font-size:8pt">Process of collecting, cleaning, processing, analyzing, and discovering patterns and useful insights from large data sets</span></a> is a field that concerns collecting, cleaning, processing, analyzing, and gaining useful insights from data <span class="citation">(<a href="#ref-aggarwal2015" role="doc-biblioref">Aggarwal 2015</a>)</span>.</li>
</ul>
<div id="supervised-and-unsupervised-learning" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Supervised and Unsupervised Learning<a href="ChapDataAnalytics.html#supervised-and-unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With multiple variables, the essential tasks of identifying variable types, exploring data, and selecting models are similar in principle to that described for single variables in Section <a href="ChapDataAnalytics.html#S:SingleVarAnalytics">2.3</a>. When exploring data in multiple dimensions, additional considerations such as clustering like observations and reducing the dimension arise. As these considerations will not arise in the applications in this book, we provide only a brief introduction in Technical Supplement Section <a href="ChapDataAnalytics.html#S:MultiEDA">2.6.1</a>.</p>
<p>The construction of models differs dramatically when comparing single to multiple variable modeling. With many variables, we have the opportunity to think about some of them as inputs and others outputs of a system. These are known as <a href="#" class="tooltip" style="color:green"><em>supervised learning methods</em><span style="font-size:8pt">Model that predicts a response target variable using explanatory predictors as input</span></a> or as <a href="#" class="tooltip" style="color:green"><em>regression methods</em><span style="font-size:8pt">Classical supervised learning method where the response may be continuous, binary, or a mixture of discrete and continuous</span></a>. <a href="#tab:2.3">Table 2.3</a> gives a list of common names for different types of variables <span class="citation">(<a href="#ref-frees2009regression" role="doc-biblioref">Frees 2009</a>)</span>. When the target variable is a categorical variable, supervised learning methods are called <a href="#" class="tooltip" style="color:green"><em>classification methods</em><span style="font-size:8pt">Supervised learning method where the response is a categorical variable</span></a>.</p>
<p><a id=tab:2.3></a></p>
<p>Table 2.3. <strong>Common Names of Different Variables</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{ll}
\hline
\textbf{Target Variable}  &amp;  \textbf{Explanatory Variable}\\\hline
\text{Dependent variable} &amp; \text{Independent variable}\\
\text{Response} &amp; \text{Treatment} \\
\text{Output} &amp; \text{Input} \\
\text{Endogenous variable} &amp; \text{Exogenous variable} \\
\text{Predicted variable} &amp; \text{Predictor variable} \\
\text{Regressand} &amp; \text{Regressor} \\
\hline
\end{array}
}
\]</span></p>
<p>Methods for data analysis can be divided into two types <span class="citation">(<a href="#ref-abbott2014" role="doc-biblioref">Abbott 2014</a>; <a href="#ref-james2013introduction" role="doc-biblioref">James et al. 2013</a>)</span>: supervised learning methods and unsupervised learning methods. <a href="#" class="tooltip" style="color:green"><em>Unsupervised learning methods</em><span style="font-size:8pt">Models that work with explanatory variables only to describe patterns or groupings</span></a> work where are data are treated the same and there is no artificial divide between inputs and outputs. As a result, unsupervised learning methods are particularly useful at the exploratory stage of an analysis.</p>
</div>
<div id="algorithmic-modeling" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Algorithmic Modeling<a href="ChapDataAnalytics.html#algorithmic-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Early data analysis traced the movements of orbits of bodies about the sun using astronomical observations from at least the 1750s by Boscovich, continued in the early 1800s by Legendre and Gauss (the latter two in connection with their development of least squares). This work was done using algorithmic <em>fitting</em> approaches (such as least squares) without regard to distributions of random variables.</p>
<p>The idea underpinning algorithmic fitting is easy to interpret. One variable, <span class="math inline">\(Y\)</span>, is determined to be a target variable. Other variables, <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>, are used to understand or explain the target <span class="math inline">\(Y\)</span>. The goal is to determine an appropriate function <span class="math inline">\(f(\cdot)\)</span> so that <span class="math inline">\(f(X_1, X_2, \ldots, X_k)\)</span> is a useful predictor of <span class="math inline">\(Y\)</span>.</p>
<p><strong>Linear Regression.</strong> To illustrate, consider the classic linear regression context. In this case, we have <span class="math inline">\(n\)</span> observations of a target and explanatory variables, with the <span class="math inline">\(i\)</span>th observation denoted as <span class="math inline">\((x_{i1}, \ldots, x_{ik}, y_i) =\)</span> <span class="math inline">\(({\bf x}_i, y_i)\)</span>. One would like to determine a single function <span class="math inline">\(f\)</span> so that <span class="math inline">\(f({\bf x}_i)\)</span> is a reasonable approximation for <span class="math inline">\(y_i\)</span>, for each <span class="math inline">\(i\)</span>. For the linear regression, one restricts considerations to functions of the form</p>
<p><span class="math display">\[
f(x_{i1}, \ldots, x_{ik}) = \beta_1 x_{i1} + \cdots + \beta_k x_{ik} = {\bf x}_i^{\prime} \boldsymbol \beta.
\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol \beta = (\beta_1, \ldots, \beta_k)&#39;\)</span> is a vector of <em>regression coefficients</em>. This function is <em>linear</em> in the explanatory variables that gives rise to the name <a href="#" class="tooltip" style="color:green"><em>linear regression</em><span style="font-size:8pt">Supervised model that uses a linear function to approximate the relationship between the target and explanatory variables</span></a>.</p>
<p>The <em>ordinary least squares (OLS)</em> estimates are the solution of the following minimization problem,</p>
<p><span class="math display" id="eq:OLSSolution">\[
\begin{array}{cc}
{\small \text{minimize}}_{\boldsymbol \beta}  &amp; \frac{1}{n} \sum_{i=1}^n (y_i - {\bf x}_i^{\prime} \boldsymbol \beta)^2 .\\
\end{array}
\tag{2.1}
\]</span></p>
<p>The <em>OLS</em> estimates are historically prominent in part because of their ease of computation and interpretation. Naturally, a squared difference such as <span class="math inline">\((y_i - {\bf x}_i^{\prime} \boldsymbol \beta)^2\)</span> is not the only way to measure the deviation between a target <span class="math inline">\(y_i\)</span> and an estimate <span class="math inline">\({\bf x}_i^{\prime} \boldsymbol \beta\)</span>. In generally, analysts use the term <em>loss function</em> <span class="math inline">\(l(y_i, {\bf x}_i^{\prime} \boldsymbol \beta)\)</span> to measure this deviation; as an alternative, it is not uncommon to use an absolute deviation.</p>
<p><strong>Algorithmic Modeling Culture.</strong> As introduced in Section <a href="ChapDataAnalytics.html#S:DataAlgorithm">2.1.2</a>, a culture has developed across widespread communities that emphasize algorithmic fitting particularly in complex problems such as voice, image, and handwriting recognition. Algorithmic methods are especially useful when the goal is prediction, as noted in Section <a href="ChapDataAnalytics.html#S:expred">2.3.3.2</a>. Many of these algorithms take an approach similar to linear regression. As examples, other widely used algorithmic fitting methods include ridge and lasso regression, as well as regularization methods.</p>
<p><strong>Ridge Regression.</strong> One limitation of <em>OLS</em> is that it tends to overfit, particularly when the number of regression coefficients <span class="math inline">\(k\)</span> becomes large. In fact, with <span class="math inline">\(k=n\)</span> one gets an exact match between the targets <span class="math inline">\(y_i\)</span> and the predictor function. A modification introduced in 1970 by <span class="citation"><a href="#ref-hoerl1970ridge" role="doc-biblioref">Hoerl and Kennard</a> (<a href="#ref-hoerl1970ridge" role="doc-biblioref">1970</a>)</span> is known as <em>ridge regression</em> where one determines regression coefficients <span class="math inline">\(\boldsymbol \beta\)</span> as in equation <a href="ChapDataAnalytics.html#eq:OLSSolution">(2.1)</a> although subject to the constraint that <span class="math inline">\(\sum_{j=1}^p |\beta_j |^2 \le c_{ridge}\)</span>, where <span class="math inline">\(c_{ridge}\)</span> is an appropriately chosen constant. Naturally, if <span class="math inline">\(c_{ridge}\)</span> is very large, then the constraint has no effect and the ridge estimates equal the <em>OLS</em> solution. However, as <span class="math inline">\(c_{ridge}\)</span> becomes small, it reduces the size of the regression coefficients. In this sense, the ridge regression estimator is said to be shrunk towards zero.</p>
<p>In the actuarial applications, we might have a portfolio of only a few thousand risks that we wish to model. With ridge regression, we can utilize millions of variables as potential inputs to develop predictive models.</p>
<p><strong>Lasso Regression.</strong> Similar to ridge regression, one can determine regression coefficients <span class="math inline">\(\boldsymbol \beta\)</span> as in equation <a href="ChapDataAnalytics.html#eq:OLSSolution">(2.1)</a> although subject to the constraint that <span class="math inline">\(\sum_{j=1}^p |\beta_j | \le c_{lasso}\)</span>, where <span class="math inline">\(c_{lasso}\)</span> is an appropriately chosen constant. This procedure is known as <em>lasso regression.</em> Here, one uses absolute values in the constraint function (although still squared errors for the loss function).</p>
<p>The lasso overcomes an important limitation of ridge regression. With ridge regression, we might reduce the size of the constant <span class="math inline">\(c_{ridge}\)</span> that forces the regression coefficient to become small but does not ensure that they become zero. In contrast, the lasso ensures that trivial regression coefficients become zero. In the linear regression approximation, a zero regression coefficient means that the variable drops from the function approximation, thus reducing model complexity.</p>
<p><strong>Regularization</strong>. Both the ridge and lasso regressions are constrained minimization problems. It is not too hard to show that they can be written as</p>
<p><span class="math display">\[
{\small \text{minimize}}_{\boldsymbol \beta} \left( 
\frac{1}{n} \sum_{i=1}^n (y_i - {\bf x}_i^{\prime} \boldsymbol \beta)^2 +
LM \sum_{j=1}^p |\beta_j |^s \right) ,
\]</span></p>
<p>where <span class="math inline">\(s=2\)</span> is for ridge regression and <span class="math inline">\(s=1\)</span> is for lasso regression. We can interpret the first part inside the minimization operation as the goodness of fit and the second part as a penalty for size of the regression coefficients. As we have discussed, reducing the coefficients can mean reducing modeling complexity. In this sense, this expression demonstrates a balance between goodness of fit and model complexity, controlled by the parameter <span class="math inline">\(LM\)</span> (In this case, because it is a constrained optimization problem, the parameter is a Lagrange multiplier.). The choice of <span class="math inline">\(LM=0\)</span> reduces to the <em>OLS</em> estimator that focuses on goodness of fit. As <span class="math inline">\(LM\)</span> becomes large, the focus moves away from the data (and hence goodness of fit). This is an example of a <em>regularization</em> method in data analytics, where one expresses a prior belief concerning the smoothness of functions used for our predictions.</p>
</div>
<div id="data-modeling" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Data Modeling<a href="ChapDataAnalytics.html#data-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One way to motivate an algorithmic development is through the use of a data model introduced in Section <a href="ChapDataAnalytics.html#S:DataAlgorithm">2.1.2</a>. Here, we can also think of this as a probability or likelihood based model, in that our main goal is to understand the target (<span class="math inline">\(Y\)</span>) distribution, typically in terms of the explanatory variables. Thus, data models are particularly useful the goal of explanation previously discussed in Section <a href="ChapDataAnalytics.html#S:expred">2.3.3.2</a>.</p>
<p>Data models were initially developed in the early twentieth century through the work of R.A. Fisher and E.P. George Box (among many, many others) whose work focused on data as the result of experiments with a small number of outcomes and even fewer explanatory (control) variables.</p>
<p><strong>Linear Regression.</strong> The (algorithmic) linear regression with <em>OLS</em> estimates can be motivated using a probabilistic framework, as follows. We can think of the target variable <span class="math inline">\(y_i\)</span> as having a normal distribution with unknown variance and a mean equal to <span class="math inline">\({\bf x}_i^{\prime} \boldsymbol \beta\)</span>, a linear combination of the explanatory variables. Assuming independence among observations, it can be shown that the maximum likelihood estimates are equivalent to the <em>OLS</em> estimates determine in equation <a href="ChapDataAnalytics.html#eq:OLSSolution">(2.1)</a>.</p>
<p>Maximum likelihood estimation is used extensively in this text, you can get a quick overview in Chapter 18 Appendix C. For additional background on <em>OLS</em> and maximum likelihood in the linear regression case see, for example, <span class="citation"><a href="#ref-frees2009regression" role="doc-biblioref">Frees</a> (<a href="#ref-frees2009regression" role="doc-biblioref">2009</a>)</span> for more details.</p>
<p><strong>Poisson Regression.</strong> In the case where the target variable <span class="math inline">\(Y\)</span> represents a count (such as the number of insurance losses), then it is common to use a Poisson distribution to represent the likelihood of potential outcomes. The Poisson has only one parameter, the mean, and if explanatory variables are available, then one can take the mean to equal <span class="math inline">\(\exp\left({\bf x}_i^{\prime} \boldsymbol \beta\right)\)</span>. One motivation for using the exponential (<span class="math inline">\(\exp(\cdot)\)</span>) function is that it ensures that estimated means are non-negative (a necessary condition for the Poisson distribution). When maximum likelihood is used to estimate the regression coefficients, then this is known as <em>Poisson regression.</em></p>
<p><strong>Generalized Linear Model.</strong> The <a href="#" class="tooltip" style="color:green"><em>generalized linear model</em><span style="font-size:8pt">Supervised model that generalizes linear regression by allowing the linear component to be related to the response variable via a link function and by allowing the variance of each measurement to be a function of its predicted value</span></a> (<em>GLM</em>) consists of a wide family of regression models that include linear and Poisson regression models as special cases. In a <em>GLM</em>, the mean of the target variable is assumed to be a function of a linear combinations of the explanatory variables. As with a Poisson regression, the mean can vary by observations by allowing some parameters to change yet the regression parameters <span class="math inline">\(\boldsymbol{\beta}\)</span> are assumed to be constant.</p>
<p>In a <em>GLM</em>, the target variable is assumed to follow a distribution from the <em>linear exponential family</em>, this is a collection of distribution that includes the normal, Poisson, Bernoulli, Weibull, and others. Thus, a <em>GLM</em> is one way of developing a broader class that includes linear and Poisson regression. Using a Bernoulli distribution, it also includes zero-one target variables resulting in what is known as <em>logistic regression.</em> Thus, the <em>GLM</em> provides a unifying framework to handle different types of target variables, including discrete and continuous variables. Extensions to other distributions that are not part of linear exponential family, such as a Pareto distribution, are also possible. But, <em>GLMs</em> have historically been found useful because their form permits efficient calculation of estimators (through what is known as <em>iterative reweighted least squares</em>). For more information about <em>GLM</em>s, readers are referred to <span class="citation"><a href="#ref-Dejong08" role="doc-biblioref">De Jong and Heller</a> (<a href="#ref-Dejong08" role="doc-biblioref">2008</a>)</span> and <span class="citation"><a href="#ref-frees2009regression" role="doc-biblioref">Frees</a> (<a href="#ref-frees2009regression" role="doc-biblioref">2009</a>)</span>.</p>
<div id="surveyElement2Data4">

</div>
<div id="surveyResult2Data4">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz2Data4.1" href="javascript:toggleQuiz
('display.Quiz2Data4.2','display.Quiz2Data4.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz2Data4.2" style="display: none">
<p id="Quiz2Data4Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz2Data4.js">
</script>
</div>
</div>
<div id="S:DataLearn" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Data<a href="ChapDataAnalytics.html#S:DataLearn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<hr />
<p>In this section, you learn how to describe data considerations in terms of</p>
<ul>
<li>data types,</li>
<li>data structure and storage,</li>
<li>data cleaning,</li>
<li>big data issues, and</li>
<li>ethical issues.</li>
</ul>
<hr />
<p>Data constitute the backbone of data analytics. Without data containing useful information, no level of sophisticated analytic techniques can provide useful guidance for making good decisions.</p>
<p>The prior sections of this chapter provide the foundations of data considerations needed for the rest of this book. However, readers who wish to specialize in data analytics, the following subsections provide a useful starting point for further study.</p>
<div id="data-types" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Data Types<a href="ChapDataAnalytics.html#data-types" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In terms of how data are collected, data can be divided into two types <span class="citation">(<a href="#ref-hox2005data" role="doc-biblioref">Hox and Boeije 2005</a>)</span>: primary and secondary data. Primary data are the original data that are collected for a specific research problem. Secondary data are data originally collected for a different purpose and reused for another research problem. A major advantage of using primary data is that the theoretical constructs, the research design, and the data collection strategy can be tailored to the underlying research question to ensure that data collected help to solve the problem. A disadvantage of using primary data is that data collection can be costly and time consuming. Using secondary data has the advantage of lower cost and faster access to relevant information. However, using secondary data may not be optimal for the research question under consideration.</p>
<p>In terms of the degree of organization, data can be also divided into two types structured data and unstructured data. <a href="#" class="tooltip" style="color:green"><em>Structured data</em><span style="font-size:8pt">Data that can be organized into a repository format, typically a database</span></a> have a predictable and regularly occurring format. In contrast, <a href="#" class="tooltip" style="color:green"><em>unstructured data</em><span style="font-size:8pt">Data that is not in a predefined format, most notably text, audio visual</span></a> lack any regularly occurring format and have no structure that is recognizable to a computer. Structured data consist of records, attributes, keys, and indices and are typically managed by a database management system such as IBM DB2, Oracle, MySQL, and Microsoft SQL Server. As a result, most units of structured data can be located quickly and easily. Unstructured data have many different forms and variations. One common form of unstructured data is text. Accessing unstructured data can be awkward. To find a given unit of data in a long text, for example, a sequential search is usually performed.</p>
</div>
<div id="data-structures-and-storage" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Data Structures and Storage<a href="ChapDataAnalytics.html#data-structures-and-storage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned in the previous subsection, there are structured data as well as unstructured data. Structured data are highly organized data and usually have the following tabular format:</p>
<p><span class="math display">\[
\begin{matrix}
\begin{array}{lllll} \hline
 &amp; V_1 &amp; V_2 &amp; \cdots &amp; V_d \  
\\\hline
\textbf{x}_1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} \\
\textbf{x}_2 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} \\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
\textbf{x}_n &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nd} \\
\hline
\end{array}
\end{matrix}
\]</span></p>
<p>In other words, structured data can be organized into a table consisting of rows and columns. Typically, each row represents a record and each column represents an attribute. A table can be decomposed into several tables that can be stored in a relational database such as the Microsoft SQL Server. The SQL (Structured Query Language) can be used to access and modify the data easily and efficiently.</p>
<p>Unstructured data do not follow a regular format. Examples of unstructured data include documents, videos, and audio files. Most of the data we encounter are unstructured data. In fact, the term big data was coined to reflect this fact. Traditional relational databases cannot meet the challenges on the varieties and scales brought by massive unstructured data nowadays. NoSQL databases have been used to store massive unstructured data.</p>
<p>There are three main NoSQL databases <span class="citation">(<a href="#ref-chen2014b" role="doc-biblioref">Chen et al. 2014</a>)</span>: key-value databases, column-oriented databases, and document-oriented databases. <a href="#" class="tooltip" style="color:green"><em>Key-value databases</em><span style="font-size:8pt">Data storage method that stores amd finds records using a unique key hash</span></a> use a simple data model and store data according to key values. Modern key-value databases have higher expandability and smaller query response times than relational databases. Examples of key-value databases include Dynamo used by Amazon and Voldemort used by LinkedIn. <a href="#" class="tooltip" style="color:green"><em>Column-oriented databases</em><span style="font-size:8pt">Data storage method that stores records by column instead of by row</span></a> store and process data according to columns rather than rows. The columns and rows are segmented in multiple nodes to achieve expandability. Examples of column-oriented databases include BigTable developed by Google and Cassandra developed by FaceBook. <a href="#" class="tooltip" style="color:green"><em>Document databases</em><span style="font-size:8pt">Data storage method that uses the document metadata for search and retrieval, also known as semi-structured data</span></a> are designed to support more complex data forms than those stored in key-value databases. Examples of document databases include MongoDB, SimpleDB, and CouchDB. MongoDB is an open-source document-oriented database that stores documents as binary objects. SimpleDB is a distributed NoSQL database used by Amazon. CouchDB is another open-source document-oriented database.</p>
</div>
<div id="data-cleaning" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Data Cleaning<a href="ChapDataAnalytics.html#data-cleaning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Raw data usually need to be cleaned before useful analysis can be conducted. In particular, the following areas need attention when preparing data for analysis <span class="citation">(<a href="#ref-janert2010" role="doc-biblioref">Janert 2010</a>)</span>:</p>
<ul>
<li><strong>Missing values.</strong> It is common to have missing values in raw data. Depending on the situation, we can discard the record, discard the variable, or impute the missing values.</li>
<li><strong>Outliers.</strong> Raw data may contain unusual data points such as outliers. We need to handle outliers carefully. We cannot just remove outliers without knowing the reason for their existence. Although sometimes outliers can be simple mistakes such as those caused by clerical errors, sometimes their unusual behavior can point to precisely the type of effect that we are looking for.</li>
<li><strong>Junk.</strong> Raw data may contain garbage, or junk, such as nonprintable characters. When it happens, junk is typically rare and not easily noticed. However, junk can cause serious problems in downstream applications.</li>
<li><strong>Format.</strong> Raw data may be formatted in a way that is inconvenient for subsequent analysis. For example, components of a record may be split into multiple lines in a text file. In such cases, lines corresponding to a single record should be merged before loading to a data analysis software such as <code>R</code>.</li>
<li><strong>Duplicate records.</strong> Raw data may contain duplicate records. Duplicate records should be recognized and removed. This task may not be trivial depending on what you consider duplicate.</li>
<li><strong>Merging datasets.</strong> Raw data may come from different sources. In such cases, we need to merge data from different sources to ensure compatibility.</li>
</ul>
<p>For more information about how to handle data in <code>R</code>, readers are referred to <span class="citation"><a href="#ref-forte2015" role="doc-biblioref">Forte</a> (<a href="#ref-forte2015" role="doc-biblioref">2015</a>)</span> and <span class="citation"><a href="#ref-buttrey2017" role="doc-biblioref">Buttrey and Whitaker</a> (<a href="#ref-buttrey2017" role="doc-biblioref">2017</a>)</span>.</p>
</div>
<div id="Sec:BigDataAnalysis" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Big Data Analysis<a href="ChapDataAnalytics.html#Sec:BigDataAnalysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Unlike traditional data analysis, big data analysis employs additional methods and tools that can extract information rapidly from massive data. In particular, big data analysis uses the following processing methods <span class="citation">(<a href="#ref-chen2014b" role="doc-biblioref">Chen et al. 2014</a>)</span>:</p>
<ul>
<li>A <strong>bloom filter</strong> is a space-efficient probabilistic data structure that is used to determine whether an element belongs to a set. It has the advantages of high space efficiency and high query speed. A drawback of using bloom filter is that there is a certain nonrecognition rate.</li>
<li><strong>Hashing</strong> is a method that transforms data into fixed-length numerical values through a hash function. It has the advantages of rapid reading and writing. However, sound hash functions are difficult to find.</li>
<li><strong>Indexing</strong> refers to a process of partitioning data in order to speed up reading. Hashing is a special case of indexing.</li>
<li>A <strong>trie,</strong> also called digital tree, is a method to improve query efficiency by using common prefixes of character strings to reduce comparisons among character strings.</li>
<li><strong>Parallel computing</strong> uses multiple computing resources to complete a computation task. Parallel computing tools include Message Passing Interface (MPI), MapReduce, and Dryad.</li>
</ul>
<p>Big data analysis can be conducted in the following levels <span class="citation">(<a href="#ref-chen2014b" role="doc-biblioref">Chen et al. 2014</a>)</span>: memory-level, business intelligence (BI) level, and massive level. Memory-level analysis is conducted when data can be loaded to the memory of a cluster of computers. Current hardware can handle hundreds of gigabytes (GB) of data in memory. BI level analysis can be conducted when data surpass the memory level. It is common for BI level analysis products to support data over terabytes (TB). Massive level analysis is conducted when data surpass the capabilities of products for BI level analysis. Usually Hadoop and MapReduce are used in massive level analysis.</p>
</div>
<div id="ethical-issues" class="section level3 hasAnchor" number="2.5.5">
<h3><span class="header-section-number">2.5.5</span> Ethical Issues<a href="ChapDataAnalytics.html#ethical-issues" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Analysts may face ethical issues and dilemmas during the data analysis process. In some fields, ethical issues and dilemmas include participant consent, benefits, risk, confidentiality, and <a href="#" class="tooltip" style="color:green"><em>data ownership</em><span style="font-size:8pt">Governance process that details legal ownership of enterprise-wide data and outlines who has ability to create, edit, modify, share and restrict access to the data</span></a> <span class="citation">(<a href="#ref-miles2014" role="doc-biblioref">Miles, Hberman, and Sdana 2014</a>)</span>. For example, regarding privacy and confidentiality, one might confront the following questions: How do we make sure that the information is kept confidentially? How do we verify where are raw data and analysis results stored? How will we have access to them? These questions should be addressed and documented in explicit confidentiality agreements.</p>
<p>Within the insurance sector, discrimination, privacy, and confidentiality are major concerns. Discrimination in insurance is particularly difficult because the entire industry is based on discriminating, or classifying, insureds into homogeneous categories for the purposes of risk sharing. Many variables that insurers use are seemingly innocuous (e.g., blindness for auto insurance), yet others can be viewed as wrong (e.g., religious affiliation), unfair (e.g., onset of cancer for health insurance), sensitive (e.g., marital status), or mysterious (e.g., Artificial Intelligence produced). Regulators and policymakers decide whether it is not permitted to use a variable for classification. In part because they depend on differing attitudes, perspectives can vary dramatically across jurisdictions. For example, gender-based pricing of auto insurance is permitted in all but a handful of U.S. states (the exceptions being Hawaii, Massachusetts, Montana, North Carolina, Pennsylvania, and, as of 2019, California) yet not permitted within the European Union. Moreover, for personal lines such as auto and homeowners, availability of big data may also lead to issues regarding <em>proxy discrimination</em>. Proxy discrimination occurs when a surrogate, or proxy, is used in place of a prohibited trait such as race or gender, see, for example, <span class="citation"><a href="#ref-frees2021discriminating" role="doc-biblioref">Frees and Huang</a> (<a href="#ref-frees2021discriminating" role="doc-biblioref">2021</a>)</span>.</p>
<div id="surveyElement2Data5">

</div>
<div id="surveyResult2Data5">

</div>
<h5 style="text-align: center;">
<a id="display.Quiz2Data5.1" href="javascript:toggleQuiz
('display.Quiz2Data5.2','display.Quiz2Data5.1');"><i><strong>Show Quiz Solution</strong></i></a>
</h5>
<div id="display.Quiz2Data5.2" style="display: none">
<p id="Quiz2Data5Soln">
</p>
<hr />
</div>
<script type="text/javascript" src="./Quizzes/QuizJavascript/Quiz2Data5.js">
</script>
</div>
</div>
<div id="DS:further-reading-and-resources" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Further Resources and Contributors<a href="ChapDataAnalytics.html#DS:further-reading-and-resources" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="contributors-1" class="section level4 unnumbered hasAnchor">
<h4>Contributors<a href="ChapDataAnalytics.html#contributors-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><strong>Guojun Gan</strong>, University of Connecticut, was the principal author of the initial version of this chapter.
<ul>
<li>Chapter reviewers include: Runhuan Feng, Himchan Jeong, Lei Hua, Min Ji, and Toby White.</li>
</ul></li>
<li><strong>Hirokazu (Iwahiro) Iwasawa</strong> and <strong>Edward (Jed) Frees</strong>, University of Wisconsin-Madison and Australian National University, are the authors of the second edition of this chapter. Email: <a href="mailto:iwahiro@bb.mbn.or.jp" class="email">iwahiro@bb.mbn.or.jp</a> and/or <a href="mailto:jfrees@bus.wisc.edu" class="email">jfrees@bus.wisc.edu</a> for chapter comments and suggested improvements.</li>
</ul>
</div>
<div id="further-readings-and-references" class="section level4 unnumbered hasAnchor">
<h4>Further Readings and References<a href="ChapDataAnalytics.html#further-readings-and-references" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li><span class="citation"><a href="#ref-stigler1986history" role="doc-biblioref">Stigler</a> (<a href="#ref-stigler1986history" role="doc-biblioref">1986</a>)</span> gives a definitive account of the early contributions of Boscovich, Legendre and Gauss.</li>
<li><span class="citation"><a href="#ref-breiman2001modeling" role="doc-biblioref">Breiman</a> (<a href="#ref-breiman2001modeling" role="doc-biblioref">2001</a>)</span> compares the data modeling and the algorithmic modeling cultures.</li>
<li><span class="citation"><a href="#ref-good1983data" role="doc-biblioref">Good</a> (<a href="#ref-good1983data" role="doc-biblioref">1983</a>)</span> compares the two phases of data analysis, exploratory data analysis (EDA) and confirmatory data analysis (CDA)</li>
<li>See, for example, <span class="citation"><a href="#ref-breiman2001modeling" role="doc-biblioref">Breiman</a> (<a href="#ref-breiman2001modeling" role="doc-biblioref">2001</a>)</span> and <span class="citation"><a href="#ref-shmueli2010model" role="doc-biblioref">Shmueli</a> (<a href="#ref-shmueli2010model" role="doc-biblioref">2010</a>)</span>, for more discussions of the two goals in data analysis: explanation and prediction.</li>
<li>Comparisons of structured data and unstructured data can be found in <span class="citation"><a href="#ref-inmon2014" role="doc-biblioref">Inmon and Linstedt</a> (<a href="#ref-inmon2014" role="doc-biblioref">2014</a>)</span>, <span class="citation"><a href="#ref-leary2013bigdata" role="doc-biblioref">OLeary</a> (<a href="#ref-leary2013bigdata" role="doc-biblioref">2013</a>)</span> ,<span class="citation"><a href="#ref-hashem2015bigdata" role="doc-biblioref">Hashem et al.</a> (<a href="#ref-hashem2015bigdata" role="doc-biblioref">2015</a>)</span>, <span class="citation"><a href="#ref-abdullah2013data" role="doc-biblioref">Abdullah and Ahmad</a> (<a href="#ref-abdullah2013data" role="doc-biblioref">2013</a>)</span>, and <span class="citation"><a href="#ref-pries2015" role="doc-biblioref">Pries and Dunnigan</a> (<a href="#ref-pries2015" role="doc-biblioref">2015</a>)</span>, among others.</li>
</ul>
</div>
<div id="S:MultiEDA" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Technical Supplement: Multivariate Exploratory Analysis<a href="ChapDataAnalytics.html#S:MultiEDA" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="principal-component-analysis" class="section level4 hasAnchor" number="2.6.1.1">
<h4><span class="header-section-number">2.6.1.1</span> Principal Component Analysis<a href="ChapDataAnalytics.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="#" class="tooltip" style="color:green"><em>Principal component analysis</em><span style="font-size:8pt">Dimension reduction technique that uses orthogonal transformations to convert a set of possibly correlated variables into a set of linearly uncorrelated variables</span></a> (PCA) is a statistical procedure that transforms a dataset described by possibly correlated variables into a dataset described by linearly uncorrelated variables, which are called principal components and are ordered according to their variances. PCA is a technique for dimension reduction. If the original variables are highly correlated, then the first few principal components can account for most of the variation of the original data.</p>
<p>The principal components of the variables are related to the eigenvalues and eigenvectors of the covariance matrix of the variables. For <span class="math inline">\(i=1,2,\ldots,d\)</span>, let <span class="math inline">\((\lambda_i, \textbf{e}_i)\)</span> be the <span class="math inline">\(i\)</span>th eigenvalue-eigenvector pair of the covariance matrix <span class="math inline">\({\Sigma}\)</span> of <span class="math inline">\(d\)</span> variables <span class="math inline">\(X_1,X_2,\ldots,X_d\)</span> such that <span class="math inline">\(\lambda_1\ge \lambda_2\ge \ldots\ge \lambda_d\ge 0\)</span> and the eigenvectors are normalized. Then the <span class="math inline">\(i\)</span>th principal component is given by</p>
<p><span class="math display">\[
Z_{i} = \textbf{e}_i&#39; \textbf{X} =\sum_{j=1}^d e_{ij} X_j,
\]</span></p>
<p>where <span class="math inline">\(\textbf{X}=(X_1,X_2,\ldots,X_d)&#39;\)</span>. It can be shown that <span class="math inline">\(\mathrm{Var~}{(Z_i)} = \lambda_i\)</span>. As a result, the proportion of variance explained by the <span class="math inline">\(i\)</span>th principal component is calculated as</p>
<p><span class="math display">\[
\frac{\mathrm{Var~}{(Z_i)}}{ \sum_{j=1}^{d} \mathrm{Var~}{(Z_j)}} = \frac{\lambda_i}{\lambda_1+\lambda_2+\cdots+\lambda_d}.
\]</span></p>
<p>For more information about PCA, readers are referred to <span class="citation"><a href="#ref-mirkin2011" role="doc-biblioref">Mirkin</a> (<a href="#ref-mirkin2011" role="doc-biblioref">2011</a>)</span>.</p>
</div>
<div id="cluster-analysis" class="section level4 hasAnchor" number="2.6.1.2">
<h4><span class="header-section-number">2.6.1.2</span> Cluster Analysis<a href="ChapDataAnalytics.html#cluster-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="#" class="tooltip" style="color:green"><em>Cluster analysis</em><span style="font-size:8pt">Unsupervised learning method that aims to splot data into homogenous groups using a similarity measure</span></a> (aka data clustering) refers to the process of dividing a dataset into homogeneous groups or clusters such that points in the same cluster are similar and points from different clusters are quite distinct <span class="citation">(<a href="#ref-gan2007" role="doc-biblioref">Gan, Ma, and Wu 2007</a>; <a href="#ref-gan2011" role="doc-biblioref">Gan 2011</a>)</span>. Data clustering is one of the most popular tools for exploratory data analysis and has found its applications in many scientific areas.</p>
<p>During the past several decades, many clustering algorithms have been proposed. Among these clustering algorithms, the <span class="math inline">\(k\)</span>-means algorithm is perhaps the most well-known algorithm due to its simplicity. To describe the <a href="#" class="tooltip" style="color:green"><em>k-means algorithm</em><span style="font-size:8pt">Type of clustering that aims to partition data into k mutually exclusive clusters by assigning observations to the cluster with the nearest centroid</span></a>, let <span class="math inline">\(X=\{\textbf{x}_1,\textbf{x}_2,\ldots,\textbf{x}_n\}\)</span> be a dataset containing <span class="math inline">\(n\)</span> points, each of which is described by <span class="math inline">\(d\)</span> numerical features. Given a desired number of clusters <span class="math inline">\(k\)</span>, the <span class="math inline">\(k\)</span>-means algorithm aims at minimizing the following objective function:</p>
<p><span class="math display">\[
P(U,Z) = \sum_{l=1}^k\sum_{i=1}^n u_{il} \Vert \textbf{x}_i-\textbf{z}_l\Vert^2,
\]</span></p>
<p>where <span class="math inline">\(U=(u_{il})_{n\times k}\)</span> is an <span class="math inline">\(n\times k\)</span> partition matrix, <span class="math inline">\(Z=\{\textbf{z}_1,\textbf{z}_2,\ldots,\textbf{z}_k\}\)</span> is a set of cluster centers, and <span class="math inline">\(\Vert\cdot\Vert\)</span> is the <span class="math inline">\(L^2\)</span> norm or Euclidean distance. The partition matrix <span class="math inline">\(U\)</span> satisfies the following conditions:</p>
<p><span class="math display">\[
u_{il}\in \{0,1\},\quad i=1,2,\ldots,n,\:l=1,2,\ldots,k,
\]</span></p>
<p><span class="math display">\[
\sum_{l=1}^k u_{il}=1,\quad i=1,2,\ldots,n.
\]</span></p>
<p>The <span class="math inline">\(k\)</span>-means algorithm employs an iterative procedure to minimize the objective function. It repeatedly updates the partition matrix <span class="math inline">\(U\)</span> and the cluster centers <span class="math inline">\(Z\)</span> alternately until some stop criterion is met. For more information about <span class="math inline">\(k\)</span>-means, readers are referred to <span class="citation"><a href="#ref-gan2007" role="doc-biblioref">Gan, Ma, and Wu</a> (<a href="#ref-gan2007" role="doc-biblioref">2007</a>)</span> and <span class="citation"><a href="#ref-mirkin2011" role="doc-biblioref">Mirkin</a> (<a href="#ref-mirkin2011" role="doc-biblioref">2011</a>)</span>.</p>
</div>
</div>
<div id="tree-based-models" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Tree-based Models<a href="ChapDataAnalytics.html#tree-based-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="#" class="tooltip" style="color:green"><em>Decision trees</em><span style="font-size:8pt">Modeling technique that uses a tree-like model of decisions to divide the sample space into non-overlapping regions to make predictions</span></a>, also known as tree-based models, involve dividing the predictor space (i.e., the space formed by independent variables) into a number of simple regions and using the mean or the mode of the region for prediction <span class="citation">(<a href="#ref-breiman1984" role="doc-biblioref">Breiman et al. 1984</a>)</span>. There are two types of tree-based models: classification trees and regression trees. When the dependent variable is categorical, the resulting tree models are called classification trees. When the dependent variable is continuous, the resulting tree models are called regression trees.</p>
<p>The process of building classification trees is similar to that of building regression trees. Here we only briefly describe how to build a regression tree. To do that, the predictor space is divided into non-overlapping regions such that the following objective function</p>
<p><span class="math display">\[
f(R_1,R_2,\ldots,R_J) = \sum_{j=1}^J \sum_{i=1}^n I_{R_j}(\textbf{x}_i)(y_i - \mu_j)^2
\]</span></p>
<p>is minimized, where <span class="math inline">\(I\)</span> is an indicator function, <span class="math inline">\(R_j\)</span> denotes the set of indices of the observations that belong to the <span class="math inline">\(j\)</span>th box, <span class="math inline">\(\mu_j\)</span> is the mean response of the observations in the <span class="math inline">\(j\)</span>th box, <span class="math inline">\(\textbf{x}_i\)</span> is the vector of predictor values for the <span class="math inline">\(i\)</span>th observation, and <span class="math inline">\(y_i\)</span> is the response value for the <span class="math inline">\(i\)</span>th observation.</p>
<p>In terms of predictive accuracy, decision trees generally do not perform to the level of other regression and classification models. However, tree-based models may outperform linear models when the relationship between the response and the predictors is nonlinear. For more information about decision trees, readers are referred to <span class="citation"><a href="#ref-breiman1984" role="doc-biblioref">Breiman et al.</a> (<a href="#ref-breiman1984" role="doc-biblioref">1984</a>)</span> and <span class="citation"><a href="#ref-mitchell1997" role="doc-biblioref">Mitchell</a> (<a href="#ref-mitchell1997" role="doc-biblioref">1997</a>)</span>.</p>
<!-- ###Statistical Inference -->
</div>
<div id="technical-supplement-some-r-functions" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> Technical Supplement: Some R Functions<a href="ChapDataAnalytics.html#technical-supplement-some-r-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><code>R</code> is an open-source software for statistical computing and graphics. The <code>R</code> software can be downloaded from the <code>R</code> project website at <a href="https://www.r-project.org/">https://www.r-project.org/</a>. In this section, we give some <code>R</code> function for data analysis, especially the data analysis tasks mentioned in previous sections.</p>
<p><a id=tab:2.4></a></p>
<p>Table 2.4. <strong>Some <code>R</code> Functions for Data Analysis</strong></p>
<p><span class="math display">\[
\small{
\begin{array}{lll} \hline
\text{Data Analysis Task} &amp; \text{R Package} &amp; \text{R Function} \\\hline
\text{Descriptive Statistics} &amp; \texttt{base} &amp; \texttt{summary}\\
\text{Principal Component Analysis} &amp; \texttt{stats} &amp; \texttt{prcomp} \\
\text{Data Clustering} &amp; \texttt{stats} &amp; \texttt{kmeans}, \texttt{hclust} \\
\text{Fitting Distributions} &amp; \texttt{MASS} &amp; \texttt{fitdistr} \\
\text{Linear Regression Models} &amp; \texttt{stats} &amp; \texttt{lm} \\
\text{Generalized Linear Models} &amp; \texttt{stats} &amp; \texttt{glm} \\
\text{Regression Trees} &amp; \texttt{rpart} &amp; \texttt{rpart} \\
\text{Survival Analysis} &amp; \texttt{survival} &amp; \texttt{survfit} \\
\hline
\end{array}
}
\]</span></p>
<p><a href="#tab:2.4">Table 2.4</a> lists a few <code>R</code> functions for different data analysis tasks. Readers can go to the <code>R</code> documentation to learn how to use these functions. There are also other <code>R</code> packages that do similar things. However, the functions listed in this table provide good starting points for readers to conduct data analysis in <code>R</code>. For analyzing large datasets in <code>R</code> in an efficient way, readers are referred to <span class="citation"><a href="#ref-daroczi2015" role="doc-biblioref">Daroczi</a> (<a href="#ref-daroczi2015" role="doc-biblioref">2015</a>)</span>.</p>
<hr />
<p><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</p>

</div>
</div>
</div>
<h3>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-abbott2014" class="csl-entry">
Abbott, Dean. 2014. <em>Applied Predictive Analytics: Principles and Techniques for the Professional Data Analyst</em>. Hoboken, NJ: Wiley.
</div>
<div id="ref-abdullah2013data" class="csl-entry">
Abdullah, Mohammad F., and Kamsuriah Ahmad. 2013. <span>The Mapping Process of Unstructured Data to Structured Data.</span> In <em>2013 International Conference on Research and Innovation in Information Systems (ICRIIS)</em>, 15155.
</div>
<div id="ref-aggarwal2015" class="csl-entry">
Aggarwal, Charu C. 2015. <em>Data Mining: The Textbook</em>. New York, NY: Springer.
</div>
<div id="ref-bishop2007" class="csl-entry">
Bishop, Christopher M. 2007. <em>Pattern Recognition and Machine Learning</em>. New York, NY: Springer.
</div>
<div id="ref-breiman2001modeling" class="csl-entry">
Breiman, Leo. 2001. <span>Statistical Modeling: The Two Cultures.</span> <em>Statistical Science</em> 16 (3): 199231.
</div>
<div id="ref-breiman1984" class="csl-entry">
Breiman, Leo, Jerome Friedman, Charles J. Stone, and R. A. Olshen. 1984. <em>Classification and Regression Trees</em>. Raton Boca, FL: Chapman; Hall/CRC.
</div>
<div id="ref-buttrey2017" class="csl-entry">
Buttrey, Samuel E., and Lyn R. Whitaker. 2017. <em>A Data Scientists Guide to Acquiring, Cleaning, and Managing Data in <span>R</span></em>. Hoboken, NJ: Wiley.
</div>
<div id="ref-chen2014b" class="csl-entry">
Chen, Min, Shiwen Mao, Yin Zhang, and Victor CM Leung. 2014. <em>Big Data: Related Technologies, Challenges and Future Prospects</em>. New York, NY: Springer.
</div>
<div id="ref-daroczi2015" class="csl-entry">
Daroczi, Gergely. 2015. <em>Mastering Data Analysis with <span>R</span></em>. Birmingham, UK: Packt Publishing.
</div>
<div id="ref-Dejong08" class="csl-entry">
De Jong, Piet, and Gillian Z. Heller. 2008. <em>Generalized Linear Models for Insurance Data</em>. Cambridge University Press, Cambridge.
</div>
<div id="ref-forte2015" class="csl-entry">
Forte, Rui Miguel. 2015. <em>Mastering Predictive Analytics with r</em>. Birmingham, UK: Packt Publishing.
</div>
<div id="ref-frees2009regression" class="csl-entry">
Frees, Edward W. 2009. <em>Regression Modeling with Actuarial and Financial Applications</em>. Cambridge University Press. <a href="https://doi.org/10.1017/CBO9780511814372">https://doi.org/10.1017/CBO9780511814372</a>.
</div>
<div id="ref-frees2015analytics" class="csl-entry">
. 2015. <span>Analytics of Insurance Markets.</span> <em>Annual Review of Financial Economics</em> 7: 25377.
</div>
<div id="ref-Frees2019" class="csl-entry">
Frees, Edward W, and Lisa Gao. 2019. <span>Predictive Analytics and Medical Malpractice.</span> <em>North American Actuarial Journal</em>, 117. <a href="https://doi.org/10.1080/10920277.2019.1634597">https://doi.org/10.1080/10920277.2019.1634597</a>.
</div>
<div id="ref-frees2021discriminating" class="csl-entry">
. 2021. <span>The Discriminating (Pricing) Actuary.</span> <em>North American Actuarial Journal</em>, 123. <a href="https://www.tandfonline.com/doi/pdf/10.1080/10920277.2021.1951296">https://www.tandfonline.com/doi/pdf/10.1080/10920277.2021.1951296</a>.
</div>
<div id="ref-gan2011" class="csl-entry">
Gan, Guojun. 2011. <em>Data Clustering in c++: An Object-Oriented Approach</em>. Data Mining and Knowledge Discovery Series. Boca Raton, FL, USA: Chapman &amp; Hall/CRC Press. <a href="https://doi.org/10.1201/b10814">https://doi.org/10.1201/b10814</a>.
</div>
<div id="ref-gan2007" class="csl-entry">
Gan, Guojun, Chaoqun Ma, and Jianhong Wu. 2007. <em>Data Clustering: Theory, Algorithms, and Applications</em>. Philadelphia, PA: SIAM Press. <a href="https://doi.org/10.1137/1.9780898718348">https://doi.org/10.1137/1.9780898718348</a>.
</div>
<div id="ref-goldberger1972structural" class="csl-entry">
Goldberger, Arthur S. 1972. <span>Structural Equation Methods in the Social Sciences.</span> <em>Econometrica: Journal of the Econometric Society</em>, 9791001.
</div>
<div id="ref-good1983data" class="csl-entry">
Good, I. J. 1983. <span>The Philosophy of Exploratory Data Analysis.</span> <em>Philosophy of Science</em> 50 (2): 28395.
</div>
<div id="ref-hashem2015bigdata" class="csl-entry">
Hashem, Ibrahim Abaker Targio, Ibrar Yaqoob, Nor Badrul Anuar, Salimah Mokhtar, Abdullah Gani, and Samee Ullah Khan. 2015. <span>The Rise of <span>Big Data</span> on Cloud Computing: Review and Open Research Issues.</span> <em>Information Systems</em> 47: 98115.
</div>
<div id="ref-hastie2009elements" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome H Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer.
</div>
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, Arthur E, and Robert W. Kennard. 1970. <span>Ridge Regression: Biased Estimation for Nonorthogonal Problems.</span> <em>Technometrics</em> 12 (1): 5567.
</div>
<div id="ref-hox2005data" class="csl-entry">
Hox, Joop J., and Hennie R. Boeije. 2005. <span>Data Collection, Primary Versus Secondary.</span> In <em>Encyclopedia of Social Measurement</em>, 59399. Elsevier.
</div>
<div id="ref-inmon2014" class="csl-entry">
Inmon, W. H., and Dan Linstedt. 2014. <em>Data Architecture: A Primer for the Data Scientist: Big Data, Data Warehouse and Data Vault</em>. Cambridge, MA: Morgan Kaufmann.
</div>
<div id="ref-james2013introduction" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.
</div>
<div id="ref-janert2010" class="csl-entry">
Janert, Philipp K. 2010. <em>Data Analysis with Open Source Tools</em>. Sebastopol, CA: OReilly Media.
</div>
<div id="ref-miles2014" class="csl-entry">
Miles, Matthew, Michael Hberman, and Johnny Sdana. 2014. <em>Qualitative Data Analysis: A Methods Sourcebook</em>. 3rd ed. Thousand Oaks, CA: Sage.
</div>
<div id="ref-mirkin2011" class="csl-entry">
Mirkin, Boris. 2011. <em>Core Concepts in Data Analysis: Summarization, Correlation and Visualization</em>. London, UK: Springer.
</div>
<div id="ref-mitchell1997" class="csl-entry">
Mitchell, Tom M. 1997. <em>Machine Learning</em>. McGraw-Hill.
</div>
<div id="ref-leary2013bigdata" class="csl-entry">
OLeary, D. E. 2013. <span>Artificial Intelligence and Big Data.</span> <em>IEEE Intelligent Systems</em> 28 (2): 9699.
</div>
<div id="ref-pries2015" class="csl-entry">
Pries, Kim H., and Robert Dunnigan. 2015. <em>Big Data Analytics: A Practical Guide for Managers</em>. Boca Raton, FL: CRC Press.
</div>
<div id="ref-ruppert2003semiparametric" class="csl-entry">
Ruppert, David, Matt P Wand, and Raymond J Carroll. 2003. <em>Semiparametric Regression</em>. 12. Cambridge University Press.
</div>
<div id="ref-samuel1959ml" class="csl-entry">
Samuel, A. L. 1959. <span>Some Studies in Machine Learning Using the Game of Checkers.</span> <em>IBM Journal of Research and Development</em> 3 (3): 21029.
</div>
<div id="ref-shmueli2010model" class="csl-entry">
Shmueli, Galit. 2010. <span>To Explain or to Predict?</span> <em>Statistical Science</em> 25 (3): 289310.
</div>
<div id="ref-stigler1986history" class="csl-entry">
Stigler, Stephen M. 1986. <em>The History of Statistics: The Measurement of Uncertainty Before 1900</em>. Harvard University Press.
</div>
<div id="ref-tukey1962data" class="csl-entry">
Tukey, John W. 1962. <span>The Future of Data Analysis.</span> <em>The Annals of Mathematical Statistics</em> 33 (1): 167.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ChapIntro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ChapFrequency-Modeling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
